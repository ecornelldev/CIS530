{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.matlib import repmat\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from cvxpy import *\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import minimize as m\n",
    "\n",
    "import l2distance\n",
    "import visclassifier\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from scipy.stats import linregress\n",
    "\n",
    "import pylab\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "<!--announcements-->\n",
    "<blockquote>\n",
    "    <center>\n",
    "    <img src=\"yinyang.png\" width=\"400px\" /></a>\n",
    "    </center>\n",
    "      <p><cite><center>\"Just as we have two eyes and two feet,<br>\n",
    "      duality is part of life.\"<br>\n",
    "<b>--Carlos Santana</b><br>\n",
    "      </center></cite></p>\n",
    "</blockquote>\n",
    "\n",
    "\n",
    "<h3>Introduction</h3>\n",
    "In this project, you will implement a linear support vector machine. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Linear classification</h4>\n",
    "\n",
    "<p> The first assignment is to implement a linear support vector machine. Before we get started we can generate some data to see if everything is working:  \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genrandomdata(n=100,b=0.5):\n",
    "    # generate random data and linearly separable labels\n",
    "    xTr = np.random.randn(n, 2)\n",
    "    yTr = np.ones(n)\n",
    "    # defining random hyperplane\n",
    "#     w0 = np.random.rand(2, 1)\n",
    "    # assigning labels +1, -1 labels depending on what side of the plane they lie on\n",
    "#     yTr = np.sign(np.dot(xTr, w0)+b).flatten()\n",
    "    xTr[:n // 2 ] += 5\n",
    "    xTr[n // 2: ] += 10\n",
    "    yTr[n // 2: ] = -1\n",
    "    return xTr, yTr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Recall that the unconstrained loss function for linear SVM is \n",
    "    \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{\\mathbf{w},b}\\underbrace{\\mathbf{w}^T\\mathbf{w}}_{l_{2}-regularizer} +  C\\  \\sum_{i=1}^{n}\\underbrace{\\max\\left [ 1-y_{i}(\\mathbf{w}^T \\mathbf{x}_i+b),0 \\right ]}_{hinge-loss}\n",
    "\\end{aligned}\n",
    "$$\n",
    "You will need to implement  the function <code>primalSVM</code>, which takes in training data <code>xTr</code> ($n\\times d$) and labels <code>yTr</code> ($n$) with <code>yTr[i]</code>$\\in \\{-1,1\\}$. Currently, the code below is a placeholder example of a <code>cvxpy</code> optimization problem. You need to update the objective, the constraints and introduce new variables to output the correct hyperplane and bias. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinge(w, b, xTr, yTr, C):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    w     : d   dimensional weight vector\n",
    "    xTr   : nxd dimensional matrix (each row is an input vector)\n",
    "    yTr   : n   dimensional vector (each entry is a label)\n",
    "    lmbda : regression constant (scalar)\n",
    "    \n",
    "    OUTPUTS:\n",
    "    loss     : the total loss obtained with w on xTr and yTr (scalar)\n",
    "    gradient : d dimensional gradient at w\n",
    "    \"\"\"\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    margin = yTr*(xTr @ w + b)\n",
    "#     margin[np.isclose(margin, 1)] = 1\n",
    "#     loss = w.T @ w + C*(np.sum(np.maximum(1 - margin, 0)))\n",
    "    loss = w.T @ w + C*(np.sum(np.maximum(1 - margin, 0) ** 2))\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinge_grad(w, b, xTr, yTr, C):\n",
    "    \n",
    "    n, d = xTr.shape\n",
    "    \n",
    "    wgrad = np.zeros(n)\n",
    "    bgrad = np.zeros(1)\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    margin = yTr*(xTr @ w + b)\n",
    "    margin[np.isclose(margin, 1)] = 1\n",
    "    indicator = (1 - margin > 0).astype(int)\n",
    "#     print(hinge_loss.shape)\n",
    "#     wgrad = 2*w + C*np.sum((indicator * -yTr).reshape(-1, 1) * xTr, axis=0)\n",
    "#     bgrad = C*np.sum((indicator * -yTr), axis=0)\n",
    "    \n",
    "    wgrad = 2*w + C*np.sum((2 * np.maximum(1 - margin, 0) * indicator *-yTr).reshape(-1, 1) * xTr, axis=0)\n",
    "    bgrad = C*np.sum((2 * np.maximum(1 - margin, 0) * indicator * -yTr), axis=0)\n",
    "    ### END SOLUTION\n",
    "    \n",
    "#     print(wgrad, bgrad)\n",
    "    \n",
    "    return wgrad, bgrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTr, yTr = genrandomdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "w = np.zeros(2)\n",
    "b = np.zeros(1)\n",
    "C=1000\n",
    "\n",
    "def loss_lambda(X):\n",
    "    return hinge(X[:-1], X[-1], xTr, yTr, C)\n",
    "    \n",
    "def grad_lambda(X):\n",
    "    return np.append(*hinge_grad(X[:-1], X[-1], xTr, yTr, C))\n",
    "\n",
    "def reporter(p):\n",
    "    global ps\n",
    "    ps.append(p)\n",
    "    \n",
    "X = np.append(w, np.array(b))\n",
    "\n",
    "ps = [X]\n",
    "X1 = minimize(loss_lambda, x0=X, jac=grad_lambda, method='SLSQP', callback=reporter, options={'ftol': 1e-70, 'maxiter':1000})\n",
    "# w, b = m.minimize_helper(hinge, hinge_grad, w, b, xTr, yTr, C)\n",
    "# print(hinge(w, b, xTr, yTr, C))\n",
    "# # print(w, b)\n",
    "\n",
    "# print(hinge_grad(w, b, xTr, yTr, C))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12970112025826677"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import check_grad, approx_fprime\n",
    "\n",
    "init = np.append(w, np.array(b))\n",
    "\n",
    "check_grad(loss_lambda, grad_lambda, x0=init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([466134.5443279 , 511515.30062637,      0.        ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_lambda(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.66134634e+05, 5.11515395e+05, 9.76562500e-04])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "approx_fprime(init, loss_lambda, epsilon=np.sqrt(np.finfo(float).eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03066275, 0.03094425, 0.00926449])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs((grad_lambda(init) - approx_fprime(init, loss_lambda, epsilon=np.sqrt(np.finfo(float).eps  / 10))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "517.105977056802\n"
     ]
    }
   ],
   "source": [
    "print(loss_lambda(X1.x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     fun: 0.41360012446293787\n",
       "     jac: array([-3.46302310e-06, -4.00083526e-06, -5.67043301e-07])\n",
       " message: 'Optimization terminated successfully.'\n",
       "    nfev: 190\n",
       "     nit: 75\n",
       "    njev: 75\n",
       "  status: 0\n",
       " success: True\n",
       "       x: array([-0.53436425, -0.35775648,  6.77798494])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41360012446293787\n",
      "[-3.46302310e-06 -4.00083526e-06 -5.67043301e-07]\n",
      "5.32172432792309e-06\n"
     ]
    }
   ],
   "source": [
    "losses = {'scipy': loss_lambda(X1.x)}\n",
    "solutions = {'scipy': X1.x}\n",
    "grads = {'scipy': grad_lambda(X1.x)}\n",
    "print(loss_lambda(X1.x))\n",
    "print(grad_lambda(X1.x))\n",
    "print(np.linalg.norm(grad_lambda(X1.x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w [-0.53436425 -0.35775648]\n",
      "b 6.777984936449184\n",
      "Loss 0.41360012446293787\n",
      "Grad (array([-3.46302310e-06, -4.00083526e-06]), -5.670433012028298e-07)\n",
      "Training error: 0.0%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# xTr,yTr=genrandomdata()\n",
    "# fun,w,b=primalSVM(xTr,yTr,C=10)\n",
    "w = X1.x[:2]\n",
    "b = X1.x[-1]\n",
    "\n",
    "print('w', w)\n",
    "print('b', b)\n",
    "print('Loss', hinge(w, b, xTr, yTr, C))\n",
    "print('Grad', hinge_grad(w, b, xTr, yTr, C))\n",
    "err=np.mean((np.sign(xTr.dot(w) + b))!=yTr)\n",
    "print(\"Training error: %2.1f%%\" % (err*100))\n",
    "\n",
    "\n",
    "fun = lambda x: x.dot(w) + b\n",
    "# fun2 = lambda x: x.dot(w_torch.detach().numpy()) + b_torch.detach().numpy()\n",
    "visclassifier.visclassifier(xTr,yTr,w=w,b=b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.00540856 1.97844203 1.05937625 2.45584056 1.93648323 2.63253626\n",
      " 2.53916177 2.36135457 1.96723691 1.91671336 1.93062791 2.56619845\n",
      " 0.99992033 2.61033767 1.83383133 1.15295397 2.22177294 2.29421075\n",
      " 2.43735593 2.4933632  2.27199215 2.28147654 1.07416355 2.15976656\n",
      " 2.05968382 2.03579111 2.02440421 2.84612891 2.71167005 2.62166691\n",
      " 1.77247333 1.83869893 2.20761791 1.181134   3.26533095 1.6541716\n",
      " 1.8589141  2.35658772 2.79550444 1.56619571 3.35434506 1.30988857\n",
      " 2.52678624 0.99987287 1.98464181 1.97018091 2.73013311 2.03285212\n",
      " 2.38586756 2.4663805  1.52779933 3.30283163 2.67804312 2.95466832\n",
      " 2.0202173  0.9997932  1.67438395 2.84408023 2.94248994 2.45336014\n",
      " 1.32658248 2.70593778 2.07844438 2.70847141 2.28234722 2.31273741\n",
      " 2.61441299 1.67553111 1.4142779  2.13658812 2.06289469 2.49885043\n",
      " 1.18230888 2.24014149 3.45512373 1.12439095 1.767126   3.85009724\n",
      " 1.75443017 1.41113651 1.16764943 1.76045591 1.88570135 1.56227695\n",
      " 2.6686496  1.77120513 3.21704499 1.60462089 3.28733554 2.31329178\n",
      " 2.23466419 1.61657434 1.32027415 1.93534746 2.21621219 2.67911218\n",
      " 2.03674556 2.76992229 2.66192808 2.36723588]\n",
      "2.49336319642138\n",
      "[-1.00540856e+00 -9.78442028e-01 -5.93762547e-02 -1.45584056e+00\n",
      " -9.36483232e-01 -1.63253626e+00 -1.53916177e+00 -1.36135457e+00\n",
      " -9.67236911e-01 -9.16713358e-01 -9.30627911e-01 -1.56619845e+00\n",
      "  7.96726243e-05 -1.61033767e+00 -8.33831331e-01 -1.52953971e-01\n",
      " -1.22177294e+00 -1.29421075e+00 -1.43735593e+00 -1.49336320e+00\n",
      " -1.27199215e+00 -1.28147654e+00 -7.41635483e-02 -1.15976656e+00\n",
      " -1.05968382e+00 -1.03579111e+00 -1.02440421e+00 -1.84612891e+00\n",
      " -1.71167005e+00 -1.62166691e+00 -7.72473326e-01 -8.38698925e-01\n",
      " -1.20761791e+00 -1.81134003e-01 -2.26533095e+00 -6.54171595e-01\n",
      " -8.58914101e-01 -1.35658772e+00 -1.79550444e+00 -5.66195708e-01\n",
      " -2.35434506e+00 -3.09888574e-01 -1.52678624e+00  1.27127720e-04\n",
      " -9.84641812e-01 -9.70180909e-01 -1.73013311e+00 -1.03285212e+00\n",
      " -1.38586756e+00 -1.46638050e+00 -5.27799326e-01 -2.30283163e+00\n",
      " -1.67804312e+00 -1.95466832e+00 -1.02021730e+00  2.06800061e-04\n",
      " -6.74383952e-01 -1.84408023e+00 -1.94248994e+00 -1.45336014e+00\n",
      " -3.26582479e-01 -1.70593778e+00 -1.07844438e+00 -1.70847141e+00\n",
      " -1.28234722e+00 -1.31273741e+00 -1.61441299e+00 -6.75531106e-01\n",
      " -4.14277897e-01 -1.13658812e+00 -1.06289469e+00 -1.49885043e+00\n",
      " -1.82308881e-01 -1.24014149e+00 -2.45512373e+00 -1.24390950e-01\n",
      " -7.67126004e-01 -2.85009724e+00 -7.54430165e-01 -4.11136507e-01\n",
      " -1.67649432e-01 -7.60455910e-01 -8.85701353e-01 -5.62276950e-01\n",
      " -1.66864960e+00 -7.71205128e-01 -2.21704499e+00 -6.04620890e-01\n",
      " -2.28733554e+00 -1.31329178e+00 -1.23466419e+00 -6.16574339e-01\n",
      " -3.20274146e-01 -9.35347463e-01 -1.21621219e+00 -1.67911218e+00\n",
      " -1.03674556e+00 -1.76992229e+00 -1.66192808e+00 -1.36723588e+00]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "(array([12, 43, 55]),)\n",
      "[7.96726243e-05 1.27127720e-04 2.06800061e-04]\n",
      "[-0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -1. -0. -0. -0. -0. -0.\n",
      " -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0.\n",
      " -0. -0. -0. -0. -0. -0. -0. -1. -0. -0. -0. -0. -0. -0.  0.  0.  0.  0.\n",
      "  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "-1.0\n"
     ]
    }
   ],
   "source": [
    "margin = yTr * (xTr @ w + b)\n",
    "margin[np.isclose(margin, 1)] = 1\n",
    "print(margin)\n",
    "print(margin[19])\n",
    "print(1.0 - (margin))\n",
    "\n",
    "# indicator = np.maximum(0, np.sign(1 - margin))\n",
    "# # indicator[indicator == -1] = 0\n",
    "indicator = ((1 - margin) > 0).astype(int)\n",
    "print(indicator)\n",
    "\n",
    "print(np.where(indicator != 0))\n",
    "print((1 - margin)[indicator != 0])\n",
    "print((indicator * (-yTr)))\n",
    "print(np.sum((indicator* (-yTr))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = LinearSVC(penalty='l2', loss='hinge', dual=True, C=2*C, max_iter=100000, verbose=10)\n",
    "# clf.fit(xTr, yTr)\n",
    "# print('w', clf.coef_[0])\n",
    "# print('b', clf.intercept_[0])\n",
    "# print('Loss', hinge(clf.coef_[0], clf.intercept_[0], xTr, yTr, C))\n",
    "# print('Grad', hinge_grad(clf.coef_[0], clf.intercept_[0], xTr, yTr, C))\n",
    "\n",
    "# losses['sklearn'] = hinge(clf.coef_[0], clf.intercept_[0], xTr, yTr, C)\n",
    "# grads['sklearn'] = hinge_grad(clf.coef_[0], clf.intercept_[0], xTr, yTr, C)\n",
    "# # print(np.append(np.array(clf.coef_[0]), np.array(clf.intercept_[0])))\n",
    "# solutions['sklearn'] = np.array(np.append(np.array(clf.coef_[0]), np.array(clf.intercept_[0])))\n",
    "\n",
    "# # fun = lambda x: x.dot(w) + b\n",
    "# # fun2 = lambda x: x.dot(w_torch.detach().numpy()) + b_torch.detach().numpy()\n",
    "# visclassifier.visclassifier(xTr,yTr,w=clf.coef_[0],b=clf.intercept_[0])\n",
    "\n",
    "\n",
    "# err=np.mean((np.sign(xTr.dot(clf.coef_[0]) + clf.intercept_[0]))!=yTr)\n",
    "# print(\"Training error: %2.1f%%\" % (err*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "def primalSVM(xTr, yTr, C=1):\n",
    "    \"\"\"\n",
    "    function (classifier,w,b) = primalSVM(xTr,yTr;C=1)\n",
    "    constructs the SVM primal formulation and uses a built-in \n",
    "    convex solver to find the optimal solution. \n",
    "    \n",
    "    Input:\n",
    "        xTr   | training data (nxd)\n",
    "        yTr   | training labels (n)\n",
    "        C     | the SVM regularization parameter\n",
    "    \n",
    "    Output:\n",
    "        fun   | usage: predictions=fun(xTe); predictions.shape = (n,)\n",
    "        wout  | the weight vector calculated by the solver\n",
    "        bout  | the bias term calculated by the solver\n",
    "    \"\"\"\n",
    "    N, d = xTr.shape\n",
    "    y = yTr.flatten()\n",
    "    # dummy code: example of establishing objective and constraints, and let the solver solve it.\n",
    "    w = Variable(d)\n",
    "    b = Variable(1)\n",
    "    objective = sum_squares(w)\n",
    "    constraints = [w >= 0]\n",
    "    prob = Problem(Minimize(objective), constraints)\n",
    "    prob.solve()\n",
    "    wout = w.value\n",
    "    bout = b.value\n",
    "    # End of dummy code\n",
    "    \n",
    "    ## Solution Start\n",
    "    w = Variable(d)\n",
    "    b = Variable(1)\n",
    "    e = Variable(N)\n",
    "    objective = sum_squares(w) + C*sum(e)\n",
    "    constraints = [e >= 0,\n",
    "                   multiply(y, xTr*w + b) >= 1-e]\n",
    "#     objective = sum_squares(w) + C*sum(pos(1 - multiply(y, xTr*w + b)))\n",
    "    prob = Problem(Minimize(objective), constraints)\n",
    "    prob.solve(max_iter=1000000, verbose=True)\n",
    "    wout = w.value\n",
    "    bout = b.value\n",
    "#     eout = e.value\n",
    "    ## Solution End\n",
    "    \n",
    "    fun = lambda x: x.dot(wout) + bout\n",
    "    return fun, wout, bout\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "           OSQP v0.5.0  -  Operator Splitting QP Solver\n",
      "              (c) Bartolomeo Stellato,  Goran Banjac\n",
      "        University of Oxford  -  Stanford University 2018\n",
      "-----------------------------------------------------------------\n",
      "problem:  variables n = 103, constraints m = 200\n",
      "          nnz(P) + nnz(A) = 502\n",
      "settings: linear system solver = qdldl,\n",
      "          eps_abs = 1.0e-04, eps_rel = 1.0e-04,\n",
      "          eps_prim_inf = 1.0e-04, eps_dual_inf = 1.0e-04,\n",
      "          rho = 1.00e-01 (adaptive),\n",
      "          sigma = 1.00e-06, alpha = 1.60, max_iter = 1000000\n",
      "          check_termination: on (interval 25),\n",
      "          scaling: on, scaled_termination: off\n",
      "          warm start: on, polish: on\n",
      "\n",
      "iter   objective    pri res    dua res    rho        time\n",
      "   1  -1.5821e+06   1.85e+01   2.26e+03   1.00e-01   1.76e-04s\n",
      " 200   9.7625e-01   3.72e-03   4.65e-01   1.36e-02   4.28e-04s\n",
      " 400   6.0985e-01   9.48e-04   5.17e-01   1.36e-02   1.06e-03s\n",
      " 600   4.4699e-01   1.89e-04   7.67e-01   1.36e-02   1.70e-03s\n",
      " 800   8.3464e-01   2.53e-03   2.11e-01   1.36e-02   2.64e-03s\n",
      "1000   3.4267e-01   5.05e-04   6.22e-01   1.06e-01   6.64e-04s\n",
      "1050   3.7104e-01   3.96e-04   5.78e-02   1.06e-01   8.79e-04s\n",
      "plsh   4.1367e-01   1.23e-15   1.13e-12   --------   9.95e-04s\n",
      "\n",
      "status:               solved\n",
      "solution polish:      successful\n",
      "number of iterations: 1050\n",
      "optimal objective:    0.4137\n",
      "run time:             9.95e-04s\n",
      "optimal rho estimate: 2.18e-01\n",
      "\n",
      "[-0.53446429 -0.35778952] [6.77895526]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.41366541093938225\n",
      "Grad:  (array([-1.06892857, -0.71557903]), 0.0)\n",
      "Training error: 0.0%\n"
     ]
    }
   ],
   "source": [
    "func, wout, bout = primalSVM(xTr, yTr, C=1000)\n",
    "print(wout, bout)\n",
    "visclassifier.visclassifier(xTr,yTr,w=wout,b=bout)\n",
    "\n",
    "print('Loss: ', hinge(wout, bout, xTr, yTr, C))\n",
    "print('Grad: ', hinge_grad(wout, bout, xTr, yTr, C))\n",
    "\n",
    "losses['cvxpy'] = hinge(wout, bout, xTr, yTr, C)\n",
    "grads['cvxpy'] = hinge_grad(wout, bout, xTr, yTr, C)\n",
    "solutions['cvxpy'] = np.append(wout, bout)\n",
    "err=np.mean(np.sign(xTr.dot(wout) + bout)!=yTr)\n",
    "print(\"Training error: %2.1f%%\" % (err*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'scipy': 0.41360012446293787, 'cvxpy': 0.41366541093938225}\n",
      "{'scipy': array([-3.46302310e-06, -4.00083526e-06, -5.67043301e-07]), 'cvxpy': (array([-1.06892857, -0.71557903]), 0.0)}\n",
      "{'scipy': array([-0.53436425, -0.35775648,  6.77798494]), 'cvxpy': array([-0.53446429, -0.35778952,  6.77895526])}\n"
     ]
    }
   ],
   "source": [
    "print(losses)\n",
    "print(grads)\n",
    "print(solutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_torch(w, b, xTr, yTr, C):\n",
    "    n, d = xTr.shape\n",
    "    loss = torch.matmul(w, w)\n",
    "    loss = loss + C * torch.sum((torch.max(1 - yTr * (torch.matmul(xTr, w) + b), torch.zeros(n).double()) ** 2), dim=0)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-3.46302310e-06, -4.00083526e-06]), -5.670433012028298e-07)\n",
      "0.41360012446293787\n",
      "Torch tensor([-3.4630e-06, -4.0008e-06], dtype=torch.float64) tensor(-5.6704e-07, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "C = 1000\n",
    "w = w\n",
    "b = b\n",
    "print(hinge_grad(w, b, xTr, yTr, C))\n",
    "\n",
    "xTr_torch, yTr_torch = torch.from_numpy(xTr), torch.from_numpy(yTr)\n",
    "w_torch = torch.from_numpy(w) #torch.zeros(2, requires_grad=True)\n",
    "b_torch = torch.from_numpy(np.array(b)) # torch.zeros(1, requires_grad=True)\n",
    "w_torch.requires_grad_(True)\n",
    "b_torch.requires_grad_(True)\n",
    "\n",
    "loss = loss_torch(w_torch, b_torch, xTr_torch, yTr_torch, C)\n",
    "print(loss.item())\n",
    "loss.backward()\n",
    "print('Torch', w_torch.grad, b_torch.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.46302310e-06, -4.00083526e-06])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_torch.grad.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4136, dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of scalar type Float but got scalar type Double for argument #2 'other'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-e1680e651d05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mparam_group\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_torch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_torch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_torch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxTr_torch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myTr_torch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-ca1d4b44708d>\u001b[0m in \u001b[0;36mloss_torch\u001b[0;34m(w, b, xTr, yTr, C)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxTr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0myTr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxTr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of scalar type Float but got scalar type Double for argument #2 'other'"
     ]
    }
   ],
   "source": [
    "xTr_torch, yTr_torch = torch.from_numpy(xTr).float(), torch.from_numpy(yTr).float()\n",
    "# w_torch = torch.zeros(2, requires_grad=True)\n",
    "# b_torch = torch.zeros(1, requires_grad=True)\n",
    "w_torch = torch.from_numpy(w).float()\n",
    "b_torch = torch.from_numpy(np.array([b])).float()\n",
    "w_torch.requires_grad_(True)\n",
    "b_torch.requires_grad_(True)\n",
    "C = 1000\n",
    "\n",
    "lr = 1e-6\n",
    "optimizer = torch.optim.SGD([w_torch, b_torch], lr=lr)\n",
    "epochs = 20000\n",
    "for i in range(epochs):\n",
    "    if i > 10000:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr / 10\n",
    "    elif i > 15000:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr / 100\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_torch(w_torch, b_torch, xTr_torch, yTr_torch, C)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(i, loss.item())\n",
    "\n",
    "print(i + 1, loss.item())\n",
    "\n",
    "print(w_torch, b_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w_torch.grad)\n",
    "print(b_torch.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hinge(w_torch.data.numpy(), b_torch.data.numpy(), xTr, yTr, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(losses)\n",
    "print(grads)\n",
    "print(solutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w_torch = w_torch.detach().numpy()\n",
    "b_torch = b_torch.detach().numpy()\n",
    "visclassifier.visclassifier(xTr,yTr,w=w_torch,b=b_torch)\n",
    "\n",
    "\n",
    "err=np.mean(np.sign(xTr.dot(w_torch) + b_torch)!=yTr)\n",
    "print(\"Training error: %2.1f%%\" % (err*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test your SVM primal solver with the following randomly generated data set. We label it in a way that it is guaranteed to be linearly separable. If your code works correctly the hyper-plane should separate all the $x$'s into the red half and all the $o$'s into the blue half. With sufficiently large values of $C$ (e.g. $C>10$) you should obtain $0\\%$ training error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions used to create animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateboundary():\n",
    "    global w,b,Xdata,ldata,stepsize\n",
    "\n",
    "    _, w_pre, b_pre = primalSVM(np.transpose(Xdata),np.array(ldata),C=10)\n",
    "    w = np.array(w_pre).reshape(-1)\n",
    "    b = b_pre\n",
    "    stepsize+=1\n",
    "\n",
    "def updatescreen():\n",
    "    global w,b,ax,line \n",
    "    q=-b/(w**2).sum()*w;\n",
    "    if line==None:\n",
    "        line, = ax.plot([q[0]-w[1],q[0]+w[1]],[q[1]+w[0],q[1]-w[0]],'b--')\n",
    "    else:\n",
    "        line.set_ydata([q[1]+w[0],q[1]-w[0]])\n",
    "        line.set_xdata([q[0]-w[1],q[0]+w[1]])\n",
    "    \n",
    "def animate(i):\n",
    "    if len(ldata)>0 and (min(ldata)+max(ldata)==0):\n",
    "        if stepsize<1000:\n",
    "            updateboundary()\n",
    "            updatescreen();\n",
    "    \n",
    "def onclick(event):\n",
    "    global Xdata, stepsize  \n",
    "    if event.key == 'shift': # add positive point\n",
    "        ax.plot(event.xdata,event.ydata,'or')\n",
    "        label=1\n",
    "    else: # add negative point\n",
    "        ax.plot(event.xdata,event.ydata,'ob')\n",
    "        label=-1    \n",
    "    pos=np.array([[event.xdata],[event.ydata]])\n",
    "    ldata.append(label);\n",
    "    Xdata=np.hstack((Xdata,pos))\n",
    "    stepsize=1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "Xdata=pylab.rand(2,0)\n",
    "ldata=[]\n",
    "w=[]\n",
    "b=[]\n",
    "line=None\n",
    "stepsize=1;\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "cid = fig.canvas.mpl_connect('button_press_event', onclick)\n",
    "ani = FuncAnimation(fig, animate,pylab.arange(1,100,1),interval=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Spiral data set</h4>\n",
    "\n",
    "<p>The linear classifier works great in simple linear cases. But what if the data is more complicated? We provide you with a \"spiral\" data set. You can load it and visualize it with the following two code snippets:\n",
    "<pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spiraldata(N=300):\n",
    "    r = np.linspace(1,2*np.pi,N)\n",
    "    xTr1 = np.array([np.sin(2.*r)*r, np.cos(2*r)*r]).T\n",
    "    xTr2 = np.array([np.sin(2.*r+np.pi)*r, np.cos(2*r+np.pi)*r]).T\n",
    "    xTr = np.concatenate([xTr1, xTr2], axis=0)\n",
    "    yTr = np.concatenate([np.ones(N), -1 * np.ones(N)])\n",
    "    xTr = xTr + np.random.randn(xTr.shape[0], xTr.shape[1])*0.2\n",
    "    \n",
    "    xTe = xTr[::2,:]\n",
    "    yTe = yTr[::2]\n",
    "    xTr = xTr[1::2,:]\n",
    "    yTr = yTr[1::2]\n",
    "    \n",
    "    return xTr,yTr,xTe,yTe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTr,yTr,xTe,yTe=spiraldata()\n",
    "plt.scatter(xTr[yTr == 1, 0], xTr[yTr == 1, 1], c='b')\n",
    "plt.scatter(xTr[yTr != 1, 0], xTr[yTr != 1, 1], c='r')\n",
    "plt.legend([\"+1\",\"-1\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>If you apply your previously functioning linear classifier on this data set you will see that you get terrible results. Your training error will increase drastically. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun,w,b=primalSVM(xTr,yTr,C=10)\n",
    "visclassifier(fun,xTr,yTr,w=[],b=0)\n",
    "err=np.mean(arrayify(np.sign(fun(xTr)))!=yTr)\n",
    "print(\"Training error: %2.1f%%\" % (err*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Implementing a kernelized SVM</h3>\n",
    "\n",
    "<p> For a data set as complex as the spiral data set, you will need a more complex classifier. \n",
    "First implement the kernel function\n",
    "<pre>\tcomputeK(kerneltype,X,Z,kpar)</pre>\n",
    "It takes as input a kernel type (kerneltype) and two data sets $\\mathbf{X}$ in $\\mathcal{R}^{n\\times d}$ and $\\mathbf{Z}$ in $\\mathcal{R}^{m\\times d}$ and outputs a kernel matrix $\\mathbf{K}\\in{\\mathcal{R}^{n\\times m}}$. The last input, <code>kpar</code> specifies the kernel parameter (e.g. the inverse kernel width $\\gamma$ in the RBF case or the degree $p$ in the polynomial case.)\n",
    "\t<ol>\n",
    "\t<li>For the linear kernel (<code>ktype='linear'</code>) svm, use $k(\\mathbf{x},\\mathbf{z})=x^Tz$ </li> \n",
    "\t<li>For the radial basis function kernel (<code>ktype='rbf'</code>) svm use $k(\\mathbf{x},\\mathbf{z})=\\exp(-\\gamma ||x-z||^2)$ (gamma is a hyperparameter, passed a the value of kpar)</li>\n",
    "\t<li>For the polynomial kernel (<code>ktype='poly'</code>) use  $k(\\mathbf{x},\\mathbf{z})=(x^Tz + 1)^d$ (d is the degree of the polymial, passed as the value of kpar)</li>\n",
    "</ol>\n",
    "\n",
    "<p>You can use the function <b><code>l2distance</code></b> as a helperfunction.</p>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "def computeK(kerneltype, X, Z, kpar=0):\n",
    "    \"\"\"\n",
    "    function K = computeK(kernel_type, X, Z)\n",
    "    computes a matrix K such that Kij=k(x,z);\n",
    "    for three different function linear, rbf or polynomial.\n",
    "    \n",
    "    Input:\n",
    "    kerneltype: either 'linear','polynomial','rbf'\n",
    "    X: n input vectors of dimension d (nxd);\n",
    "    Z: m input vectors of dimension d (mxd);\n",
    "    kpar: kernel parameter (inverse kernel width gamma in case of RBF, degree in case of polynomial)\n",
    "    \n",
    "    OUTPUT:\n",
    "    K : nxm kernel matrix\n",
    "    \"\"\"\n",
    "    assert kerneltype in [\"linear\",\"polynomial\",\"poly\",\"rbf\"], \"Kernel type %s not known.\" % kerneltype\n",
    "    assert X.shape[1] == Z.shape[1], \"Input dimensions do not match\"\n",
    "    \n",
    "    ## Solution Start\n",
    "    if kerneltype == \"linear\":\n",
    "        K = X.dot(Z.T)\n",
    "    elif kerneltype == \"polynomial\" or kerneltype == \"poly\":\n",
    "        K = np.power((X.dot(Z.T) + 1), kpar)\n",
    "    else:\n",
    "        K = np.exp(-kpar*np.square(l2distance.l2distance(X,Z)))\n",
    "    ## Solution End\n",
    "    \n",
    "    return K\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The following code snippet plots an image of the kernel matrix for the data points in the spiral set. Use it to test your <b><code>computeK</code></b> function:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTr,yTr,xTe,yTe=spiraldata()\n",
    "K=computeK(\"rbf\",xTr,xTr,kpar=0.05)\n",
    "# plot an image of the kernel matrix\n",
    "plt.pcolormesh(K, cmap='jet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the SVM optimization has the following dual formulation:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "             &\\min_{\\alpha_1,\\cdots,\\alpha_n}\\frac{1}{2} \\sum_{i,j}\\alpha_i \\alpha_j y_i y_j \\mathbf{K}_{ij} - \\sum_{i=1}^{n}\\alpha_i  \\\\\n",
    "       \\text{s.t.}  &\\quad 0 \\leq \\alpha_i \\leq C\\\\\n",
    "             &\\quad \\sum_{i=1}^{n} \\alpha_i y_i = 0.\n",
    "\\end{aligned}\n",
    "$$\n",
    "This is equivalent to solving for the SVM primal\n",
    "$$ L(\\mathbf{w},b) = C\\sum_{i=1}^n \\max(1-y_i(\\mathbf{w}^\\top\\phi(\\mathbf{x}_i)+b),0) + ||w||_2^2$$\n",
    "where $\\mathbf{w}=\\sum_{i=1}^n y_i \\alpha_i \\phi(\\mathbf{x}_i)$ and $\\mathbf{K}_{ij}=k(\\mathbf{x}_i,\\mathbf{x}_j)=\\phi(\\mathbf{x}_i)^\\top\\phi(\\mathbf{x}_j)$, for some mapping $\\phi(\\cdot)$.  Please note that here all $\\alpha_i\\geq 0$, which is possible because we multiply by $y_i$ in the definition of $\\mathbf{w}$. One advantage of keeping all $\\alpha_i$ non-negative is that we can easily identify non-support vectors as vectors with $\\alpha_i=0$. \n",
    "\n",
    "<p>Implement the function <code>dualqp</code>, which takes as input a kernel matrix $K$, a vector of labels $yTr$ in $\\mathcal{R}^{n}$, and a regularization constant $C\\geq 0$. This function should solve the quadratic optimization problem and output the optimal vector $\\mathbf{\\alpha}\\in{\\mathcal{R}^n}$.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "def dualqp(K,yTr,C):\n",
    "    \"\"\"\n",
    "    function alpha = dualqp(K,yTr,C)\n",
    "    constructs the SVM dual formulation and uses a built-in \n",
    "    convex solver to find the optimal solution. \n",
    "    \n",
    "    Input:\n",
    "        K     | the (nxn) kernel matrix\n",
    "        yTr   | training labels (nx1)\n",
    "        C     | the SVM regularization parameter\n",
    "    \n",
    "    Output:\n",
    "        alpha | the calculated solution vector (nx1)\n",
    "    \"\"\"\n",
    "    y = yTr.flatten()\n",
    "    N, _ = K.shape\n",
    "    alpha = Variable(N)\n",
    "    \n",
    "    ## Solution Start\n",
    "    mat = multiply(y,alpha)\n",
    "    q_mat = quad_form(mat,K)\n",
    "    objective = 0.5*q_mat - sum(alpha)\n",
    "    constraints = [0 <= alpha, alpha <= C, sum(multiply(y,alpha)) == 0]\n",
    "    prob = Problem(Minimize(objective), constraints)\n",
    "    prob.solve()\n",
    "    ## Solution End\n",
    "    \n",
    "    return np.array(alpha.value).flatten()\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code shows a usecase of how <code>dualqp</code> could be used in practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 10\n",
    "lmbda = 0.25\n",
    "ktype = \"rbf\"\n",
    "xTr,yTr,xTe,yTe=spiraldata()\n",
    "# compute kernel (make sure it is PSD)\n",
    "K = computeK(ktype,xTr,xTr)\n",
    "eps = 1e-10\n",
    "# make sure it is symmetric and positive semi-definite\n",
    "K = (K + K.T) / 2 + eps * np.eye(K.shape[0])\n",
    "\n",
    "alpha=dualqp(K,yTr,C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Now that you can solve the dual correctly, you should have the values for $\\alpha_i$. But you are not done yet. You still need to be able to classify new test points. Remember from class that $h(\\mathbf{x})=\\sum_{i=1}^n \\alpha_i y_i k(\\mathbf{x}_i,\\mathbf{x})+b$. You need to obtain $b$. It is easy to show (and omitted here) that if $C>\\alpha_i>0$ (with strict $>$), then we must have that $y_i(\\mathbf{w}^\\top \\phi(\\mathbf{x}_i)+b)=1$. Rephrase this equality in terms of $\\alpha_i$ and solve for $b$. Implement\n",
    "\n",
    "<p> b=recoverBias(K,yTr,alphas,C); </p>\n",
    "\n",
    "<p> where <code>b</code> is the hyperplane bias.\n",
    "(Hint: This is most stable if you pick an $\\alpha_i$ that is furthest from $C$ and $0$. )</p>\n",
    "\n",
    "<p>Please note that this use of the word bias has absolutely nothing to do with the word bias in the bias variance trade-off. It is just the same word but two completely different meanings. This unfortunate term collision comes from the fact that we are borrowing concepts from geometry and statistics.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "def recoverBias(K,yTr,alpha,C):\n",
    "    \"\"\"\n",
    "    function bias=recoverBias(K,yTr,alpha,C);\n",
    "    Solves for the hyperplane bias term, which is uniquely specified by the \n",
    "    support vectors with alpha values 0<alpha<C\n",
    "    \n",
    "    INPUT:\n",
    "    K : nxn kernel matrix\n",
    "    yTr : nx1 input labels\n",
    "    alpha  : nx1 vector of alpha values\n",
    "    C : regularization constant\n",
    "    \n",
    "    Output:\n",
    "    bias : the scalar hyperplane bias of the kernel SVM specified by alphas\n",
    "    \"\"\"\n",
    "\n",
    "    ## Solution Start\n",
    "    idx = (np.abs(alpha-(C/2))).argmin()\n",
    "    y_i = yTr[idx]\n",
    "    bias = y_i - np.sum(alpha*yTr*K[idx])\n",
    "    ## Solution End\n",
    "    \n",
    "    return bias\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Test your <b><code>recoverBias</code></b> function with the following code, which uses the dual solver on a linearly separable dataset:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTr,yTr=genrandomdata(b=0.5)\n",
    "C=10\n",
    "K=computeK(\"linear\",xTr,xTr)\n",
    "eps=1e-10\n",
    "K = (K + K.T) / 2 + eps * np.eye(K.shape[0])\n",
    "alpha = dualqp(K,yTr,C)\n",
    "ba=recoverBias(K,yTr,alpha,C)\n",
    "wa = (alpha * yTr).dot(xTr)\n",
    "fun = lambda x: x.dot(wa) + ba\n",
    "visclassifier(fun, xTr, yTr, w=wa, b=ba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    Implement the function \n",
    "    <pre>\n",
    "    svmclassify=dualSVM(xTr,yTr,C,ktype,kpar);\n",
    "    </pre>\n",
    "    It should use your functions <code><b>computeK</b></code> and <code><b>generateQP</b></code> to solve the SVM dual problem of an SVM specified by a training data set (<code><b>xTr,yTr</b></code>), a regularization parameter (<code>C</code>), a kernel type (<code>ktype</code>) and kernel parameter (<code>lmbda</code>, to be used as kpar in Kernel construction). Then, find the support vectors and recover the bias to return <b><code>svmclassify</code></b>, a function that uses your SVM to classify a set of test points <code>xTe</code>.\n",
    "\n",
    "<b>Hint: You need to ensure that the kernel matrix is positive semi-definite during training. The best way to do this is to make sure it is strictly symmetric and to add the identity matrix to it, multiplied by a tiny epsilon value.</b>\n",
    "    \n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "def dualSVM(xTr,yTr,C,ktype,lmbda,eps=1e-10):\n",
    "    \"\"\"\n",
    "    function classifier = dualSVM(xTr,yTr,C,ktype,lmbda);\n",
    "    Constructs the SVM dual formulation and uses a built-in \n",
    "    convex solver to find the optimal solution. \n",
    "    \n",
    "    Input:\n",
    "        xTr   | training data (nxd)\n",
    "        yTr   | training labels (nx1)\n",
    "        C     | the SVM regularization parameter\n",
    "        ktype | the type of kernelization: 'rbf','polynomial','linear'\n",
    "        lmbda | the kernel parameter - degree for poly, inverse width for rbf\n",
    "    \n",
    "    Output:\n",
    "        svmclassify | usage: predictions=svmclassify(xTe);\n",
    "    \"\"\"\n",
    "    svmclassify = lambda x: x #Dummy code\n",
    "    \n",
    "    # Solution Start\n",
    "    y = yTr.flatten()\n",
    "    \n",
    "    # Construct Kernel\n",
    "    K = computeK(ktype, xTr, xTr, lmbda)\n",
    "    K = (K + K.T) / 2 + eps * np.eye(K.shape[0])\n",
    "\n",
    "    # Solve DualQP\n",
    "    alpha = dualqp(K,y,C)\n",
    "    \n",
    "    # Recover Bias\n",
    "    b = recoverBias(K,y,alpha,C)\n",
    "\n",
    "    # Build classifier\n",
    "    interm = alpha*y\n",
    "    svmclassify = lambda x: np.sign(computeK(ktype, xTr, x, lmbda).transpose().dot(interm) + b)\n",
    "    \n",
    "    # Solution End\n",
    "    \n",
    "    return svmclassify\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now we try the SVM with RBF kernel on the spiral data. If you implemented it correctly, train and test error should be close to zero.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xTr,yTr,xTe,yTe=spiraldata()\n",
    "C=10.0\n",
    "sigma=0.25\n",
    "ktype=\"rbf\"\n",
    "svmclassify=dualSVM(xTr,yTr,C,ktype,sigma)\n",
    "\n",
    "visclassifier(svmclassify,xTr,yTr)\n",
    "\n",
    "# compute training and testing error\n",
    "predsTr=svmclassify(xTr)\n",
    "trainingerr=np.mean(np.sign(predsTr)!=yTr)\n",
    "print(\"Training error: %2.4f\" % trainingerr)\n",
    "\n",
    "predsTe=svmclassify(xTe)\n",
    "testingerr=np.mean(np.sign(predsTe)!=yTe)\n",
    "print(\"Testing error: %2.4f\" % testingerr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVMs are pretty sensitive to hyper-parameters. We can visualize the results of a hyper-parameter grid search as a heat-map, where we sweep across different values of C and kpar and output the result on a validation dataset. Now we ask you to implement a cross-validation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "def cross_validation(xTr,yTr,xValid,yValid,ktype,CList,lmbdaList):\n",
    "    \"\"\"\n",
    "    function bestC,bestLmbda,ErrorMatrix = cross_validation(xTr,yTr,xValid,yValid,ktype,CList,lmbdaList);\n",
    "    Use the parameter search to find the optimal parameter,\n",
    "    Individual models are trained on (xTr,yTr) while validated on (xValid,yValid)\n",
    "    \n",
    "    Input:\n",
    "        xTr      | training data (nxd)\n",
    "        yTr      | training labels (nx1)\n",
    "        xValid   | training data (mxd)\n",
    "        yValid   | training labels (mx1)\n",
    "        ktype    | the type of kernelization: 'rbf','polynomial','linear'\n",
    "        CList    | The list of values to try for the SVM regularization parameter C (ax1)\n",
    "        lmbdaList| The list of values to try for the kernel parameter lmbda- degree for poly, inverse width for rbf (bx1)\n",
    "    \n",
    "    Output:\n",
    "        bestC      | the best C parameter\n",
    "        bestLmbda  | the best Lmbda parameter\n",
    "        ErrorMatrix| the test error rate for each given C and Lmbda when trained on (xTr,yTr) and tested on (xValid,yValid),(axb)\n",
    "    \"\"\"\n",
    "    # gridsearch for best parameters\n",
    "    ErrorMatrix=np.zeros((len(CList),len(lmbdaList)))\n",
    "    bestC,bestLmbda = 0.,0.\n",
    "    \n",
    "    # Start Solution\n",
    "    for i in range(len(CList)):\n",
    "        c = CList[i]\n",
    "        for j in range(len(lmbdaList)):\n",
    "            lm = lmbdaList[j]\n",
    "            svmclassify=dualSVM(xTr,yTr,c,ktype,lm)\n",
    "            predsTe=svmclassify(xValid)\n",
    "            ErrorMatrix[i][j]=np.mean(np.sign(predsTe)!=yValid)\n",
    "\n",
    "    cInd,lmInd = np.unravel_index(ErrorMatrix.argmin(), ErrorMatrix.shape)\n",
    "    bestC = CList[cInd]\n",
    "    bestLmbda = lmbdaList[lmInd]\n",
    "    # End Solution\n",
    "            \n",
    "    return bestC,bestLmbda,ErrorMatrix\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xTr,yTr,xValid,yValid=spiraldata(100)\n",
    "CList=(2.0**np.linspace(-1,5,7))\n",
    "lmbdaList=(np.linspace(0.1,0.5,5))\n",
    "\n",
    "bestC,bestLmbda,ErrorMatrix = cross_validation(xTr,yTr,xValid,yValid,'rbf',CList,lmbdaList)\n",
    "\n",
    "plt.pcolormesh(ErrorMatrix, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"lmbda_idx\")\n",
    "plt.ylabel(\"C_idx\")\n",
    "plt.title(\"Validation error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you implemented everything correctly, the result should look similar to this image:\n",
    "<center>\n",
    " <img src=\"crossval.png\" width=\"300px\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Competition: we ask you to implement function autosvm, which given xTr and yTr, splits them into training data and validation data, and then uses a hyperparameter search to find the optimal hyper parameters. \n",
    "\n",
    "Function autosvm should return a function which will act as a classifier on xTe.\n",
    "\n",
    "You have a 5 minute time limit on multiple datasets, each dataset having different optimal hyperparameters, so you should strive for a good method of finding hyperparameters (within the time limit) instead of just trying to find a static set of good hyperparameters. \n",
    "\n",
    "You will get extra credit for the competition if you can beat the base benchmark of 34% error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "def autosvm(xTr,yTr):\n",
    "    \"\"\"\n",
    "    svmclassify = autosvm(xTr,yTr), where yTe = svmclassify(xTe)\n",
    "    \"\"\"\n",
    "\n",
    "    # (Optional) TODO 7\n",
    "    return dualSVM(xTr, yTr, 10, 'linear', .25)\n",
    "\n",
    "#</GRADED>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
