{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cd9c4d4e3dcf43bafad3e36e5b03aa79",
     "grade": false,
     "grade_id": "cell-6da27a81ca807569",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import helper as h\n",
    "import matplotlib.pyplot as plt\n",
    "from visclassifier import visclassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6a9c58a43398a4811433f6ff8641e5db",
     "grade": false,
     "grade_id": "cell-5b36c08fbebe600d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Recall that the we can express the solution as a linear combination of the training examples $ \\{\\phi(\\mathbf{x}_1), \\cdots, \\phi(\\mathbf{x}_n) \\}$:\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "    \\mathbf{w} = \\sum_{j = 1}^n \\alpha_j y_j \\phi(\\mathbf{x}_j)\n",
    "    \\end{aligned}\n",
    "$$\n",
    "\n",
    "To simplify, we define $\\beta_j = \\alpha_j y_j$ and we can express the solution as: \n",
    "$$\n",
    "    \\begin{aligned}\n",
    "    \\mathbf{w} = \\sum_{j = 1}^n \\beta_j \\phi(\\mathbf{x}_j)\n",
    "    \\end{aligned}\n",
    "$$\n",
    "\n",
    "If we substitute the formulation above into our unconstrained regularized squared hinge loss, \n",
    "$$\n",
    "    \\begin{aligned}\n",
    "    \\min_{\\mathbf{w},b}\\underbrace{\\mathbf{w}^T\\mathbf{w}}_{l_{2} \\text{ regularizer} } +  C\\  \\sum_{i=1}^{n}\\underbrace{\\max\\left [ 1-y_{i}(\\mathbf{w}^T \\mathbf{x}_i+b),0 \\right ] ^2}_{ \\text{squared hinge loss} }\n",
    "    \\end{aligned}\n",
    "$$\n",
    "\n",
    "we obtain: \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "       \\underbrace{\\sum_{i = 1} ^n \\sum_{j = 1}^n \\beta_i  \\beta_j \\phi(\\mathbf{x} _i)^T \\phi(\\mathbf{x}_j)}_{l_{2} \\text{ regularizer}} +  C\\  \\sum_{i=1}^{n}\\underbrace{\\max\\left [ 1-y_{i}(\\sum_{j = 1}^n \\beta_j \\phi(\\mathbf{x}_j)^T \\phi(\\mathbf{x}_i)+b),0 \\right ] ^ 2}_{\\text{squared hinge loss}}     \n",
    "\\end{aligned}\n",
    "$$\n",
    "Now, if we replace all the $\\phi(\\mathbf{x} _i)^T \\phi(\\mathbf{x}_j)$ with $k(\\mathbf{x}_i, \\mathbf{x}_j)$, we obtained the \"kernelized\" hinge loss. \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "      \\min_{\\beta_1,\\cdots,\\beta_n, b} \\underbrace{\\sum_{i = 1} ^n \\sum_{j = 1}^n \\beta_i \\beta_j k(\\mathbf{x} _i, \\mathbf{x}_j)}_{l_{2} \\text{ regularizer}} +  C\\  \\sum_{i=1}^{n}\\underbrace{\\max\\left [ 1-y_{i}(\\sum_{j = 1}^n \\beta_j k(\\mathbf{x}_j, \\mathbf{x}_i)+b),0 \\right ] ^ 2}_{\\text{squared hinge loss}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The expression above seems a little bit unweildy, so we are going to simplify the expression again. First, we will simplfy the $l_{2}$ regularizer. Define $\\mathbf{\\beta} = [\\beta_1, \\cdots, \\beta_n]^T$ and $K_{train}$ to be the kernel matrix calculated on the training set, namely, the (i, j)-th entry of $K_{train}$ is $k(\\mathbf{x} _i, \\mathbf{x}_j)$. It is easy to verify that the $l_{2}$ regularizer can be expressed as the quadratic form  \n",
    "\n",
    "$$\n",
    "    \\sum_{i = 1} ^n \\sum_{j = 1}^n \\beta_i \\beta_j k(\\mathbf{x} _i, \\mathbf{x}_j) = \\mathbf{\\beta}^T K_{train} \\mathbf{\\beta}\n",
    "$$\n",
    "\n",
    "Now, let's simplify the squared hinge loss. To start, we define $K_{train}[i]$ as the i-th row of $k_{train}$ expressed as a column vector. Now, the summation term within the hinge loss can be expressed as: \n",
    "\n",
    "$$\n",
    "    \\sum_{ j = 1}^n \\beta_j k(\\mathbf{x}_j, \\mathbf{x}_i) = \\mathbf{\\beta}^T K_{train}[i]\n",
    "$$\n",
    "\n",
    "Combining the two simplifications we have, we arrive at: \n",
    "$$\n",
    "\\begin{aligned}\n",
    "      \\min_{\\mathbf{\\beta}, b} \\underbrace{\\mathbf{\\beta}^T k_{train} \\mathbf{\\beta}}_{l_{2}  \\text{ regularizer}} +  C\\  \\sum_{i=1}^{n}\\underbrace{\\max\\left [ 1-y_{i}(\\mathbf{\\beta}^T K_{train}[i]+b),0 \\right ] ^ 2}_{\\text{squared hinge loss}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "Note that the loss function we have above is very similar to the vanilla linear SVM, the differences are: \n",
    "\n",
    "1. Instead of $\\mathbf{w}$, we have $\\mathbf{\\beta}$\n",
    "2. The $l_{2}$ regularizer $\\mathbf{w}^T \\mathbf{w} = \\mathbf{w}^T I \\mathbf{w}$ is changed to $\\mathbf{\\beta}^T k_{train} \\mathbf{\\beta}$ to account for using $\\mathbf{\\beta}$ instead of $\\mathbf{w}$\n",
    "3. The inner product $\\mathbf{w}^T \\mathbf{x}_i$ in the hinge loss is changed to $\\mathbf{\\beta}^T K_{train}[i]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2fc23e4e8e5258fd546dcfad25cde371",
     "grade": false,
     "grade_id": "cell-38378dba31605ea0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Before we start, let's generate some data and visualize the training set. We are going to use the linear separable data that we used in our previous project!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "88cc81aa4f85257c42334cbc78ba5019",
     "grade": false,
     "grade_id": "cell-38e09fdf6da3e10b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "xTr,yTr = h.generate_data()\n",
    "h.visualize_2D(xTr, yTr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6de09e4218785f02253156ccbfd9c9ab",
     "grade": false,
     "grade_id": "cell-4a8d3426274ccdbd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In this assignment, you need to implement three functions <code>computeK</code> that computes the kernel function efficiently, <code>loss</code> that calculates the unconstrained kernelized hinge loss and <code>grad</code> that calculates the gradient of the loss with respect to $\\mathbf{\\beta}, b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e23ae17eb586c7aaa116402f5a07381f",
     "grade": false,
     "grade_id": "cell-c36368930d78904c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In <code>computeK</code>, you are going to calcuate the values of different kernel functions. Note that when calculating the rbf kernel, you can use the <code>h.l2distance(X, Z)</code> functions to calculate the pairwise l2 distance efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d41932ffea7065dc8175fa15159f1be8",
     "grade": false,
     "grade_id": "cell-54020608e1b5747d",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def computeK(kerneltype, X, Z, kpar=1):\n",
    "    \"\"\"\n",
    "    function K = computeK(kernel_type, X, Z)\n",
    "    computes a matrix K such that Kij=k(x,z);\n",
    "    for three different function linear, rbf or polynomial.\n",
    "    \n",
    "    Input:\n",
    "    kerneltype: either 'linear','polynomial','rbf'\n",
    "    X: n input vectors of dimension d (nxd);\n",
    "    Z: m input vectors of dimension d (mxd);\n",
    "    kpar: kernel parameter (inverse kernel width gamma in case of RBF, degree in case of polynomial)\n",
    "    \n",
    "    OUTPUT:\n",
    "    K : nxm kernel matrix\n",
    "    \"\"\"\n",
    "    assert kerneltype in [\"linear\",\"polynomial\",\"rbf\"], \"Kernel type %s not known.\" % kerneltype\n",
    "    assert X.shape[1] == Z.shape[1], \"Input dimensions do not match\"\n",
    "    \n",
    "    K = None\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "809ea7826eb149ef7ff2563fa1b50483",
     "grade": false,
     "grade_id": "cell-18c9391e0771d3f3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# These tests test whether your computeK() is implemented correctly\n",
    "\n",
    "xTr_test, yTr_test = h.generate_data(100)\n",
    "xTr_test2, yTr_test2 = h.generate_data(50)\n",
    "n, d = xTr_test.shape\n",
    "\n",
    "# Checks whether computeK compute the kernel matrix with the right dimension\n",
    "def computeK_test1():\n",
    "    s1 = (computeK('rbf', xTr_test, xTr_test2, kpar=1).shape == (100, 50))\n",
    "    s2 = (computeK('polynomial', xTr_test, xTr_test2, kpar=1).shape == (100, 50))\n",
    "    s3 = (computeK('linear', xTr_test, xTr_test2, kpar=1).shape == (100, 50))\n",
    "    return (s1 and s2 and s3)\n",
    "\n",
    "# Checks whether the kernel matrix is symmetric\n",
    "def computeK_test2():\n",
    "    k_rbf = computeK('rbf', xTr_test, xTr_test, kpar=1)\n",
    "    s1 = np.allclose(k_rbf, k_rbf.T)\n",
    "    k_poly = computeK('polynomial', xTr_test, xTr_test, kpar=1)\n",
    "    s2 = np.allclose(k_poly, k_poly.T)\n",
    "    k_linear = computeK('linear', xTr_test, xTr_test, kpar=1)\n",
    "    s3 = np.allclose(k_linear, k_linear.T)\n",
    "    return (s1 and s2 and s3)\n",
    "\n",
    "# Checks whether the kernel matrix is positive semi-definite\n",
    "def computeK_test3():\n",
    "    k_rbf = computeK('rbf', xTr_test2, xTr_test2, kpar=1)\n",
    "    eigen_rbf = np.linalg.eigvals(k_rbf)\n",
    "    eigen_rbf[np.isclose(eigen_rbf, 0)] = 0\n",
    "    s1 = np.all(eigen_rbf >= 0)\n",
    "    k_poly = computeK('polynomial', xTr_test2, xTr_test2, kpar=1)\n",
    "    eigen_poly = np.linalg.eigvals(k_poly)\n",
    "    eigen_poly[np.isclose(eigen_poly, 0)] = 0\n",
    "    s2 = np.all(eigen_poly >= 0)\n",
    "    k_linear = computeK('linear', xTr_test2, xTr_test2, kpar=1)\n",
    "    eigen_linear = np.linalg.eigvals(k_linear)\n",
    "    eigen_linear[np.isclose(eigen_linear, 0)] = 0\n",
    "    s3 = np.all(eigen_linear >= 0)\n",
    "    return (s1 and s2 and s3)\n",
    "\n",
    "# Checks whether computeK compute the right kernel matrix with rbf kernel\n",
    "def computeK_test4():\n",
    "    k = computeK('rbf', xTr_test, xTr_test2, kpar=1)\n",
    "    k2 = h.computeK_grader('rbf', xTr_test, xTr_test2, kpar=1)\n",
    "    \n",
    "    return np.linalg.norm(k - k2) < 1e-5\n",
    "\n",
    "# Checks whether computeK compute the right kernel matrix with polynomial kernel\n",
    "def computeK_test5():\n",
    "    k = computeK('polynomial', xTr_test, xTr_test2, kpar=1)\n",
    "    k2 = h.computeK_grader('polynomial', xTr_test, xTr_test2, kpar=1)\n",
    "    \n",
    "    return np.linalg.norm(k - k2) < 1e-5\n",
    "\n",
    "# Checks whether computeK compute the right kernel matrix with linear kernel\n",
    "def computeK_test6():\n",
    "    k = computeK('linear', xTr_test, xTr_test2, kpar=1)\n",
    "    k2 = h.computeK_grader('linear', xTr_test, xTr_test2, kpar=1)\n",
    "    \n",
    "    return np.linalg.norm(k - k2) < 1e-5\n",
    "\n",
    "\n",
    "h.runtest(computeK_test1, 'computeK_test1')\n",
    "h.runtest(computeK_test2, 'computeK_test2')\n",
    "h.runtest(computeK_test3, 'computeK_test3')\n",
    "h.runtest(computeK_test4, 'computeK_test4')\n",
    "h.runtest(computeK_test5, 'computeK_test5')\n",
    "h.runtest(computeK_test5, 'computeK_test6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bd2c8ccbbecc00e7d6aad6329fd4edf3",
     "grade": true,
     "grade_id": "cell-computeK-test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs computeK_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "90c7eea29e746c03b7c404d207953eb7",
     "grade": true,
     "grade_id": "cell-computeK-test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs computeK_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "44c7806efdc12fc50ccc08a4a46e8a9d",
     "grade": true,
     "grade_id": "cell-computeK-test3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs computeK_test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f6eb66f21d145c4a9d9687ee4ef19569",
     "grade": true,
     "grade_id": "cell-computeK-test4",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs computeK_test4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ae6de714e9658eccb72624bc3d9fb153",
     "grade": true,
     "grade_id": "cell-computeK-test5",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs computeK_test5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9d99ad37ee48a381df2ea875e48ebf44",
     "grade": true,
     "grade_id": "cell-computeK-test6",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs computeK_test5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a91c10bcef025c98ad8b6e6d6008a7c9",
     "grade": false,
     "grade_id": "cell-7a01c840a42c22ae",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now, you are going to implement the <code>loss</code>. Note that the function signature of this function is slightly different from the previous assignment. Previously, we can pass in $\\mathbf{w}$ to calculate the loss values. However, in the kernelized version of the loss, $\\mathbf{w}$ is calculated as the linear combination of the training examples, so we need to pass the training set to the function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "032920135ed42d1ff3dfd5345ca7815c",
     "grade": false,
     "grade_id": "cell-ce269b51e709e965",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def loss(beta, b, xTr, yTr, xTe, yTe, C, kerneltype, kpar=1):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    beta : n dimensional vector that stores the linear combination coefficient\n",
    "    xTr   : nxd dimensional matrix (training set, each row is an input vector)\n",
    "    yTr   : n   dimensional vector (training label, each entry is a label)\n",
    "    b     : scalar (bias)\n",
    "    xTe   : mxd dimensional matrix (test set, each row is an input vector)\n",
    "    yTe   : m dimensional vector (test label, each entry is a label)\n",
    "    C     : scalar (constant that controls the tradeoff between l2-regularizer and hinge-loss)\n",
    "    kerneltype: either 'linear','polynomial','rbf'\n",
    "    kpar  : kernel parameter (inverse kernel width gamma in case of RBF, degree in case of polynomial)\n",
    "    \n",
    "    OUTPUTS:\n",
    "    loss     : the total loss obtained with (alpha, xTr, yTr, b) on xTe and yTe (scalar)\n",
    "    \"\"\"\n",
    "    \n",
    "    loss_val = 0.0\n",
    "    # compute the kernel values between xTr and xTr \n",
    "    kernel_train = computeK(kerneltype, xTr, xTr, kpar)\n",
    "    # compute the kernel values between xTe and xTr\n",
    "    kernel_test = computeK(kerneltype, xTe, xTr, kpar)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "050f214ba482ba44f77d7886730c287d",
     "grade": false,
     "grade_id": "cell-26c56a8bb3f4cd32",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# These tests test whether your loss() is implemented correctly\n",
    "\n",
    "xTr_test, yTr_test = h.generate_data()\n",
    "n, d = xTr_test.shape\n",
    "\n",
    "# Check whether your loss() returns a scalar\n",
    "def loss_test1():\n",
    "    beta = np.zeros(n)\n",
    "    b = np.zeros(1)\n",
    "    loss_val = loss(beta, b, xTr_test, yTr_test, xTr_test, yTr_test, 10, 'rbf')\n",
    "    \n",
    "    return np.isscalar(loss_val)\n",
    "\n",
    "\n",
    "# Check whether your loss() returns a nonnegative scalar\n",
    "def loss_test2():\n",
    "    beta = np.random.rand(n)\n",
    "    b = np.random.rand(1)\n",
    "    loss_val = loss(beta, b, xTr_test, yTr_test, xTr_test, yTr_test, 10, 'rbf')\n",
    "    \n",
    "    return loss_val >= 0\n",
    "\n",
    "# Check whether you implement l2-regularizer correctly\n",
    "def loss_test3():\n",
    "    beta = np.random.rand(n)\n",
    "    b = np.random.rand(1)\n",
    "    loss_val = loss(beta, b, xTr_test, yTr_test, xTr_test, yTr_test, 0, 'rbf')\n",
    "    loss_val_grader = h.loss_grader(beta, b, xTr_test, yTr_test, xTr_test, yTr_test, 0, 'rbf')\n",
    "    \n",
    "    return (np.linalg.norm(loss_val - loss_val_grader) < 1e-5)\n",
    "\n",
    "loss_test3()\n",
    "\n",
    "# Check whether you implement square hinge loss correctly\n",
    "def loss_test4():\n",
    "    beta = np.zeros(n)\n",
    "    b = np.random.rand(1)\n",
    "    loss_val = loss(beta, b, xTr_test, yTr_test, xTr_test, yTr_test, 10, 'rbf')\n",
    "    loss_val_grader = h.loss_grader(beta, b, xTr_test, yTr_test, xTr_test, yTr_test, 10, 'rbf')\n",
    "    \n",
    "    return (np.linalg.norm(loss_val - loss_val_grader) < 1e-5)\n",
    "\n",
    "# Check whether you implement square hinge loss correctly\n",
    "def loss_test5():\n",
    "    beta = np.zeros(n)\n",
    "    b = np.random.rand(1)\n",
    "    loss_val = loss(beta, b, xTr_test, yTr_test, xTr_test, yTr_test, 10, 'rbf')\n",
    "    loss_val_grader = h.loss_grader(beta, b, xTr_test, yTr_test, xTr_test, yTr_test, 10, 'rbf')\n",
    "    \n",
    "    return (np.linalg.norm(loss_val - loss_val_grader) < 1e-5)\n",
    "\n",
    "# Check whether you implement loss correctly\n",
    "def loss_test6():\n",
    "    beta = np.zeros(n)\n",
    "    b = np.random.rand(1)\n",
    "    loss_val = loss(beta, b, xTr_test, yTr_test, xTr_test, yTr_test, 100, 'rbf')\n",
    "    loss_val_grader = h.loss_grader(beta, b, xTr_test, yTr_test, xTr_test, yTr_test, 100, 'rbf')\n",
    "    \n",
    "    return (np.linalg.norm(loss_val - loss_val_grader) < 1e-5)\n",
    "\n",
    "h.runtest(loss_test1,'loss_test1')\n",
    "h.runtest(loss_test2,'loss_test2')\n",
    "h.runtest(loss_test3,'loss_test3')\n",
    "h.runtest(loss_test4,'loss_test4')\n",
    "h.runtest(loss_test5,'loss_test5')\n",
    "h.runtest(loss_test6,'loss_test6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e8a8bf4f84c63257cc5027f369297d0c",
     "grade": true,
     "grade_id": "cell-loss-test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs loss_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8c5cb3e3eadf76dd90def267e635eb40",
     "grade": true,
     "grade_id": "cell-loss-test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs loss test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e9e6086392f22c50444dadf2159d2c3a",
     "grade": true,
     "grade_id": "cell-loss-test3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs loss test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0ada7379e6811b8d2ed8c9a13a469f08",
     "grade": true,
     "grade_id": "cell-loss-test4",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs loss test4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "02b8b483e19d5c129f7cc8f2c3cd495a",
     "grade": true,
     "grade_id": "cell-loss-test5",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs loss test5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "52d0e4ff5205be1aa4a08460ca8f211f",
     "grade": true,
     "grade_id": "cell-loss-test6",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs loss test6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b8dc6228f0a44ef44ad14fcd3e470c25",
     "grade": false,
     "grade_id": "cell-1d2bd7b8103367e8",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def grad(beta, b, xTr, yTr, xTe, yTe, C, kerneltype, kpar=1):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    beta : n dimensional vector that stores the linear combination coefficient\n",
    "    xTr   : nxd dimensional matrix (training set, each row is an input vector)\n",
    "    yTr   : n   dimensional vector (training label, each entry is a label)\n",
    "    b     : scalar (bias)\n",
    "    xTe   : mxd dimensional matrix (test set, each row is an input vector)\n",
    "    yTe   : m dimensional vector (test label, each entry is a label)\n",
    "    C     : scalar (constant that controls the tradeoff between l2-regularizer and hinge-loss)\n",
    "    kerneltype: either 'linear','polynomial','rbf'\n",
    "    kpar  : kernel parameter (inverse kernel width gamma in case of RBF, degree in case of polynomial)\n",
    "    \n",
    "    \n",
    "    OUTPUTS:\n",
    "    beta_grad :  n dimensional vector (the gradient of the hinge loss with respect to the alphas)\n",
    "    bgrad :  constant (the gradient of he hinge loss with respect to the bias, b)\n",
    "    \"\"\"\n",
    "    \n",
    "    n, d = xTr.shape\n",
    "    \n",
    "    beta_grad = np.zeros(n)\n",
    "    bgrad = np.zeros(1)\n",
    "    \n",
    "    # compute the kernel values between xTr and xTr \n",
    "    kernel_train = computeK(kerneltype, xTr, xTr, kpar)\n",
    "    # compute the kernel values between xTe and xTr\n",
    "    kernel_test = computeK(kerneltype, xTe, xTr, kpar)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return beta_grad, bgrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cf9795d8c76b00a9995c479db7f78308",
     "grade": false,
     "grade_id": "cell-89ba8bb08ba592a6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# These tests test whether your grad() is implemented correctly\n",
    "\n",
    "xTr_test, yTr_test = h.generate_data()\n",
    "n, d = xTr_test.shape\n",
    "\n",
    "# Checks whether grad returns a tuple\n",
    "def grad_test1():\n",
    "    beta = np.random.rand(n)\n",
    "    b = np.random.rand(1)\n",
    "    out = grad(beta, b, xTr_test, yTr_test, xTr_test, yTr_test, 10, 'rbf')\n",
    "    return len(out) == 2\n",
    "\n",
    "# Checks the dimension of gradients\n",
    "def grad_test2():\n",
    "    beta = np.random.rand(n)\n",
    "    b = np.random.rand(1)\n",
    "    beta_grad, bgrad = grad(beta, b, xTr_test, yTr_test, xTr_test, yTr_test, 10, 'rbf')\n",
    "    return len(beta_grad) == n and np.isscalar(bgrad)\n",
    "\n",
    "# Checks the gradient of the l2 regularizer\n",
    "def grad_test3():\n",
    "    beta = np.random.rand(n)\n",
    "    b = np.random.rand(1)\n",
    "    beta_grad, bgrad = grad(beta, b, xTr_test, yTr_test, xTr_test, yTr_test, 0, 'rbf')\n",
    "    beta_grad_grader, bgrad_grader = h.grad_grader(beta, b, xTr_test, yTr_test, xTr_test, yTr_test, 0, 'rbf')\n",
    "    return (np.linalg.norm(beta_grad - beta_grad_grader) < 1e-5) and \\\n",
    "        (np.linalg.norm(bgrad - bgrad_grader) < 1e-5)\n",
    "\n",
    "# Checks the gradient of the square hinge loss\n",
    "def grad_test4():\n",
    "    beta = np.zeros(n)\n",
    "    b = np.random.rand(1)\n",
    "    beta_grad, bgrad = grad(beta, b, xTr_test, yTr_test, xTr_test, yTr_test, 1, 'rbf')\n",
    "    beta_grad_grader, bgrad_grader = h.grad_grader(beta, b, xTr_test, yTr_test, xTr_test, yTr_test, 1, 'rbf')\n",
    "    return (np.linalg.norm(beta_grad - beta_grad_grader) < 1e-5) and \\\n",
    "        (np.linalg.norm(bgrad - bgrad_grader) < 1e-5)\n",
    "\n",
    "# Checks the gradient of the loss\n",
    "def grad_test5():\n",
    "    beta = np.random.rand(n)\n",
    "    b = np.random.rand(1)\n",
    "    beta_grad, bgrad = grad(beta, b, xTr_test, yTr_test, xTr_test, yTr_test, 10, 'rbf')\n",
    "    beta_grad_grader, bgrad_grader = h.grad_grader(beta, b, xTr_test, yTr_test, xTr_test, yTr_test, 10, 'rbf')\n",
    "    return (np.linalg.norm(beta_grad - beta_grad_grader) < 1e-5) and \\\n",
    "        (np.linalg.norm(bgrad - bgrad_grader) < 1e-5)\n",
    "\n",
    "h.runtest(grad_test1, 'grad_test1')\n",
    "h.runtest(grad_test2, 'grad_test2')\n",
    "h.runtest(grad_test3, 'grad_test3')\n",
    "h.runtest(grad_test4, 'grad_test4')\n",
    "h.runtest(grad_test5, 'grad_test5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e6a5e50adcf9780a271d1d05c561fdaf",
     "grade": true,
     "grade_id": "cell-grad-test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs grad test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9092ccd637ef0692daf56b2d73f025a4",
     "grade": true,
     "grade_id": "cell-grad-test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs grad test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "49eef008dacc9d50d1e58f47be24cceb",
     "grade": true,
     "grade_id": "cell-grad-test3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs grad test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f0f17b5b6aa3d7e8cd72fb7fc4601e18",
     "grade": true,
     "grade_id": "cell-grad-test4",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs grad test4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9868abc360f647a4d380736d02d8dc84",
     "grade": true,
     "grade_id": "cell-grad-test5",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs grad test5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cf8975e5a0ac9d066e2e6f251d4605aa",
     "grade": false,
     "grade_id": "cell-7ad9af17accaddf0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now, you are going to call the optimization routine that we have implemented for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a5f296a9ee503acac3e4a8330467fead",
     "grade": false,
     "grade_id": "cell-fa26a32043f70279",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f2b3d3430e92a26e705307077e1e9826",
     "grade": false,
     "grade_id": "cell-8e81c269f495b733",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "If everything is implemented correctly, you should be able to get a training error of zero when you run the cell below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fd399d2efdadd3da916d754190cd296e",
     "grade": false,
     "grade_id": "cell-20ca56dcce47a510",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "svmclassify = lambda x: np.sign(computeK('linear', x, xTr, 1).dot(beta_sol) + bias_sol)\n",
    "\n",
    "predsTr=svmclassify(xTr)\n",
    "trainingerr=np.mean(np.sign(predsTr)!=yTr)\n",
    "print(\"Training error: %2.4f\" % trainingerr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "382e1a21319709d06962074df062f11e",
     "grade": false,
     "grade_id": "cell-d7214490d41e9d20",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Also, when you visualize the classifier, you should see a max margin separator!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3defee7515328f6ecb63beae2cfe2d7b",
     "grade": false,
     "grade_id": "cell-e0f813d39a20941d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "h.visclassifier(svmclassify, xTr, yTr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "027af6a58691af2caaf67653ac1a418b",
     "grade": false,
     "grade_id": "cell-f922d756ef836ee9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now, we are going to try another nonlinear dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "92e4f956ac53612f00f27f8aebfaabd8",
     "grade": false,
     "grade_id": "cell-c69bc918b0db1293",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "xTr_spiral,yTr_spiral,xTe_spiral,yTe_spiral = h.spiraldata()\n",
    "\n",
    "%matplotlib notebook\n",
    "h.visualize_2D(xTr_spiral, yTr_spiral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "93a4e8c432f6339646135c596f1fc3d2",
     "grade": false,
     "grade_id": "cell-89594e6378c785cb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Since the dataset is nonlinear, we are going to use the rbf kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e400463926a45fbefcec25740650111b",
     "grade": false,
     "grade_id": "cell-f4114c6db1b51e2c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "beta_sol_spiral, bias_sol_spiral, final_loss_spiral = h.minimize(objective=loss, grad=grad, xTr=xTr_spiral, yTr=yTr_spiral, C=100, kerneltype='rbf', kpar=1)\n",
    "print('The Final Loss of your model is: {:0.4f}'.format(final_loss_spiral))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3024813dcdb212a3e6c40b87b6128d64",
     "grade": false,
     "grade_id": "cell-28c6cee7c68d04e9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "If you do everything correctly, your training error and test error should both be zero!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "25a221d777e6a1d4838b9449fd33c46e",
     "grade": false,
     "grade_id": "cell-16357e4762b8370b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "svmclassify_spiral = lambda x: np.sign(computeK('rbf', xTr_spiral, x, 1).transpose().dot(beta_sol_spiral) + bias_sol_spiral)\n",
    "\n",
    "predsTr_spiral = svmclassify_spiral(xTr_spiral)\n",
    "trainingerr_spiral = np.mean(predsTr_spiral != yTr_spiral)\n",
    "print(\"Training error: %2.4f\" % trainingerr_spiral)\n",
    "\n",
    "predsTe_spiral = svmclassify_spiral(xTe_spiral)\n",
    "testerr_spiral = np.mean(predsTe_spiral != yTe_spiral)\n",
    "print(\"Test error: %2.4f\" % testerr_spiral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3457199bf2ebe8866cbbc23403ca0d81",
     "grade": false,
     "grade_id": "cell-52cb1b9aae3993cb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now, let's visualize the classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4b23f96d3b31ef5310872efe146a9d3c",
     "grade": false,
     "grade_id": "cell-40d1227ae7bf20e6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "h.visclassifier(svmclassify_spiral, xTr_spiral, yTr_spiral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
