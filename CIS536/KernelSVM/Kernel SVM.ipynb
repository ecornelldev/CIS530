{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>About this Project</h2>\n",
    "<p>In this project, you will implement a kernelized SVM (support vector machine). You will generate linearly separable and non-linearly separable datasets, write functions to SVMs that support a variety of different kernels, and then visualize the decision boundary created.</p>\n",
    "\n",
    "<h3>Evaluation</h3>\n",
    "\n",
    "<p><strong>This project must be successfully completed and submitted in order to receive credit for this course. Your score on this project will be included in your final grade calculation.</strong><p>\n",
    "    \n",
    "<p>You are expected to write code where you see <em># YOUR CODE HERE</em> within the cells of this notebook. Not all cells will be graded; code input cells followed by cells marked with <em>#Autograder test cell</em> will be graded. Upon submitting your work, the code you write at these designated positions will be assessed using an \"autograder\" that will run all test cells to assess your code. You will receive feedback from the autograder that will identify any errors in your code. Use this feedback to improve your code if you need to resubmit. Be sure not to change the names of any provided functions, classes, or variables within the existing code cells, as this will interfere with the autograder. Also, remember to execute all code cells sequentially, not just those you’ve edited, to ensure your code runs properly.</p>\n",
    "    \n",
    "<p>You can resubmit your work as many times as necessary before the submission deadline. If you experience difficulty or have questions about this exercise, use the Q&A discussion board to engage with your peers or seek assistance from the instructor.<p>\n",
    "\n",
    "<p>Before starting your work, please review <a href=\"https://s3.amazonaws.com/ecornell/global/eCornellPlagiarismPolicy.pdf\">eCornell's policy regarding plagiarism</a> (the presentation of someone else's work as your own without source credit).</p>\n",
    "\n",
    "<h3>Submit Code for Autograder Feedback</h3>\n",
    "\n",
    "<p>Once you have completed your work on this notebook, you will submit your code for autograder review. Follow these steps:</p>\n",
    "\n",
    "<ol>\n",
    "  <li><strong>Save your notebook.</strong></li>\n",
    "  <li><strong>Mark as Completed —</strong> In the blue menu bar along the top of this code exercise window, you’ll see a menu item called <strong>Education</strong>. In the <strong>Education</strong> menu, click <strong>Mark as Completed</strong> to submit your code for autograder/instructor review. This process will take a moment and a progress bar will show you the status of your submission.</li>\n",
    "\t<li><strong>Review your results —</strong> Once your work is marked as complete, the results of the autograder will automatically be presented in a new tab within the code exercise window. You can click on the assessment name in this feedback window to see more details regarding specific feedback/errors in your code submission.</li>\n",
    "  <li><strong>Repeat, if necessary —</strong> The Jupyter notebook will always remain accessible in the first tabbed window of the exercise. To reattempt the work, you will first need to click <strong>Mark as Uncompleted</strong> in the <strong>Education</strong> menu and then proceed to make edits to the notebook. Once you are ready to resubmit, follow steps one through three. You can repeat this procedure as many times as necessary.</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Getting Started</h2>\n",
    "<h3>Python Initialization</h3> \n",
    "\n",
    "Please run the following code to initialize your Python kernel. You should be running a version of Python 3.x. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6da27a81ca807569",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import helper as h\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-38378dba31605ea0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Generate and Visualize Data\n",
    "\n",
    "Before we start, let's generate some data and visualize the training set. We are going to use the linearly separable data that we used in our previous project!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-38e09fdf6da3e10b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "xTr,yTr = h.generate_data()\n",
    "h.visualize_2D(xTr, yTr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4a8d3426274ccdbd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Kernel SVM\n",
    "\n",
    "In this assignment, you need to implement three functions <code>computeK</code> that computes the kernel function efficiently, <code>loss</code> that calculates the unconstrained kernelized hinge loss and <code>grad</code> that calculates the gradient of the loss with respect to $\\mathbf{\\beta}, b$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c36368930d78904c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part One: Compute K [Graded]\n",
    "\n",
    "In <code>computeK</code>, you are going to calcuate the values of different kernel functions given inputs `X` and `Z`. You will return $K = K(X,Z)$. $K_{ij} = k(\\mathbf x_i, \\mathbf z_j)$ where $k( \\mathbf x_i, \\mathbf z_j) = \\phi(\\mathbf x_i)^T\\phi(\\mathbf z_j)$. This function takes in the parameter `kerneltype` to decide which of the three different kernel functions to evaluate. To review, recall the following types of kernels:\n",
    "\n",
    "- Linear: $K(X,Z) = X^TZ$\n",
    "- Polynomial: $K(X, Z)=\\left(X^{T} Z+1\\right)^{d}$ where `kpar` = $d$\n",
    "- RBF: $K(X,Z) = \\exp\\{-\\frac{||X-Z||^2}{\\sigma^2}\\}$ where `kpar` = $\\frac{1}{\\sigma^2}$\n",
    "\n",
    "Note that when calculating the RBF kernel, you can use the <code>h.l2distance(X, Z)</code> functions to calculate the pairwise l2 distance efficiently.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-54020608e1b5747d",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def computeK(kerneltype, X, Z, kpar=1):\n",
    "    \"\"\"\n",
    "    function K = computeK(kernel_type, X, Z)\n",
    "    computes a matrix K such that Kij=k(x,z);\n",
    "    for three different function linear, rbf or polynomial.\n",
    "    \n",
    "    Input:\n",
    "    kerneltype: either 'linear','polynomial','rbf'\n",
    "    X: n input vectors of dimension d (nxd);\n",
    "    Z: m input vectors of dimension d (mxd);\n",
    "    kpar: kernel parameter (inverse kernel width gamma in case of RBF, degree in case of polynomial)\n",
    "    \n",
    "    OUTPUT:\n",
    "    K : nxm kernel matrix\n",
    "    \"\"\"\n",
    "    assert kerneltype in [\"linear\",\"polynomial\",\"rbf\"], \"Kernel type %s not known.\" % kerneltype\n",
    "    assert X.shape[1] == Z.shape[1], \"Input dimensions do not match\"\n",
    "    \n",
    "    K = None\n",
    "    \n",
    "    ## BEGIN SOLUTION\n",
    "    if kerneltype == \"linear\":\n",
    "        K = X.dot(Z.T)\n",
    "    elif kerneltype == \"polynomial\":\n",
    "        K = np.power((X.dot(Z.T) + 1), kpar)\n",
    "    elif kerneltype =='rbf':\n",
    "        K = np.exp(-kpar*np.square(h.l2distance(X,Z)))\n",
    "    else:\n",
    "        raise ValueError('Invalid Kernel Type!')\n",
    "    ## END SOLUTION\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-18c9391e0771d3f3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# These tests test whether your computeK() is implemented correctly\n",
    "\n",
    "xTr_test, yTr_test = h.generate_data(100)\n",
    "xTr_test2, yTr_test2 = h.generate_data(50)\n",
    "n, d = xTr_test.shape\n",
    "\n",
    "# Checks whether computeK compute the kernel matrix with the right dimension\n",
    "def computeK_test1():\n",
    "    s1 = (computeK('rbf', xTr_test, xTr_test2, kpar=1).shape == (100, 50))\n",
    "    s2 = (computeK('polynomial', xTr_test, xTr_test2, kpar=1).shape == (100, 50))\n",
    "    s3 = (computeK('linear', xTr_test, xTr_test2, kpar=1).shape == (100, 50))\n",
    "    return (s1 and s2 and s3)\n",
    "\n",
    "# Checks whether the kernel matrix is symmetric\n",
    "def computeK_test2():\n",
    "    k_rbf = computeK('rbf', xTr_test, xTr_test, kpar=1)\n",
    "    s1 = np.allclose(k_rbf, k_rbf.T)\n",
    "    k_poly = computeK('polynomial', xTr_test, xTr_test, kpar=1)\n",
    "    s2 = np.allclose(k_poly, k_poly.T)\n",
    "    k_linear = computeK('linear', xTr_test, xTr_test, kpar=1)\n",
    "    s3 = np.allclose(k_linear, k_linear.T)\n",
    "    return (s1 and s2 and s3)\n",
    "\n",
    "# Checks whether the kernel matrix is positive semi-definite\n",
    "def computeK_test3():\n",
    "    k_rbf = computeK('rbf', xTr_test2, xTr_test2, kpar=1)\n",
    "    eigen_rbf = np.linalg.eigvals(k_rbf)\n",
    "    eigen_rbf[np.isclose(eigen_rbf, 0)] = 0\n",
    "    s1 = np.all(eigen_rbf >= 0)\n",
    "    k_poly = computeK('polynomial', xTr_test2, xTr_test2, kpar=1)\n",
    "    eigen_poly = np.linalg.eigvals(k_poly)\n",
    "    eigen_poly[np.isclose(eigen_poly, 0)] = 0\n",
    "    s2 = np.all(eigen_poly >= 0)\n",
    "    k_linear = computeK('linear', xTr_test2, xTr_test2, kpar=1)\n",
    "    eigen_linear = np.linalg.eigvals(k_linear)\n",
    "    eigen_linear[np.isclose(eigen_linear, 0)] = 0\n",
    "    s3 = np.all(eigen_linear >= 0)\n",
    "    return (s1 and s2 and s3)\n",
    "\n",
    "# Checks whether computeK compute the right kernel matrix with rbf kernel\n",
    "def computeK_test4():\n",
    "    k = computeK('rbf', xTr_test, xTr_test2, kpar=1)\n",
    "    k2 = h.computeK_grader('rbf', xTr_test, xTr_test2, kpar=1)\n",
    "    \n",
    "    return np.linalg.norm(k - k2) < 1e-5\n",
    "\n",
    "# Checks whether computeK compute the right kernel matrix with polynomial kernel\n",
    "def computeK_test5():\n",
    "    k = computeK('polynomial', xTr_test, xTr_test2, kpar=1)\n",
    "    k2 = h.computeK_grader('polynomial', xTr_test, xTr_test2, kpar=1)\n",
    "    \n",
    "    return np.linalg.norm(k - k2) < 1e-5\n",
    "\n",
    "# Checks whether computeK compute the right kernel matrix with linear kernel\n",
    "def computeK_test6():\n",
    "    k = computeK('linear', xTr_test, xTr_test2, kpar=1)\n",
    "    k2 = h.computeK_grader('linear', xTr_test, xTr_test2, kpar=1)\n",
    "    \n",
    "    return np.linalg.norm(k - k2) < 1e-5\n",
    "\n",
    "\n",
    "h.runtest(computeK_test1, 'computeK_test1')\n",
    "h.runtest(computeK_test2, 'computeK_test2')\n",
    "h.runtest(computeK_test3, 'computeK_test3')\n",
    "h.runtest(computeK_test4, 'computeK_test4')\n",
    "h.runtest(computeK_test5, 'computeK_test5')\n",
    "h.runtest(computeK_test5, 'computeK_test6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-computeK-test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs computeK_test1\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "xTr_test, yTr_test = h.generate_data(100)\n",
    "xTr_test2, yTr_test2 = h.generate_data(50)\n",
    "n, d = xTr_test.shape\n",
    "\n",
    "# Checks whether computeK compute the kernel matrix with the right dimension\n",
    "s1 = (computeK('rbf', xTr_test, xTr_test2, kpar=1).shape == (100, 50))\n",
    "s2 = (computeK('polynomial', xTr_test, xTr_test2, kpar=1).shape == (100, 50))\n",
    "s3 = (computeK('linear', xTr_test, xTr_test2, kpar=1).shape == (100, 50))\n",
    "assert (s1 and s2 and s3)\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-computeK-test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs computeK_test2\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "xTr_test, yTr_test = h.generate_data(100)\n",
    "xTr_test2, yTr_test2 = h.generate_data(50)\n",
    "n, d = xTr_test.shape\n",
    "\n",
    "# Checks whether the kernel matrix is symmetric\n",
    "k_rbf = computeK('rbf', xTr_test, xTr_test, kpar=1)\n",
    "s1 = np.allclose(k_rbf, k_rbf.T)\n",
    "k_poly = computeK('polynomial', xTr_test, xTr_test, kpar=1)\n",
    "s2 = np.allclose(k_poly, k_poly.T)\n",
    "k_linear = computeK('linear', xTr_test, xTr_test, kpar=1)\n",
    "s3 = np.allclose(k_linear, k_linear.T)\n",
    "assert (s1 and s2 and s3)\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-computeK-test3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs computeK_test3\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "xTr_test, yTr_test = h.generate_data(100)\n",
    "xTr_test2, yTr_test2 = h.generate_data(50)\n",
    "n, d = xTr_test.shape\n",
    "\n",
    "k_rbf = computeK('rbf', xTr_test2, xTr_test2, kpar=1)\n",
    "eigen_rbf = np.linalg.eigvals(k_rbf)\n",
    "eigen_rbf[np.isclose(eigen_rbf, 0)] = 0\n",
    "s1 = np.all(eigen_rbf >= 0)\n",
    "k_poly = computeK('polynomial', xTr_test2, xTr_test2, kpar=1)\n",
    "eigen_poly = np.linalg.eigvals(k_poly)\n",
    "eigen_poly[np.isclose(eigen_poly, 0)] = 0\n",
    "s2 = np.all(eigen_poly >= 0)\n",
    "k_linear = computeK('linear', xTr_test2, xTr_test2, kpar=1)\n",
    "eigen_linear = np.linalg.eigvals(k_linear)\n",
    "eigen_linear[np.isclose(eigen_linear, 0)] = 0\n",
    "s3 = np.all(eigen_linear >= 0)\n",
    "assert (s1 and s2 and s3)\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-computeK-test4",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs computeK_test4\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "xTr_test, yTr_test = h.generate_data(100)\n",
    "xTr_test2, yTr_test2 = h.generate_data(50)\n",
    "n, d = xTr_test.shape\n",
    "\n",
    "# Checks whether computeK compute the right kernel matrix with rbf kernel\n",
    "k = computeK('rbf', xTr_test, xTr_test2, kpar=1)\n",
    "k2 = h.computeK_grader('rbf', xTr_test, xTr_test2, kpar=1)\n",
    "\n",
    "assert np.linalg.norm(k - k2) < 1e-5\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-computeK-test5",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs computeK_test5\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "xTr_test, yTr_test = h.generate_data(100)\n",
    "xTr_test2, yTr_test2 = h.generate_data(50)\n",
    "n, d = xTr_test.shape\n",
    "\n",
    "# Checks whether computeK compute the right kernel matrix with rbf kernel\n",
    "k = computeK('polynomial', xTr_test, xTr_test2, kpar=1)\n",
    "k2 = h.computeK_grader('polynomial', xTr_test, xTr_test2, kpar=1)\n",
    "\n",
    "assert np.linalg.norm(k - k2) < 1e-5\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-computeK-test6",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs computeK_test5\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "xTr_test, yTr_test = h.generate_data(100)\n",
    "xTr_test2, yTr_test2 = h.generate_data(50)\n",
    "n, d = xTr_test.shape\n",
    "\n",
    "# Checks whether computeK compute the right kernel matrix with rbf kernel\n",
    "k = computeK('linear', xTr_test, xTr_test2, kpar=1)\n",
    "k2 = h.computeK_grader('linear', xTr_test, xTr_test2, kpar=1)\n",
    "\n",
    "assert np.linalg.norm(k - k2) < 1e-5\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7a01c840a42c22ae",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part Two: Compute Loss [Graded]\n",
    "\n",
    "Now, you are going to implement the function <code>loss</code>. Note that the function signature of this function is slightly different from the previous assignment. Previously, we can pass in $\\mathbf{w}$ to calculate the loss values. However, in the kernelized version of the loss, $\\mathbf{w}$ is calculated as a linear combination of the training examples, so we need to pass the training set to the function. \n",
    "\n",
    "The following is an explanation of the theory that leads us to the loss expression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5b36c08fbebe600d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Recall that we can express the solution as a linear combination of the training examples $ \\{\\phi(\\mathbf{x}_1), \\cdots, \\phi(\\mathbf{x}_n) \\}$:\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "    \\mathbf{w} = \\sum_{j = 1}^n \\alpha_j y_j \\phi(\\mathbf{x}_j)\n",
    "    \\end{aligned}\n",
    "$$\n",
    "\n",
    "To simplify, we define $\\beta_j = \\alpha_j y_j$ and we can express the solution as: \n",
    "$$\n",
    "    \\begin{aligned}\n",
    "    \\mathbf{w} = \\sum_{j = 1}^n \\beta_j \\phi(\\mathbf{x}_j)\n",
    "    \\end{aligned}\n",
    "$$\n",
    "\n",
    "If we substitute the formulation above into our unconstrained regularized squared hinge loss, \n",
    "$$\n",
    "    \\begin{aligned}\n",
    "    \\min_{\\mathbf{w},b}\\underbrace{\\mathbf{w}^T\\mathbf{w}}_{l_{2} \\text{ regularizer} } +  C\\  \\sum_{i=1}^{n}\\underbrace{\\max\\left [ 1-y_{i}(\\mathbf{w}^T \\mathbf{x}_i+b),0 \\right ] ^2}_{ \\text{squared hinge loss} }\n",
    "    \\end{aligned}\n",
    "$$\n",
    "\n",
    "we obtain: \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "       \\underbrace{\\sum_{i = 1} ^n \\sum_{j = 1}^n \\beta_i  \\beta_j \\phi(\\mathbf{x} _i)^T \\phi(\\mathbf{x}_j)}_{l_{2} \\text{ regularizer}} +  C\\  \\sum_{i=1}^{n}\\underbrace{\\max\\left [ 1-y_{i}(\\sum_{j = 1}^n \\beta_j \\phi(\\mathbf{x}_j)^T \\phi(\\mathbf{x}_i)+b),0 \\right ] ^ 2}_{\\text{squared hinge loss}}     \n",
    "\\end{aligned}\n",
    "$$\n",
    "Now, if we replace all the $\\phi(\\mathbf{x} _i)^T \\phi(\\mathbf{x}_j)$ with $k(\\mathbf{x}_i, \\mathbf{x}_j)$, we obtained the \"kernelized\" hinge loss. \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "      \\min_{\\beta_1,\\cdots,\\beta_n, b} \\underbrace{\\sum_{i = 1} ^n \\sum_{j = 1}^n \\beta_i \\beta_j k(\\mathbf{x} _i, \\mathbf{x}_j)}_{l_{2} \\text{ regularizer}} +  C\\  \\sum_{i=1}^{n}\\underbrace{\\max\\left [ 1-y_{i}(\\sum_{j = 1}^n \\beta_j k(\\mathbf{x}_j, \\mathbf{x}_i)+b),0 \\right ] ^ 2}_{\\text{squared hinge loss}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The expression above seems a little bit unweildy, so we are going to simplify the expression again. First, we will simplfy the $l_{2}$ regularizer. Define $\\mathbf{\\beta} = [\\beta_1, \\cdots, \\beta_n]^T$ and $K_{train}$ to be the kernel matrix calculated on the training set, namely, the (i, j)-th entry of $K_{train}$ is $k(\\mathbf{x} _i, \\mathbf{x}_j)$. It is easy to verify that the $l_{2}$ regularizer can be expressed as the quadratic form  \n",
    "\n",
    "$$\n",
    "    \\sum_{i = 1} ^n \\sum_{j = 1}^n \\beta_i \\beta_j k(\\mathbf{x} _i, \\mathbf{x}_j) = \\mathbf{\\beta}^T K_{train} \\mathbf{\\beta}\n",
    "$$\n",
    "\n",
    "Now, let's simplify the squared hinge loss. To start, we define $K_{train}[i]$ as the i-th row of $k_{train}$ expressed as a column vector. Now, the summation term within the hinge loss can be expressed as: \n",
    "\n",
    "$$\n",
    "    \\sum_{ j = 1}^n \\beta_j k(\\mathbf{x}_j, \\mathbf{x}_i) = \\mathbf{\\beta}^T K_{train}[i]\n",
    "$$\n",
    "\n",
    "Combining the two simplifications we have, we arrive at the following expression for the loss function: \n",
    "$$\n",
    "\\begin{aligned}\n",
    "      \\min_{\\mathbf{\\beta}, b} \\underbrace{\\mathbf{\\beta}^T K_{train} \\mathbf{\\beta}}_{l_{2}  \\text{ regularizer}} +  C\\  \\sum_{i=1}^{n}\\underbrace{\\max\\left [ 1-y_{i}(\\mathbf{\\beta}^T K_{train}[i]+b),0 \\right ] ^ 2}_{\\text{squared hinge loss}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "After we minimize this expression and have found a good $\\beta$ that works on the kernel generated from the training data, $K_{train}$, we can test using our $\\beta$ and calculating the loss against testing data: \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "      \\min_{\\mathbf{\\beta}, b} \\underbrace{\\mathbf{\\beta}^T K_{train} \\mathbf{\\beta}}_{l_{2}  \\text{ regularizer}} +  C\\  \\sum_{i=1}^{n}\\underbrace{\\max\\left [ 1-y_{i}(\\mathbf{\\beta}^T K_{test}[i]+b),0 \\right ] ^ 2}_{\\text{squared hinge loss}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Note that the loss function we have above is very similar to the vanilla linear SVM, the differences are: \n",
    "\n",
    "1. Instead of $\\mathbf{w}$, we have $\\mathbf{\\beta}$\n",
    "2. The $l_{2}$ regularizer $\\mathbf{w}^T \\mathbf{w} = \\mathbf{w}^T I \\mathbf{w}$ is changed to $\\mathbf{\\beta}^T K_{train} \\mathbf{\\beta}$ to account for using $\\mathbf{\\beta}$ instead of $\\mathbf{w}$\n",
    "3. The inner product $\\mathbf{w}^T \\mathbf{x}_i$ in the hinge loss is changed to $\\mathbf{\\beta}^T K_{train}[i]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c7a3782fe360f7c1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "This is useful because we never need to explicitly calculate $\\phi(\\mathbf x_i)$, which can be exponential in size, or in the case of RBF, have an infinite number of elements. On the other hand, $K$ will be a $m x n$ matrices that encapsulates all the data you would gain from $\\phi$. \n",
    "\n",
    "Note that in the following code you are passing in both training data (`xTr`, `yTr`) and testing data (`xTe`, `yTr`) to calculate two kernels, `kernel_train` and `kernel_test` respectively. Unfortunately, when we're training an algorithm, we don't have access to test data! That would make the test data a very poor evaluator of the performance of the algorithm. This function is written so that passing **training data** into the parameters `xTe` and `yTe` will train the SVM on training data to find the best betas. During testing, passing in **test data** into those variables can be used to evaluate how good your algorithm is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ce269b51e709e965",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def loss(beta, b, xTr, yTr, xTe, yTe, C, kerneltype, kpar=1):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    beta : n dimensional vector that stores the linear combination coefficient\n",
    "    xTr   : nxd dimensional matrix (training set, each row is an input vector)\n",
    "    yTr   : n   dimensional vector (training label, each entry is a label)\n",
    "    b     : scalar (bias)\n",
    "    xTe   : mxd dimensional matrix (test set, each row is an input vector)\n",
    "    yTe   : m dimensional vector (test label, each entry is a label)\n",
    "    C     : scalar (constant that controls the tradeoff between l2-regularizer and hinge-loss)\n",
    "    kerneltype: either 'linear','polynomial','rbf'\n",
    "    kpar  : kernel parameter (inverse kernel width gamma in case of RBF, degree in case of polynomial)\n",
    "    \n",
    "    OUTPUTS:\n",
    "    loss     : the total loss obtained with (beta, xTr, yTr, b) on xTe and yTe (scalar)\n",
    "    \"\"\"\n",
    "    \n",
    "    loss_val = 0.0\n",
    "    # compute the kernel values between xTr and xTr \n",
    "    kernel_train = computeK(kerneltype, xTr, xTr, kpar)\n",
    "    # compute the kernel values between xTe and xTr\n",
    "    kernel_test = computeK(kerneltype, xTe, xTr, kpar)\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    prediction = kernel_test @ beta  + b\n",
    "    margin = yTe * prediction\n",
    "    \n",
    "    loss_val = beta @ kernel_train @ beta + C*(np.sum(np.maximum(1 - margin, 0) ** 2))\n",
    "    ### END SOLUTION\n",
    "    return loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-26c56a8bb3f4cd32",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# These tests test whether your loss() is implemented correctly\n",
    "\n",
    "xTr_test, yTr_test = h.generate_data()\n",
    "n, d = xTr_test.shape\n",
    "\n",
    "# Check whether your loss() returns a scalar\n",
    "def loss_test1():\n",
    "    beta = np.zeros(n)\n",
    "    b = np.zeros(1)\n",
    "    loss_val = loss(beta, b, xTr_test, yTr_test, xTr_test, yTr_test, 10, 'rbf')\n",
    "    \n",
    "    return np.isscalar(loss_val)\n",
    "\n",
    "\n",
    "# Check whether your loss() returns a nonnegative scalar\n",
    "def loss_test2():\n",
    "    beta = np.random.rand(n)\n",
    "    b = np.random.rand(1)\n",
    "    loss_val = loss(beta, b, xTr_test, yTr_test, xTr_test, yTr_test, 10, 'rbf')\n",
    "    \n",
    "    return loss_val >= 0\n",
    "\n",
    "# Check whether you implement l2-regularizer correctly\n",
    "def loss_test3():\n",
    "    beta = np.random.rand(n)\n",
    "    b = np.random.rand(1)\n",
    "    loss_val = loss(beta, b, xTr_test, yTr_test, xTr_test, yTr_test, 0, 'rbf')\n",
    "    loss_val_grader = h.loss_grader(beta, b, xTr_test, yTr_test, xTr_test, yTr_test, 0, 'rbf')\n",
    "    \n",
    "    return (np.linalg.norm(loss_val - loss_val_grader) < 1e-5)\n",
    "\n",
    "loss_test3()\n",
    "\n",
    "# Check whether you implement square hinge loss correctly\n",
    "def loss_test4():\n",
    "    beta = np.zeros(n)\n",
    "    b = np.random.rand(1)\n",
    "    loss_val = loss(beta, b, xTr_test, yTr_test, xTr_test, yTr_test, 10, 'rbf')\n",
    "    loss_val_grader = h.loss_grader(beta, b, xTr_test, yTr_test, xTr_test, yTr_test, 10, 'rbf')\n",
    "    \n",
    "    return (np.linalg.norm(loss_val - loss_val_grader) < 1e-5)\n",
    "\n",
    "# Check whether you implement square hinge loss correctly\n",
    "def loss_test5():\n",
    "    beta = np.zeros(n)\n",
    "    b = np.random.rand(1)\n",
    "    loss_val = loss(beta, b, xTr_test, yTr_test, xTr_test, yTr_test, 10, 'rbf')\n",
    "    loss_val_grader = h.loss_grader(beta, b, xTr_test, yTr_test, xTr_test, yTr_test, 10, 'rbf')\n",
    "    \n",
    "    return (np.linalg.norm(loss_val - loss_val_grader) < 1e-5)\n",
    "\n",
    "# Check whether you implement loss correctly\n",
    "def loss_test6():\n",
    "    beta = np.zeros(n)\n",
    "    b = np.random.rand(1)\n",
    "    loss_val = loss(beta, b, xTr_test, yTr_test, xTr_test, yTr_test, 100, 'rbf')\n",
    "    loss_val_grader = h.loss_grader(beta, b, xTr_test, yTr_test, xTr_test, yTr_test, 100, 'rbf')\n",
    "    \n",
    "    return (np.linalg.norm(loss_val - loss_val_grader) < 1e-5)\n",
    "\n",
    "h.runtest(loss_test1,'loss_test1')\n",
    "h.runtest(loss_test2,'loss_test2')\n",
    "h.runtest(loss_test3,'loss_test3')\n",
    "h.runtest(loss_test4,'loss_test4')\n",
    "h.runtest(loss_test5,'loss_test5')\n",
    "h.runtest(loss_test6,'loss_test6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-loss-test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs loss_test1\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "xTr_test, yTr_test = h.generate_data()\n",
    "n, d = xTr_test.shape\n",
    "\n",
    "beta = np.zeros(n)\n",
    "b = np.zeros(1)\n",
    "loss_val = loss(beta, b, xTr_test, yTr_test, xTr_test, yTr_test, 10, 'rbf')\n",
    "    \n",
    "assert np.isscalar(loss_val)\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-loss-test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs loss test2\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "xTr_test, yTr_test = h.generate_data()\n",
    "n, d = xTr_test.shape\n",
    "\n",
    "beta = np.random.rand(n)\n",
    "b = np.random.rand(1)\n",
    "loss_val = loss(beta, b, xTr_test, yTr_test, xTr_test, yTr_test, 10, 'rbf')\n",
    "assert loss_val >= 0\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-loss-test3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs loss test3beta @ kernel_train\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "xTr_test, yTr_test = h.generate_data()\n",
    "n, d = xTr_test.shape\n",
    "\n",
    "beta = np.random.rand(n)\n",
    "b = np.random.rand(1)\n",
    "loss_val = loss(beta, b, xTr_test, yTr_test, xTr_test, yTr_test, 0, 'rbf')\n",
    "loss_val_grader = h.loss_grader(beta, b, xTr_test, yTr_test, xTr_test, yTr_test, 0, 'rbf')\n",
    "    \n",
    "assert (np.linalg.norm(loss_val - loss_val_grader) < 1e-5)\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-loss-test4",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs loss test4\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "xTr_test, yTr_test = h.generate_data()\n",
    "n, d = xTr_test.shape\n",
    "\n",
    "beta = np.zeros(n)\n",
    "b = np.random.rand(1)\n",
    "loss_val = loss(beta, b, xTr_test, yTr_test, xTr_test, yTr_test, 10, 'rbf')\n",
    "loss_val_grader = h.loss_grader(beta, b, xTr_test, yTr_test, xTr_test, yTr_test, 10, 'rbf')\n",
    "\n",
    "assert (np.linalg.norm(loss_val - loss_val_grader) < 1e-5)\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-loss-test5",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs loss test5\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "xTr_test, yTr_test = h.generate_data()\n",
    "n, d = xTr_test.shape\n",
    "\n",
    "beta = np.zeros(n)\n",
    "b = np.random.rand(1)\n",
    "loss_val = loss(beta, b, xTr_test, yTr_test ,xTr_test, yTr_test, 10, 'rbf')\n",
    "loss_val_grader = h.loss_grader(beta, b, xTr_test, yTr_test, xTr_test, yTr_test, 10, 'rbf')\n",
    "\n",
    "assert (np.linalg.norm(loss_val - loss_val_grader) < 1e-5)\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-loss-test6",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs loss test6\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "xTr_test, yTr_test = h.generate_data()\n",
    "n, d = xTr_test.shape\n",
    "\n",
    "beta = np.zeros(n)\n",
    "b = np.random.rand(1)\n",
    "loss_val = loss(beta, b, xTr_test, yTr_test, xTr_test, yTr_test, 100, 'rbf')\n",
    "loss_val_grader = h.loss_grader(beta, b, xTr_test, yTr_test, xTr_test, yTr_test, 100, 'rbf')\n",
    "    \n",
    "assert (np.linalg.norm(loss_val - loss_val_grader) < 1e-5)\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-349f46a1eb425985",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part Three: Compute Gradient [Graded]\n",
    "\n",
    "Now, you will need to implement function `grad`, that computes the gradient of the loss function, similarly to what you needed to do in the Linear SVM project. This function has the same input parameters as `loss` and requires the gradient with respect to $\\beta$ (`beta_grad`) and $b$ (`bgrad`). Remember that the squared hinge loss is calculated with $K_{train}$ when training, and $K_{test}$ when testing. In the code, you can represent this by using the variables associated with testing (i.e. `xTe`, `yTe`, and `k_test`), since they will contain training data if you are training the SVM.\n",
    "\n",
    "Recall the expression for loss from above. First we take the derivative with respect to $\\beta$:\n",
    "$$\n",
    "      \\frac{\\partial \\mathcal L}{\\partial \\mathbf \\beta} =  2{K_{train} \\mathbf{\\beta}} +  C\\  \\sum_{i=1}^{n}{ 2 \\max\\left [ 1-y_{i}(\\mathbf{\\beta}^T K_{test}[i]+b),0 \\right ] (-y_i K_{test}[i])}\n",
    "$$\n",
    "Then, with respect to $b$:\n",
    "$$\n",
    "      \\frac{\\partial \\mathcal L}{\\delta b} =  C\\  \\sum_{i=1}^{n}{ 2 \\max\\left [ 1-y_{i}(\\mathbf{\\beta}^T K_{test}[i]+b),0 \\right ] (-y_i)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1d2bd7b8103367e8",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def grad(beta, b, xTr, yTr, xTe, yTe, C, kerneltype, kpar=1):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    beta : n dimensional vector that stores the linear combination coefficient\n",
    "    xTr   : nxd dimensional matrix (training set, each row is an input vector)\n",
    "    yTr   : n   dimensional vector (training label, each entry is a label)\n",
    "    b     : scalar (bias)\n",
    "    xTe   : mxd dimensional matrix (test set, each row is an input vector)\n",
    "    yTe   : m dimensional vector (test label, each entry is a label)\n",
    "    C     : scalar (constant that controls the tradeoff between l2-regularizer and hinge-loss)\n",
    "    kerneltype: either 'linear','polynomial','rbf'\n",
    "    kpar  : kernel parameter (inverse kernel width gamma in case of RBF, degree in case of polynomial)\n",
    "    \n",
    "    \n",
    "    OUTPUTS:\n",
    "    beta_grad :  n dimensional vector (the gradient of the hinge loss with respect to the alphas)\n",
    "    bgrad :  constant (the gradient of he hinge loss with respect to the bias, b)\n",
    "    \"\"\"\n",
    "    \n",
    "    n, d = xTr.shape\n",
    "    \n",
    "    beta_grad = np.zeros(n)\n",
    "    bgrad = np.zeros(1)\n",
    "    \n",
    "    # compute the kernel values between xTr and xTr \n",
    "    kernel_train = computeK(kerneltype, xTr, xTr, kpar)\n",
    "    # compute the kernel values between xTe and xTr\n",
    "    kernel_test = computeK(kerneltype, xTe, xTr, kpar)\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    prediction = kernel_test @ beta + b\n",
    "    margin = yTe * prediction\n",
    "    \n",
    "    hinge = np.maximum(1 - margin, 0)\n",
    "    indicator = ((1 - margin) > 0).astype(int)\n",
    "    \n",
    "    beta_grad = 2 * (kernel_train @ (beta)) + C * np.sum((2 * hinge * indicator * -yTe).reshape(-1, 1) * kernel_test, axis=0) \n",
    "    bgrad = C * np.sum(2 * hinge * indicator * - yTe, axis=0)\n",
    "    \n",
    "    ### END SOLUTION\n",
    "    return beta_grad, bgrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-89ba8bb08ba592a6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# These tests test whether your grad() is implemented correctly\n",
    "\n",
    "xTr_test, yTr_test = h.generate_data()\n",
    "n, d = xTr_test.shape\n",
    "\n",
    "# Checks whether grad returns a tuple\n",
    "def grad_test1():\n",
    "    beta = np.random.rand(n)\n",
    "    b = np.random.rand(1)\n",
    "    out = grad(beta, b, xTr_test, yTr_test, xTr_test, yTr_test, 10, 'rbf')\n",
    "    return len(out) == 2\n",
    "\n",
    "# Checks the dimension of gradients\n",
    "def grad_test2():\n",
    "    beta = np.random.rand(n)\n",
    "    b = np.random.rand(1)\n",
    "    beta_grad, bgrad = grad(beta, b, xTr_test, yTr_test, xTr_test, yTr_test, 10, 'rbf')\n",
    "    return len(beta_grad) == n and np.isscalar(bgrad)\n",
    "\n",
    "# Checks the gradient of the l2 regularizer\n",
    "def grad_test3():\n",
    "    beta = np.random.rand(n)\n",
    "    b = np.random.rand(1)\n",
    "    beta_grad, bgrad = grad(beta, b, xTr_test, yTr_test, xTr_test, yTr_test, 0, 'rbf')\n",
    "    beta_grad_grader, bgrad_grader = h.grad_grader(beta, b, xTr_test, yTr_test, xTr_test, yTr_test, 0, 'rbf')\n",
    "    return (np.linalg.norm(beta_grad - beta_grad_grader) < 1e-5) and \\\n",
    "        (np.linalg.norm(bgrad - bgrad_grader) < 1e-5)\n",
    "\n",
    "# Checks the gradient of the square hinge loss\n",
    "def grad_test4():\n",
    "    beta = np.zeros(n)\n",
    "    b = np.random.rand(1)\n",
    "    beta_grad, bgrad = grad(beta, b, xTr_test, yTr_test, xTr_test, yTr_test, 1, 'rbf')\n",
    "    beta_grad_grader, bgrad_grader = h.grad_grader(beta, b, xTr_test, yTr_test, xTr_test, yTr_test, 1, 'rbf')\n",
    "    return (np.linalg.norm(beta_grad - beta_grad_grader) < 1e-5) and \\\n",
    "        (np.linalg.norm(bgrad - bgrad_grader) < 1e-5)\n",
    "\n",
    "# Checks the gradient of the loss\n",
    "def grad_test5():\n",
    "    beta = np.random.rand(n)\n",
    "    b = np.random.rand(1)\n",
    "    beta_grad, bgrad = grad(beta, b, xTr_test, yTr_test, xTr_test, yTr_test, 10, 'rbf')\n",
    "    beta_grad_grader, bgrad_grader = h.grad_grader(beta, b, xTr_test, yTr_test, xTr_test, yTr_test, 10, 'rbf')\n",
    "    return (np.linalg.norm(beta_grad - beta_grad_grader) < 1e-5) and \\\n",
    "        (np.linalg.norm(bgrad - bgrad_grader) < 1e-5)\n",
    "\n",
    "h.runtest(grad_test1, 'grad_test1')\n",
    "h.runtest(grad_test2, 'grad_test2')\n",
    "h.runtest(grad_test3, 'grad_test3')\n",
    "h.runtest(grad_test4, 'grad_test4')\n",
    "h.runtest(grad_test5, 'grad_test5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-grad-test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs grad test1\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "xTr_test, yTr_test = h.generate_data()\n",
    "n, d = xTr_test.shape\n",
    "\n",
    "beta = np.random.rand(n)\n",
    "b = np.random.rand(1)\n",
    "out = grad(beta, b, xTr_test, yTr_test, xTr_test, yTr_test, 10, 'rbf')\n",
    "assert len(out) == 2\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-grad-test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs grad test2\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "xTr_test, yTr_test = h.generate_data()\n",
    "n, d = xTr_test.shape\n",
    "\n",
    "# Checks the dimension of gradients\n",
    "beta = np.random.rand(n)\n",
    "b = np.random.rand(1)\n",
    "beta_grad, bgrad = grad(beta, b, xTr_test, yTr_test,xTr_test, yTr_test, 10, 'rbf')\n",
    "assert len(beta_grad) == n and np.isscalar(bgrad)\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-grad-test3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs grad test3\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "xTr_test, yTr_test = h.generate_data()\n",
    "n, d = xTr_test.shape\n",
    "\n",
    "# Checks the gradient of the l2 regularizer\n",
    "beta = np.random.rand(n)\n",
    "b = np.random.rand(1)\n",
    "beta_grad, bgrad = grad(beta, b,  xTr_test, yTr_test, xTr_test, yTr_test, 0, 'rbf')\n",
    "beta_grad_grader, bgrad_grader = h.grad_grader(beta, b,  xTr_test, yTr_test, xTr_test, yTr_test, 0, 'rbf')\n",
    "assert (np.linalg.norm(beta_grad - beta_grad_grader) < 1e-5) and \\\n",
    "    (np.linalg.norm(bgrad - bgrad_grader) < 1e-5)\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-grad-test4",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs grad test4\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "xTr_test, yTr_test = h.generate_data()\n",
    "n, d = xTr_test.shape\n",
    "\n",
    "# Checks the gradient of the square hinge loss\n",
    "beta = np.zeros(n)\n",
    "b = np.random.rand(1)\n",
    "beta_grad, bgrad = grad(beta, b, xTr_test, yTr_test, xTr_test, yTr_test, 1, 'rbf')\n",
    "beta_grad_grader, bgrad_grader = h.grad_grader(beta, b, xTr_test, yTr_test, xTr_test, yTr_test, 1, 'rbf')\n",
    "assert (np.linalg.norm(beta_grad - beta_grad_grader) < 1e-5) and \\\n",
    "        (np.linalg.norm(bgrad - bgrad_grader) < 1e-5)\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-grad-test5",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs grad test5\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "xTr_test, yTr_test = h.generate_data()\n",
    "n, d = xTr_test.shape\n",
    "\n",
    "# Checks the gradient of the loss\n",
    "beta = np.random.rand(n)\n",
    "b = np.random.rand(1)\n",
    "beta_grad, bgrad = grad(beta, b, xTr_test, yTr_test, xTr_test, yTr_test, 10, 'rbf')\n",
    "beta_grad_grader, bgrad_grader = h.grad_grader(beta, b, xTr_test, yTr_test, xTr_test, yTr_test, 10, 'rbf')\n",
    "assert (np.linalg.norm(beta_grad - beta_grad_grader) < 1e-5) and \\\n",
    "    (np.linalg.norm(bgrad - bgrad_grader) < 1e-5)\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the kernelized algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7ad9af17accaddf0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Using the cell below, you can call the optimization routine that we have implemented for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fa26a32043f70279",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "beta_sol, bias_sol, final_loss = h.minimize(objective=loss, grad=grad, xTr=xTr, yTr=yTr, C=1000, kerneltype='linear', kpar=1)\n",
    "print('The Final Loss of your model is: {:0.4f}'.format(final_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8e81c269f495b733",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "If everything is implemented correctly, you should be able to get a training error of zero when you run the cell below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-20ca56dcce47a510",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "svmclassify = lambda x: np.sign(computeK('linear', x, xTr, 1).dot(beta_sol) + bias_sol)\n",
    "\n",
    "predsTr=svmclassify(xTr)\n",
    "trainingerr=np.mean(np.sign(predsTr)!=yTr)\n",
    "print(\"Training error: %2.4f\" % trainingerr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d7214490d41e9d20",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Visualize the Decision Boundary</h3>\n",
    "\n",
    "Also, when you visualize the classifier, you should see a max margin separator!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e0f813d39a20941d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "h.visclassifier(svmclassify, xTr, yTr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f922d756ef836ee9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now, we are going to try another nonlinear dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c69bc918b0db1293",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "xTr_spiral,yTr_spiral,xTe_spiral,yTe_spiral = h.spiraldata()\n",
    "\n",
    "%matplotlib notebook\n",
    "h.visualize_2D(xTr_spiral, yTr_spiral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-89594e6378c785cb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Since the dataset is nonlinear, we are going to use the rbf kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f4114c6db1b51e2c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "beta_sol_spiral, bias_sol_spiral, final_loss_spiral = h.minimize(objective=loss, grad=grad, xTr=xTr_spiral, yTr=yTr_spiral, C=100, kerneltype='rbf', kpar=1)\n",
    "print('The Final Loss of your model is: {:0.4f}'.format(final_loss_spiral))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-28c6cee7c68d04e9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "If you do everything correctly, your training error and test error should both be zero!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-16357e4762b8370b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "svmclassify_spiral = lambda x: np.sign(computeK('rbf', xTr_spiral, x, 1).transpose().dot(beta_sol_spiral) + bias_sol_spiral)\n",
    "\n",
    "predsTr_spiral = svmclassify_spiral(xTr_spiral)\n",
    "trainingerr_spiral = np.mean(predsTr_spiral != yTr_spiral)\n",
    "print(\"Training error: %2.4f\" % trainingerr_spiral)\n",
    "\n",
    "predsTe_spiral = svmclassify_spiral(xTe_spiral)\n",
    "testerr_spiral = np.mean(predsTe_spiral != yTe_spiral)\n",
    "print(\"Test error: %2.4f\" % testerr_spiral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-52cb1b9aae3993cb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now, let's visualize the classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-40d1227ae7bf20e6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "h.visclassifier(svmclassify_spiral, xTr_spiral, yTr_spiral)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
