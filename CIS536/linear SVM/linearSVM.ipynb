{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5352bee401322381",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2>About this Project</h2>\n",
    "<p>In this project, you will implement a linear support vector machine. You will generate a linearly separable dataset, write functions to build a linear SVM, and then visualize the decision boundary. You will also have the chance to add data points to a visualization of your linear SVM to see how it responds to new data.</p>\n",
    "\n",
    "<h3>Evaluation</h3>\n",
    "\n",
    "<p><strong>This project must be successfully completed and submitted in order to receive credit for this course. Your score on this project will be included in your final grade calculation.</strong><p>\n",
    "    \n",
    "<p>You are expected to write code where you see <em># YOUR CODE HERE</em> within the cells of this notebook. Not all cells will be graded; code input cells followed by cells marked with <em>#Autograder test cell</em> will be graded. Upon submitting your work, the code you write at these designated positions will be assessed using an \"autograder\" that will run all test cells to assess your code. You will receive feedback from the autograder that will identify any errors in your code. Use this feedback to improve your code if you need to resubmit. Be sure not to change the names of any provided functions, classes, or variables within the existing code cells, as this will interfere with the autograder. Also, remember to execute all code cells sequentially, not just those you’ve edited, to ensure your code runs properly.</p>\n",
    "    \n",
    "<p>You can resubmit your work as many times as necessary before the submission deadline. If you experience difficulty or have questions about this exercise, use the Q&A discussion board to engage with your peers or seek assistance from the instructor.<p>\n",
    "\n",
    "<p>Before starting your work, please review <a href=\"https://s3.amazonaws.com/ecornell/global/eCornellPlagiarismPolicy.pdf\">eCornell's policy regarding plagiarism</a> (the presentation of someone else's work as your own without source credit).</p>\n",
    "\n",
    "<h3>Submit Code for Autograder Feedback</h3>\n",
    "\n",
    "<p>Once you have completed your work on this notebook, you will submit your code for autograder review. Follow these steps:</p>\n",
    "\n",
    "<ol>\n",
    "  <li><strong>Save your notebook.</strong></li>\n",
    "  <li><strong>Mark as Completed —</strong> In the blue menu bar along the top of this code exercise window, you’ll see a menu item called <strong>Education</strong>. In the <strong>Education</strong> menu, click <strong>Mark as Completed</strong> to submit your code for autograder/instructor review. This process will take a moment and a progress bar will show you the status of your submission.</li>\n",
    "\t<li><strong>Review your results —</strong> Once your work is marked as complete, the results of the autograder will automatically be presented in a new tab within the code exercise window. You can click on the assessment name in this feedback window to see more details regarding specific feedback/errors in your code submission.</li>\n",
    "  <li><strong>Repeat, if necessary —</strong> The Jupyter notebook will always remain accessible in the first tabbed window of the exercise. To reattempt the work, you will first need to click <strong>Mark as Uncompleted</strong> in the <strong>Education</strong> menu and then proceed to make edits to the notebook. Once you are ready to resubmit, follow steps one through three. You can repeat this procedure as many times as necessary.</li>\n",
    "<p>You can also download a copy of this notebook in multiple formats using the <strong>Download as</strong> option in the <strong>File</strong> menu above.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b3b882bc93b94a57",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2>Getting Started</h2>\n",
    "<h3>Python Initialization</h3> \n",
    "\n",
    "Please run the following code to initialize your Python kernel. You should be running a version of Python 3.x. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ad42817139e637d9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.matlib import repmat\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from helper import *\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import linregress\n",
    "\n",
    "import pylab\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "%matplotlib notebook\n",
    "print('You\\'re running python %s' % sys.version.split(' ')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-029db15eeaae00a8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Generate and Visualize Data</h3>\n",
    "\n",
    "Let's generate some linearly seperable data and visualize it in order to get a sense of what you will be working with. Run the cell below to generate and visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2827088afcbb8b23",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xTr, yTr = generate_data()\n",
    "visualize_2D(xTr, yTr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-142c315381472f51",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2>Linear SVM</h2>\n",
    "\n",
    "\n",
    "Recall that the unconstrained loss function for linear SVM is \n",
    "    \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{\\mathbf{w},b}\\underbrace{\\mathbf{w}^T\\mathbf{w}}_{l_{2} \\text{ regularizer}}  +  C\\  \\sum_{i=1}^{n}\\underbrace{\\max\\left [ 1-y_{i}(\\mathbf{w}^T \\mathbf{x}_i+b),0 \\right ]}_{\\text{hinge loss}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "However, the hinge loss is not differentiable when $1-y_{i}(\\mathbf{w}^T \\mathbf{x}_i+b)= 0$. So, we are going to use the squared hinge loss instead: \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{\\mathbf{w},b}\\underbrace{\\mathbf{w}^T\\mathbf{w}}_{l_{2} \\text{ regularizer}} +  C\\  \\sum_{i=1}^{n}\\underbrace{\\max\\left [ 1-y_{i}(\\mathbf{w}^T \\mathbf{x}_i+b),0 \\right ] ^2}_{\\text{squared hinge loss}}\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Part One: Loss Function [Graded]</h3>\n",
    "\n",
    "You will need to implement the function <code>loss</code>, which takes in training data <code>xTr</code> ($n\\times d$) and labels <code>yTr</code> ($n$) with <code>yTr[i]</code>$\\in \\{-1,1\\}$ and evaluates the <b>squared</b> hinge loss of classifier $(\\mathbf{w},b)$. \n",
    "\n",
    "Some functions that might be useful for you:\n",
    "* `np.maximum(a,b)`: returns the maximum value between `a` and `b`\n",
    "* `arr.clip(min=0)`: returns `arr` but with value 0 replacing negative entries\n",
    "* `arr.shape`: returns the tuple `(m,n)` where `m` is the row count, `n` is the column count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-loss",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def loss(w, b, xTr, yTr, C):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    w     : d   dimensional weight vector\n",
    "    b     : scalar (bias)\n",
    "    xTr   : nxd dimensional matrix (each row is an input vector)\n",
    "    yTr   : n   dimensional vector (each entry is a label)\n",
    "    C     : scalar (constant that controls the tradeoff between l2-regularizer and hinge-loss)\n",
    "    \n",
    "    OUTPUTS:\n",
    "    loss     : the total loss obtained with (w, b) on xTr and yTr (scalar)\n",
    "    \"\"\"\n",
    "    \n",
    "    loss_val = 0.0\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    margin = yTr*(xTr @ w + b)\n",
    "    loss_val = w.T @ w + C*(np.sum(np.maximum(1 - margin, 0) ** 2))\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    return loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-loss-selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# These tests test whether your loss() is implemented correctly\n",
    "\n",
    "xTr_test, yTr_test = generate_data()\n",
    "n, d = xTr_test.shape\n",
    "\n",
    "# Check whether your loss() returns a scalar\n",
    "def loss_test1():\n",
    "    w = np.random.rand(d)\n",
    "    b = np.random.rand(1)\n",
    "    loss_val = loss(w, b, xTr_test, yTr_test, 10)    \n",
    "    return np.isscalar(loss_val)\n",
    "\n",
    "# Check whether your loss() returns a nonnegative scalar\n",
    "def loss_test2():\n",
    "    w = np.random.rand(d)\n",
    "    b = np.random.rand(1)\n",
    "    loss_val = loss(w, b, xTr_test, yTr_test, 10)\n",
    "    \n",
    "    return loss_val >= 0\n",
    "\n",
    "# Check whether you implement l2-regularizer correctly\n",
    "def loss_test3():\n",
    "    w = np.random.rand(d)\n",
    "    b = np.random.rand(1)\n",
    "    loss_val = loss(w, b, xTr_test, yTr_test, 0)\n",
    "    loss_val_grader = loss_grader(w, b, xTr_test, yTr_test, 0)\n",
    "    \n",
    "    return (np.linalg.norm(loss_val - loss_val_grader) < 1e-5)\n",
    "\n",
    "# Check whether you implemented the squared hinge loss and not the standard hinge loss\n",
    "# Note, loss_grader_wrong is the wrong implementation of the standard hinge-loss, \n",
    "# so the results should NOT match.\n",
    "def loss_test4():\n",
    "    w = np.random.randn(d)\n",
    "    b = np.random.rand(1)\n",
    "    loss_val = loss(w, b, xTr_test, yTr_test, 1)\n",
    "    badloss = loss_grader_wrong(w, b, xTr_test, yTr_test, 1)    \n",
    "    return not(np.linalg.norm(loss_val - badloss) < 1e-5)\n",
    "\n",
    "\n",
    "# Check whether you implement square hinge loss correctly\n",
    "def loss_test5():\n",
    "    w = np.random.randn(d)\n",
    "    b = np.random.rand(1)\n",
    "    loss_val = loss(w, b, xTr_test, yTr_test, 10)\n",
    "    loss_val_grader = loss_grader(w, b, xTr_test, yTr_test, 10)\n",
    "    \n",
    "    return (np.linalg.norm(loss_val - loss_val_grader) < 1e-5)\n",
    "\n",
    "# Check whether you implement loss correctly\n",
    "def loss_test6():\n",
    "    w = np.random.randn(d)\n",
    "    b = np.random.rand(1)\n",
    "    loss_val = loss(w, b, xTr_test, yTr_test, 100)\n",
    "    loss_val_grader = loss_grader(w, b, xTr_test, yTr_test, 100)\n",
    "    \n",
    "    return (np.linalg.norm(loss_val - loss_val_grader) < 1e-5)\n",
    "\n",
    "runtest(loss_test1,'loss_test1')\n",
    "runtest(loss_test2,'loss_test2')\n",
    "runtest(loss_test3,'loss_test3')\n",
    "runtest(loss_test4,'loss_test4')\n",
    "runtest(loss_test5,'loss_test5')\n",
    "runtest(loss_test6,'loss_test6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-loss-test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs loss_test1\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "assert loss_test1()\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-loss-test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs loss test2\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert loss_test2()\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-loss-test3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs loss test3\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert loss_test3()\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-loss-test4",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs loss test4\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert loss_test4()\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-loss-test5",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs loss test5\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert loss_test5()\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-loss-test6",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs loss test5\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert loss_test6()\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1f9aade383794e9f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Part Two: Gradient of Loss Function [Graded]</h3>\n",
    "\n",
    "Now, implement <code>grad</code>, which takes in the same arguments as the <code>loss</code> function but returns gradient of the loss function with respect to $(\\mathbf{w},b)$. \n",
    "\n",
    "First, we take the derivative of the squared hinge loss with respect to $\\mathbf w$:\n",
    "\n",
    "$$ \\frac{\\partial \\mathcal L}{\\partial \\mathbf w} = 2 \\mathbf w + C  \\sum_{i=1}^{n} 2  \\max \\left [ 1-y_{i}(\\mathbf{w}^T \\mathbf{x}_i+b),0 \\right ]  (-y_i \\mathbf x_i) $$ \n",
    "\n",
    "Second, we take the derivative with respect to $b$:\n",
    "\n",
    "$$ \\frac{\\partial \\mathcal L}{\\partial b} = C  \\sum_{i=1}^{n} 2  \\max\\left [ 1-y_{i}(\\mathbf{w}^T \\mathbf{x}_i+b),0 \\right ]  (-y_i)$$\n",
    "\n",
    "You should be able to reuse some of the code from your implementation of `loss` above. Additionally, if it is helpful, you can use the following function: \n",
    " \n",
    "* `arr.reshape(m,n)`: If necessary, rearranges elements of `arr` to fit dimensions `(m,n)` as well as possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-grad",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def grad(w, b, xTr, yTr, C):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    w     : d   dimensional weight vector\n",
    "    b     : scalar (bias)\n",
    "    xTr   : nxd dimensional matrix (each row is an input vector)\n",
    "    yTr   : n   dimensional vector (each entry is a label)\n",
    "    C     : constant (scalar that controls the tradeoff between l2-regularizer and hinge-loss)\n",
    "    \n",
    "    OUTPUTS:\n",
    "    wgrad :  d dimensional vector (the gradient of the hinge loss with respect to the weight, w)\n",
    "    bgrad :  constant (the gradient of the hinge loss with respect to the bias, b)\n",
    "    \"\"\"\n",
    "    n, d = xTr.shape\n",
    "    \n",
    "    wgrad = np.zeros(d)\n",
    "    bgrad = np.zeros(1)\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    margin = yTr*(xTr @ w + b)\n",
    "    \n",
    "    hinge = np.maximum(1 - margin, 0)\n",
    "    \n",
    "    indicator = (1 - margin > 0).astype(int)\n",
    "    wgrad = 2 * w + C * np.sum((2 * hinge * indicator * -yTr).reshape(-1, 1) * xTr, axis=0)\n",
    "    bgrad = C * np.sum(2 * hinge * indicator * -yTr, axis=0)\n",
    "    \n",
    "    ### END SOLUTION\n",
    "    return wgrad, bgrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-grad-selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# These tests test whether your grad() is implemented correctly\n",
    "\n",
    "xTr_test, yTr_test = generate_data()\n",
    "n, d = xTr_test.shape\n",
    "\n",
    "# Checks whether grad returns a tuple\n",
    "def grad_test1():\n",
    "    w = np.random.rand(d)\n",
    "    b = np.random.rand(1)\n",
    "    out = grad(w, b, xTr_test, yTr_test, 10)\n",
    "    return len(out) == 2\n",
    "\n",
    "# Checks the dimension of gradients\n",
    "def grad_test2():\n",
    "    w = np.random.rand(d)\n",
    "    b = np.random.rand(1)\n",
    "    wgrad, bgrad = grad(w, b, xTr_test, yTr_test, 10)\n",
    "    return len(wgrad) == d and np.isscalar(bgrad)\n",
    "\n",
    "# Checks the gradient of the l2 regularizer\n",
    "def grad_test3():\n",
    "    w = np.random.rand(d)\n",
    "    b = np.random.rand(1)\n",
    "    wgrad, bgrad = grad(w, b, xTr_test, yTr_test, 0)\n",
    "    wgrad_grader, bgrad_grader = grad_grader(w, b, xTr_test, yTr_test, 0)\n",
    "    return (np.linalg.norm(wgrad - wgrad_grader) < 1e-5) and \\\n",
    "        (np.linalg.norm(bgrad - bgrad_grader) < 1e-5)\n",
    "\n",
    "# Checks the gradient of the square hinge loss\n",
    "def grad_test4():\n",
    "    w = np.zeros(d)\n",
    "    b = np.random.rand(1)\n",
    "    wgrad, bgrad = grad(w, b, xTr_test, yTr_test, 1)\n",
    "    wgrad_grader, bgrad_grader = grad_grader(w, b, xTr_test, yTr_test, 1)\n",
    "    return (np.linalg.norm(wgrad - wgrad_grader) < 1e-5) and \\\n",
    "        (np.linalg.norm(bgrad - bgrad_grader) < 1e-5)\n",
    "\n",
    "# Checks the gradient of the loss\n",
    "def grad_test5():\n",
    "    w = np.random.rand(d)\n",
    "    b = np.random.rand(1)\n",
    "    wgrad, bgrad = grad(w, b, xTr_test, yTr_test, 10)\n",
    "    wgrad_grader, bgrad_grader = grad_grader(w, b, xTr_test, yTr_test, 10)\n",
    "    return (np.linalg.norm(wgrad - wgrad_grader) < 1e-5) and \\\n",
    "        (np.linalg.norm(bgrad - bgrad_grader) < 1e-5)\n",
    "\n",
    "runtest(grad_test1, 'grad_test1')\n",
    "runtest(grad_test2, 'grad_test2')\n",
    "runtest(grad_test3, 'grad_test3')\n",
    "runtest(grad_test4, 'grad_test4')\n",
    "runtest(grad_test5, 'grad_test5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-grad-test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs grad test1\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "xTr_test, yTr_test = generate_data()\n",
    "n, d = xTr_test.shape\n",
    "\n",
    "assert grad_test1()\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-grad-test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs grad test2\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "xTr_test, yTr_test = generate_data()\n",
    "n, d = xTr_test.shape\n",
    "\n",
    "assert grad_test2()\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-grad-test3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs grad test3\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "xTr_test, yTr_test = generate_data()\n",
    "n, d = xTr_test.shape\n",
    "\n",
    "assert grad_test3()\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-grad-test4",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs grad test4\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "xTr_test, yTr_test = generate_data()\n",
    "n, d = xTr_test.shape\n",
    "\n",
    "assert grad_test4()\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-grad-test5",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs grad test5\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "xTr_test, yTr_test = generate_data()\n",
    "n, d = xTr_test.shape\n",
    "\n",
    "assert grad_test5()\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d1fd049ca8c1daf2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Obtain the Linear SVM</h3>\n",
    "\n",
    "By calling the following minimization routine implemented for you in the cell below, you will obtain your linear SVM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-df5f6745407f34da",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "w, b, final_loss = minimize(objective=loss, grad=grad, xTr=xTr, yTr=yTr, C=1000)\n",
    "print('The Final Loss of your model is: {:0.4f}'.format(final_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-21d2b446b00cd3aa",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Visualize the Decision Boundary</h3>\n",
    "\n",
    "Now, let's visualize the decision boundary on our linearly separable dataset. Since the dataset is linearly separable,  you should obtain $0\\%$ training error with sufficiently large values of $C$ (e.g. $C>1000$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3a923e0743dc46da",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "visualize_classfier(xTr, yTr, w, b)\n",
    "\n",
    "# Calculate the training error\n",
    "err=np.mean(np.sign(xTr.dot(w) + b)!=yTr)\n",
    "print(\"Training error: {:.2f} %\".format (err*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-65f2c28b9b2b1014",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Interactive Demo</h3>\n",
    "\n",
    "Running the code below will create an interactive window where you can click to add new data points to see how your linear SVM will respond. There may be a significant delay between clicks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4d7dfed9aa687af9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "Xdata = []\n",
    "ldata = []\n",
    "\n",
    "fig = plt.figure()\n",
    "details = {\n",
    "    'w': None,\n",
    "    'b': None,\n",
    "    'stepsize': 1,\n",
    "    'ax': fig.add_subplot(111), \n",
    "    'line': None\n",
    "}\n",
    "\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "plt.title('Click to add positive point and shift+click to add negative points.')\n",
    "\n",
    "def updateboundary(Xdata, ldata):\n",
    "    global details\n",
    "    w_pre, b_pre, _ = minimize(objective=loss, grad=grad, xTr=np.concatenate(Xdata), \n",
    "            yTr=np.array(ldata), C=1000)\n",
    "    details['w'] = np.array(w_pre).reshape(-1)\n",
    "    details['b'] = b_pre\n",
    "    details['stepsize'] += 1\n",
    "\n",
    "def updatescreen():\n",
    "    global details\n",
    "    b = details['b']\n",
    "    w = details['w']\n",
    "    q = -b / (w**2).sum() * w\n",
    "    if details['line'] is None:\n",
    "        details['line'], = details['ax'].plot([q[0] - w[1],q[0] + w[1]],[q[1] + w[0],q[1] - w[0]],'b--')\n",
    "    else:\n",
    "        details['line'].set_ydata([q[1] + w[0],q[1] - w[0]])\n",
    "        details['line'].set_xdata([q[0] - w[1],q[0] + w[1]])\n",
    "\n",
    "\n",
    "def generate_onclick(Xdata, ldata):    \n",
    "    global details\n",
    "\n",
    "    def onclick(event):\n",
    "        if event.key == 'shift': \n",
    "            # add positive point\n",
    "            details['ax'].plot(event.xdata,event.ydata,'or')\n",
    "            label = 1\n",
    "        else: # add negative point\n",
    "            details['ax'].plot(event.xdata,event.ydata,'ob')\n",
    "            label = -1    \n",
    "        pos = np.array([[event.xdata, event.ydata]])\n",
    "        ldata.append(label)\n",
    "        Xdata.append(pos)\n",
    "        updateboundary(Xdata,ldata)\n",
    "        updatescreen()\n",
    "    return onclick\n",
    "\n",
    "\n",
    "cid = fig.canvas.mpl_connect('button_press_event', generate_onclick(Xdata, ldata))\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
