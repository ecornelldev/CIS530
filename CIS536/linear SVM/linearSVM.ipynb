{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5352bee401322381",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2>About this Project</h2>\n",
    "<p>In this project, you will implement a linear support vector machine. You will generate a linearly separable dataset, write functions to build a linear SVM, and then visualize the decision boundary. You will also have the chance to add data points to a visualization of your linear SVM to see how it responds to new data.</p>\n",
    "\n",
    "<h3>Evaluation</h3>\n",
    "\n",
    "<p><strong>This project must be successfully completed and submitted in order to receive credit for this course. Your score on this project will be included in your final grade calculation.</strong><p>\n",
    "    \n",
    "<p>You are expected to write code where you see <em># YOUR CODE HERE</em> within the cells of this notebook. Not all cells will be graded; code input cells followed by cells marked with <em>#Autograder test cell</em> will be graded. Upon submitting your work, the code you write at these designated positions will be assessed using an \"autograder\" that will run all test cells to assess your code. You will receive feedback from the autograder that will identify any errors in your code. Use this feedback to improve your code if you need to resubmit. Be sure not to change the names of any provided functions, classes, or variables within the existing code cells, as this will interfere with the autograder. Also, remember to execute all code cells sequentially, not just those you’ve edited, to ensure your code runs properly.</p>\n",
    "    \n",
    "<p>You can resubmit your work as many times as necessary before the submission deadline. If you experience difficulty or have questions about this exercise, use the Q&A discussion board to engage with your peers or seek assistance from the instructor.<p>\n",
    "\n",
    "<p>Before starting your work, please review <a href=\"https://s3.amazonaws.com/ecornell/global/eCornellPlagiarismPolicy.pdf\">eCornell's policy regarding plagiarism</a> (the presentation of someone else's work as your own without source credit).</p>\n",
    "\n",
    "<h3>Submit Code for Autograder Feedback</h3>\n",
    "\n",
    "<p>Once you have completed your work on this notebook, you will submit your code for autograder review. Follow these steps:</p>\n",
    "\n",
    "<ol>\n",
    "  <li><strong>Save your notebook.</strong></li>\n",
    "  <li><strong>Mark as Completed —</strong> In the blue menu bar along the top of this code exercise window, you’ll see a menu item called <strong>Education</strong>. In the <strong>Education</strong> menu, click <strong>Mark as Completed</strong> to submit your code for autograder/instructor review. This process will take a moment and a progress bar will show you the status of your submission.</li>\n",
    "\t<li><strong>Review your results —</strong> Once your work is marked as complete, the results of the autograder will automatically be presented in a new tab within the code exercise window. You can click on the assessment name in this feedback window to see more details regarding specific feedback/errors in your code submission.</li>\n",
    "  <li><strong>Repeat, if necessary —</strong> The Jupyter notebook will always remain accessible in the first tabbed window of the exercise. To reattempt the work, you will first need to click <strong>Mark as Uncompleted</strong> in the <strong>Education</strong> menu and then proceed to make edits to the notebook. Once you are ready to resubmit, follow steps one through three. You can repeat this procedure as many times as necessary.</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b3b882bc93b94a57",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2>Getting Started</h2>\n",
    "<h3>Python Initialization</h3> \n",
    "\n",
    "Please run the following code to initialize your Python kernel. You should be running a version of Python 3.x. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ad42817139e637d9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.matlib import repmat\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import helper as h\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import linregress\n",
    "\n",
    "import pylab\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "%matplotlib notebook\n",
    "print('You\\'re running python %s' % sys.version.split(' ')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-029db15eeaae00a8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Generate and Visualize Data</h3>\n",
    "Let's generate some linearly seperable data and visualize it in order to get a sense of what you will be working with. Run the cell below to generate and visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2827088afcbb8b23",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xTr, yTr = h.generate_data()\n",
    "h.visualize_2D(xTr, yTr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-142c315381472f51",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2>Linear SVM</h2>\n",
    "\n",
    "\n",
    "Recall that the unconstrained loss function for linear SVM is \n",
    "    \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{\\mathbf{w},b}\\underbrace{\\mathbf{w}^T\\mathbf{w}}_{l_{2} \\text{ regularizer}}  +  C\\  \\sum_{i=1}^{n}\\underbrace{\\max\\left [ 1-y_{i}(\\mathbf{w}^T \\mathbf{x}_i+b),0 \\right ]}_{\\text{hinge loss}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "However, the hinge loss is not differentiable when $1-y_{i}(\\mathbf{w}^T \\mathbf{x}_i+b)= 0$. So, we are going to use the squared hinge loss instead: \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{\\mathbf{w},b}\\underbrace{\\mathbf{w}^T\\mathbf{w}}_{l_{2} \\text{ regularizer}} +  C\\  \\sum_{i=1}^{n}\\underbrace{\\max\\left [ 1-y_{i}(\\mathbf{w}^T \\mathbf{x}_i+b),0 \\right ] ^2}_{\\text{squared hinge loss}}\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Part One: Loss Function [Graded]</h3>\n",
    "\n",
    "You will need to implement the function <code>loss</code>, which takes in training data <code>xTr</code> ($n\\times d$) and labels <code>yTr</code> ($n$) with <code>yTr[i]</code>$\\in \\{-1,1\\}$ and evaluates the loss of classifier $(\\mathbf{w},b)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-loss",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def loss(w, b, xTr, yTr, C):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    w     : d   dimensional weight vector\n",
    "    b     : scalar (bias)\n",
    "    xTr   : nxd dimensional matrix (each row is an input vector)\n",
    "    yTr   : n   dimensional vector (each entry is a label)\n",
    "    C     : scalar (constant that controls the tradeoff between l2-regularizer and hinge-loss)\n",
    "    \n",
    "    OUTPUTS:\n",
    "    loss     : the total loss obtained with (w, b) on xTr and yTr (scalar)\n",
    "    \"\"\"\n",
    "    \n",
    "    loss_val = 0.0\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    margin = yTr*(xTr @ w + b)\n",
    "    loss_val = w.T @ w + C*(np.sum(np.maximum(1 - margin, 0) ** 2))\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    return loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-loss-selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# These tests test whether your loss() is implemented correctly\n",
    "\n",
    "xTr_test, yTr_test = h.generate_data()\n",
    "n, d = xTr_test.shape\n",
    "\n",
    "# Check whether your loss() returns a scalar\n",
    "def loss_test1():\n",
    "    w = np.zeros(d)\n",
    "    b = np.zeros(1)\n",
    "    loss_val = loss(w, b, xTr_test, yTr_test, 10)\n",
    "    \n",
    "    return np.isscalar(loss_val)\n",
    "\n",
    "# Check whether your loss() returns a nonnegative scalar\n",
    "def loss_test2():\n",
    "    w = np.random.rand(d)\n",
    "    b = np.random.rand(1)\n",
    "    loss_val = loss(w, b, xTr_test, yTr_test, 10)\n",
    "    \n",
    "    return loss_val >= 0\n",
    "\n",
    "# Check whether you implement l2-regularizer correctly\n",
    "def loss_test3():\n",
    "    w = np.random.rand(d)\n",
    "    b = np.random.rand(1)\n",
    "    loss_val = loss(w, b, xTr_test, yTr_test, 0)\n",
    "    loss_val_grader = h.loss_grader(w, b, xTr_test, yTr_test, 0)\n",
    "    \n",
    "    return (np.linalg.norm(loss_val - loss_val_grader) < 1e-5)\n",
    "\n",
    "# Check whether you implement square hinge loss correctly\n",
    "def loss_test4():\n",
    "    w = np.zeros(d)\n",
    "    b = np.random.rand(1)\n",
    "    loss_val = loss(w, b, xTr_test, yTr_test, 10)\n",
    "    loss_val_grader = h.loss_grader(w, b, xTr_test, yTr_test, 10)\n",
    "    \n",
    "    return (np.linalg.norm(loss_val - loss_val_grader) < 1e-5)\n",
    "\n",
    "# Check whether you implement square hinge loss correctly\n",
    "def loss_test5():\n",
    "    w = np.zeros(d)\n",
    "    b = np.random.rand(1)\n",
    "    loss_val = loss(w, b, xTr_test, yTr_test, 10)\n",
    "    loss_val_grader = h.loss_grader(w, b, xTr_test, yTr_test, 10)\n",
    "    \n",
    "    return (np.linalg.norm(loss_val - loss_val_grader) < 1e-5)\n",
    "\n",
    "# Check whether you implement loss correctly\n",
    "def loss_test6():\n",
    "    w = np.zeros(d)\n",
    "    b = np.random.rand(1)\n",
    "    loss_val = loss(w, b, xTr_test, yTr_test, 100)\n",
    "    loss_val_grader = h.loss_grader(w, b, xTr_test, yTr_test, 100)\n",
    "    \n",
    "    return (np.linalg.norm(loss_val - loss_val_grader) < 1e-5)\n",
    "\n",
    "h.runtest(loss_test1,'loss_test1')\n",
    "h.runtest(loss_test2,'loss_test2')\n",
    "h.runtest(loss_test3,'loss_test3')\n",
    "h.runtest(loss_test4,'loss_test4')\n",
    "h.runtest(loss_test5,'loss_test5')\n",
    "h.runtest(loss_test6,'loss_test6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-loss-test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs loss_test1\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "xTr_test, yTr_test = h.generate_data()\n",
    "n, d = xTr_test.shape\n",
    "\n",
    "w = np.zeros(d)\n",
    "b = np.zeros(1)\n",
    "loss_val = loss(w, b, xTr_test, yTr_test, 10)\n",
    "    \n",
    "assert np.isscalar(loss_val)\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-loss-test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs loss test2\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "xTr_test, yTr_test = h.generate_data()\n",
    "n, d = xTr_test.shape\n",
    "\n",
    "w = np.random.rand(d)\n",
    "b = np.random.rand(1)\n",
    "loss_val = loss(w, b, xTr_test, yTr_test, 10)\n",
    "    \n",
    "assert loss_val >= 0\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-loss-test3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs loss test3\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "xTr_test, yTr_test = h.generate_data()\n",
    "n, d = xTr_test.shape\n",
    "\n",
    "w = np.random.rand(d)\n",
    "b = np.random.rand(1)\n",
    "loss_val = loss(w, b, xTr_test, yTr_test, 0)\n",
    "loss_val_grader = h.loss_grader(w, b, xTr_test, yTr_test, 0)\n",
    "    \n",
    "assert (np.linalg.norm(loss_val - loss_val_grader) < 1e-5)\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-loss-test4",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs loss test4\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "xTr_test, yTr_test = h.generate_data()\n",
    "n, d = xTr_test.shape\n",
    "\n",
    "w = np.zeros(d)\n",
    "b = np.random.rand(1)\n",
    "loss_val = loss(w, b, xTr_test, yTr_test, 10)\n",
    "loss_val_grader = h.loss_grader(w, b, xTr_test, yTr_test, 10)\n",
    "\n",
    "assert (np.linalg.norm(loss_val - loss_val_grader) < 1e-5)\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-loss-test5",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs loss test5\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "xTr_test, yTr_test = h.generate_data()\n",
    "n, d = xTr_test.shape\n",
    "\n",
    "w = np.zeros(d)\n",
    "b = np.random.rand(1)\n",
    "loss_val = loss(w, b, xTr_test, yTr_test, 10)\n",
    "loss_val_grader = h.loss_grader(w, b, xTr_test, yTr_test, 10)\n",
    "\n",
    "assert (np.linalg.norm(loss_val - loss_val_grader) < 1e-5)\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-loss-test6",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs loss test6\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "xTr_test, yTr_test = h.generate_data()\n",
    "n, d = xTr_test.shape\n",
    "\n",
    "w = np.zeros(d)\n",
    "b = np.random.rand(1)\n",
    "loss_val = loss(w, b, xTr_test, yTr_test, 100)\n",
    "loss_val_grader = h.loss_grader(w, b, xTr_test, yTr_test, 100)\n",
    "    \n",
    "assert (np.linalg.norm(loss_val - loss_val_grader) < 1e-5)\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1f9aade383794e9f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Part Two: Gradient of Loss Function [Graded]</h3>\n",
    "\n",
    "Now, implement <code>grad</code>, which takes in the same arguments as the <code>loss</code> function but returns gradient of the loss function with respect to $(\\mathbf{w},b)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-grad",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def grad(w, b, xTr, yTr, C):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    w     : d   dimensional weight vector\n",
    "    b     : scalar (bias)\n",
    "    xTr   : nxd dimensional matrix (each row is an input vector)\n",
    "    yTr   : n   dimensional vector (each entry is a label)\n",
    "    C     : constant (scalar that controls the tradeoff between l2-regularizer and hinge-loss)\n",
    "    \n",
    "    OUTPUTS:\n",
    "    wgrad :  d dimensional vector (the gradient of the hinge loss with respect to the weight, w)\n",
    "    bgrad :  constant (the gradient of he hinge loss with respect to the bias, b)\n",
    "    \"\"\"\n",
    "    n, d = xTr.shape\n",
    "    \n",
    "    wgrad = np.zeros(d)\n",
    "    bgrad = np.zeros(1)\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    margin = yTr*(xTr @ w + b)\n",
    "    \n",
    "    hinge = np.maximum(1 - margin, 0)\n",
    "    \n",
    "    indicator = (1 - margin > 0).astype(int)\n",
    "    wgrad = 2 * w + C * np.sum((2 * hinge * indicator * -yTr).reshape(-1, 1) * xTr, axis=0)\n",
    "    bgrad = C * np.sum(2 * hinge * indicator * -yTr, axis=0)\n",
    "    \n",
    "    ### END SOLUTION\n",
    "    return wgrad, bgrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-grad-selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# These tests test whether your grad() is implemented correctly\n",
    "\n",
    "xTr_test, yTr_test = h.generate_data()\n",
    "n, d = xTr_test.shape\n",
    "\n",
    "# Checks whether grad returns a tuple\n",
    "def grad_test1():\n",
    "    w = np.random.rand(d)\n",
    "    b = np.random.rand(1)\n",
    "    out = grad(w, b, xTr_test, yTr_test, 10)\n",
    "    return len(out) == 2\n",
    "\n",
    "# Checks the dimension of gradients\n",
    "def grad_test2():\n",
    "    w = np.random.rand(d)\n",
    "    b = np.random.rand(1)\n",
    "    wgrad, bgrad = grad(w, b, xTr_test, yTr_test, 10)\n",
    "    return len(wgrad) == d and np.isscalar(bgrad)\n",
    "\n",
    "# Checks the gradient of the l2 regularizer\n",
    "def grad_test3():\n",
    "    w = np.random.rand(d)\n",
    "    b = np.random.rand(1)\n",
    "    wgrad, bgrad = grad(w, b, xTr_test, yTr_test, 0)\n",
    "    wgrad_grader, bgrad_grader = h.grad_grader(w, b, xTr_test, yTr_test, 0)\n",
    "    return (np.linalg.norm(wgrad - wgrad_grader) < 1e-5) and \\\n",
    "        (np.linalg.norm(bgrad - bgrad_grader) < 1e-5)\n",
    "\n",
    "# Checks the gradient of the square hinge loss\n",
    "def grad_test4():\n",
    "    w = np.zeros(d)\n",
    "    b = np.random.rand(1)\n",
    "    wgrad, bgrad = grad(w, b, xTr_test, yTr_test, 1)\n",
    "    wgrad_grader, bgrad_grader = h.grad_grader(w, b, xTr_test, yTr_test, 1)\n",
    "    return (np.linalg.norm(wgrad - wgrad_grader) < 1e-5) and \\\n",
    "        (np.linalg.norm(bgrad - bgrad_grader) < 1e-5)\n",
    "\n",
    "# Checks the gradient of the loss\n",
    "def grad_test5():\n",
    "    w = np.random.rand(d)\n",
    "    b = np.random.rand(1)\n",
    "    wgrad, bgrad = grad(w, b, xTr_test, yTr_test, 10)\n",
    "    wgrad_grader, bgrad_grader = h.grad_grader(w, b, xTr_test, yTr_test, 10)\n",
    "    return (np.linalg.norm(wgrad - wgrad_grader) < 1e-5) and \\\n",
    "        (np.linalg.norm(bgrad - bgrad_grader) < 1e-5)\n",
    "\n",
    "h.runtest(grad_test1, 'grad_test1')\n",
    "h.runtest(grad_test2, 'grad_test2')\n",
    "h.runtest(grad_test3, 'grad_test3')\n",
    "h.runtest(grad_test4, 'grad_test4')\n",
    "h.runtest(grad_test5, 'grad_test5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-grad-test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs grad test1\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "xTr_test, yTr_test = h.generate_data()\n",
    "n, d = xTr_test.shape\n",
    "\n",
    "w = np.random.rand(d)\n",
    "b = np.random.rand(1)\n",
    "out = grad(w, b, xTr_test, yTr_test, 10)\n",
    "assert len(out) == 2\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-grad-test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs grad test2\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "xTr_test, yTr_test = h.generate_data()\n",
    "n, d = xTr_test.shape\n",
    "\n",
    "# Checks the dimension of gradients\n",
    "w = np.random.rand(d)\n",
    "b = np.random.rand(1)\n",
    "wgrad, bgrad = grad(w, b, xTr_test, yTr_test, 10)\n",
    "assert len(wgrad) == d and np.isscalar(bgrad)\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-grad-test3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs grad test3\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "xTr_test, yTr_test = h.generate_data()\n",
    "n, d = xTr_test.shape\n",
    "\n",
    "# Checks the gradient of the l2 regularizer\n",
    "w = np.random.rand(d)\n",
    "b = np.random.rand(1)\n",
    "wgrad, bgrad = grad(w, b, xTr_test, yTr_test, 0)\n",
    "wgrad_grader, bgrad_grader = h.grad_grader(w, b, xTr_test, yTr_test, 0)\n",
    "assert (np.linalg.norm(wgrad - wgrad_grader) < 1e-5) and \\\n",
    "    (np.linalg.norm(bgrad - bgrad_grader) < 1e-5)\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-grad-test4",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs grad test4\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "xTr_test, yTr_test = h.generate_data()\n",
    "n, d = xTr_test.shape\n",
    "\n",
    "# Checks the gradient of the square hinge loss\n",
    "w = np.zeros(d)\n",
    "b = np.random.rand(1)\n",
    "wgrad, bgrad = grad(w, b, xTr_test, yTr_test, 1)\n",
    "wgrad_grader, bgrad_grader = h.grad_grader(w, b, xTr_test, yTr_test, 1)\n",
    "assert (np.linalg.norm(wgrad - wgrad_grader) < 1e-5) and \\\n",
    "        (np.linalg.norm(bgrad - bgrad_grader) < 1e-5)\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-grad-test5",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs grad test5\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "xTr_test, yTr_test = h.generate_data()\n",
    "n, d = xTr_test.shape\n",
    "\n",
    "# Checks the gradient of the loss\n",
    "w = np.random.rand(d)\n",
    "b = np.random.rand(1)\n",
    "wgrad, bgrad = grad(w, b, xTr_test, yTr_test, 10)\n",
    "wgrad_grader, bgrad_grader = h.grad_grader(w, b, xTr_test, yTr_test, 10)\n",
    "assert (np.linalg.norm(wgrad - wgrad_grader) < 1e-5) and \\\n",
    "    (np.linalg.norm(bgrad - bgrad_grader) < 1e-5)\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d1fd049ca8c1daf2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Obtain the Linear SVM</h3>\n",
    "\n",
    "By calling the following minimization routine implemented for you in the cell below, you will obtain your linear SVM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-df5f6745407f34da",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "w, b, final_loss = h.minimize(objective=loss, grad=grad, xTr=xTr, yTr=yTr, C=1000)\n",
    "print('The Final Loss of your model is: {:0.4f}'.format(final_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-21d2b446b00cd3aa",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Visualize the Decision Boundary</h3>\n",
    "\n",
    "Now, let's visualize the decision boundary on our linearly separable dataset. Since the dataset is linearly separable,  you should obtain $0\\%$ training error with sufficiently large values of $C$ (e.g. $C>1000$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3a923e0743dc46da",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "h.visualize_classfier(xTr, yTr, w, b)\n",
    "\n",
    "# Calculate the training error\n",
    "err=np.mean(np.sign(xTr.dot(w) + b)!=yTr)\n",
    "print(\"Training error: {:.2f} %\".format (err*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-65f2c28b9b2b1014",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Interactive Data Set</h3>\n",
    "Running the code below will create an interactive window where you can click to add new data points to see how your linear SVM will respond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4d7dfed9aa687af9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "Xdata = []\n",
    "ldata = []\n",
    "\n",
    "fig = plt.figure()\n",
    "details = {\n",
    "    'w': None,\n",
    "    'b': None,\n",
    "    'stepsize': 1,\n",
    "    'ax': fig.add_subplot(111), \n",
    "    'line': None\n",
    "}\n",
    "\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "plt.title('Click to add positive point and shift+click to add negative points.')\n",
    "\n",
    "def updateboundary(Xdata, ldata, details):\n",
    "    w_pre, b_pre, _ = h.minimize(objective=loss, grad=grad, xTr=np.concatenate(Xdata), \n",
    "            yTr=np.array(ldata), C=1000)\n",
    "    details['w'] = np.array(w_pre).reshape(-1)\n",
    "    details['b'] = b_pre\n",
    "    details['stepsize'] += 1\n",
    "\n",
    "def updatescreen(details):\n",
    "    b = details['b']\n",
    "    w = details['w']\n",
    "    q = -b / (w**2).sum() * w\n",
    "    if details['line'] is None:\n",
    "        details['line'], = details['ax'].plot([q[0] - w[1],q[0] + w[1]],[q[1] + w[0],q[1] - w[0]],'b--')\n",
    "    else:\n",
    "        details['line'].set_ydata([q[1] + w[0],q[1] - w[0]])\n",
    "        details['line'].set_xdata([q[0] - w[1],q[0] + w[1]])\n",
    "\n",
    "def generate_animate(Xdata, ldata, details):\n",
    "    def animate(i):\n",
    "        if (len(ldata) > 0) and (np.amin(ldata) + np.amax(ldata) == 0):\n",
    "            if details['stepsize'] < 1000:\n",
    "                updateboundary(Xdata, ldata, details)\n",
    "                updatescreen(details)\n",
    "    return animate\n",
    "\n",
    "def generate_onclick(Xdata, ldata):    \n",
    "    def onclick(event):\n",
    "        if event.key == 'shift': \n",
    "            # add positive point\n",
    "            details['ax'].plot(event.xdata,event.ydata,'or')\n",
    "            label = 1\n",
    "        else: # add negative point\n",
    "            details['ax'].plot(event.xdata,event.ydata,'ob')\n",
    "            label = -1    \n",
    "        pos = np.array([[event.xdata, event.ydata]])\n",
    "        ldata.append(label)\n",
    "        Xdata.append(pos)\n",
    "    return onclick\n",
    "\n",
    "\n",
    "cid = fig.canvas.mpl_connect('button_press_event', generate_onclick(Xdata, ldata))\n",
    "ani = FuncAnimation(fig, generate_animate(Xdata, ldata, details), np.arange(1,100,1), interval=10)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
