{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "42531eb73174a111307396870bd49862",
     "grade": false,
     "grade_id": "cell-7ca0859bbc65faff",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2>Multilayer Perceptron</h2>\n",
    "\n",
    "\n",
    "<h3>Introduction</h3>\n",
    "In this project, you will implement a simple multilayer perceptron for a regression problem.\n",
    "We broke it apart into several functions. Before we start, let's import some packages:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "379e6ea989630ddeab089a9c219ae4ff",
     "grade": false,
     "grade_id": "cell-3fe0f85f2526afff",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "998cc1c30b11942be8f3ec625fd79373",
     "grade": false,
     "grade_id": "cell-05ec67dd9c38e8fd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "import numpy as np\n",
    "from numpy.matlib import repmat\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "import time\n",
    "\n",
    "import helper as h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fc691e72f6996f242a791a205121273d",
     "grade": false,
     "grade_id": "cell-d5f8224f284306d3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Visualizing Data</h3>\n",
    "We will generate a simple 1-dimensional toy dataset by calling <code>h.generate_data</code>. This function returns the data $\\mathbf{X}$ and label $\\mathbf{y}$. Note that $X$ is of shape (N, 2). We append 1 to each example to introduce bias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ceca8ecd147ce49bdf083db9a7b557f8",
     "grade": false,
     "grade_id": "cell-07e24f138d476ac4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "X, y = h.generate_data()\n",
    "\n",
    "print(f'The shape of X is {X.shape}. This is because we append 1 to each feature vector to introduce bias!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cccde96d16ffe46cfbc0a0b83cb8ec8f",
     "grade": false,
     "grade_id": "cell-22f2e1e5eea4b07b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(X[:, 0], y, '*')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7abc8ca9ceced3fb3d0b5cdd9dbc5848",
     "grade": false,
     "grade_id": "cell-6aae1a46a88cfb3d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>0. Transition Function</h3>\n",
    "Transition function is the key component of a neural network that contributes to its nonlinearity. For our neural network, we are going to use the ReLU transition function. Recall that the ReLU transition is as follows:\n",
    "$$\\sigma(z) = \\max(z, 0)$$\n",
    "\n",
    "We have implemented the <code>ReLU</code> and <code>ReLU_grad</code> functions as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cca218120ce7f2f7c4ca4b1e48600497",
     "grade": false,
     "grade_id": "cell-e29d6d493186387e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def ReLU(z):\n",
    "    return np.maximum(z, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2587398563b3ba98ff09811d58c52ee1",
     "grade": false,
     "grade_id": "cell-0e334778f4924bd7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def ReLU_grad(z):\n",
    "    return (z > 0).astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cb810c9d4cd06d9d1d8b48233085336c",
     "grade": false,
     "grade_id": "cell-d719110877776b79",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We visualize the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "31636f939fe9277a78719b130442cd2d",
     "grade": false,
     "grade_id": "cell-ea052350528f5135",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(-4, 4, 100), ReLU(np.linspace(-4, 4, 100)))\n",
    "plt.xlabel('z')\n",
    "plt.ylabel(r'$\\max$ (z, 0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "07afbd6a524584126a92165e31fefc25",
     "grade": false,
     "grade_id": "cell-a07168b188ef1922",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>1. Forward Pass</h3>\n",
    "\n",
    "In this section, you will implement the forward pass function <code>forward_pass(W, xTr)</code>. Note that $\\mathbf{W}$ specifies the weights of the network at each layer. More specifically, $W[0]$ stores the weights for the first layer of the network, $W[i]$ stores the weights of the (i + 1)-th layer and $W[l-1]$ stores the weights of the last layer.\n",
    "\n",
    "Each layer of the network produce two outputs, $A[i + 1]$ and \n",
    "$Z[i + 1]$, where \n",
    "\n",
    "$$A[i + 1]=Z[i] * W[i]$$ \n",
    "for $i = 0, 1, 2, ..., l-1$ and \n",
    "$$Z[i+1]=\\sigma(A[i+1])$$ for $i = 0, 1, 2, ..., l-2$ and \n",
    "$$Z[l-1]=A[l-1]$$\n",
    "\n",
    "\n",
    "Here, $*$ stands for matrix multiplication and $Z[0], A[0]$ are both initialized to be the the training set.\n",
    "\n",
    "For simplicity, we did not include bias when calculating $A[i + 1]$. For the purpose of this assignment, this is fine since we have appended one in all the raw features and  our dataset is rather simple. In general, bias should always be included when calculating $A[i + 1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8dcd2b0edf6cabdb4cee2d68cec87ded",
     "grade": false,
     "grade_id": "cell-b652c9590b2cfe79",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The following function will randomly generate initial weights for the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b3591f8a4845ab595034de849e41d07b",
     "grade": false,
     "grade_id": "cell-ce4c1f90a8b0779c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def initweights(specs):\n",
    "    \"\"\"\n",
    "    Given a specification of the neural network, output a random weight array\n",
    "    INPUT:\n",
    "        specs - array of length m+1. specs[0] should be the dimension of the feature and spec[-1] \n",
    "                should be the dimension of output\n",
    "    \n",
    "    OUTPUT:\n",
    "        W - array of length m, each element is a matrix\n",
    "            where size(weights[i]) = (specs[i], specs[i+1])\n",
    "    \"\"\"\n",
    "    W = []\n",
    "    for i in range(len(specs) - 1):\n",
    "        W.append(np.random.randn(specs[i], specs[i+1]))\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "68b3a9ca3da6c40493f7522f8e831fde",
     "grade": false,
     "grade_id": "cell-679cd28956259b78",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# If we want to create a network that \n",
    "# i)takes in feature of dimension 2\n",
    "# ii) has 1 hidden layer with 3 hidden units\n",
    "# iii) output a scalar\n",
    "# We initialize the the weights the following way\n",
    "\n",
    "W = initweights([2, 3, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e9f2a388d41c5e5ced021a9bd9a8ad79",
     "grade": false,
     "grade_id": "cell-0ee91db35bb678d4",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def forward_pass(W, xTr):\n",
    "    \"\"\"\n",
    "    function forward_pass(weights,xTr)\n",
    "    \n",
    "    INPUT:\n",
    "    W - an array of l weight matrices\n",
    "    xTr - nxd matrix. Each row is an input vector\n",
    "    \n",
    "    OUTPUTS:\n",
    "    A - an array (of length l) that stores result of matrix multiplication at each layer \n",
    "    Z - an array (of length l) that stores result of matrix multiplication at each layer \n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize A and Z\n",
    "    A = [xTr]\n",
    "    Z = [xTr]\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return A, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "621d93438fe6562d0960428f351f26d4",
     "grade": false,
     "grade_id": "cell-291fd4186629ceb1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def forward_test1():\n",
    "    X, _ = h.generate_data() # generate data\n",
    "    W = initweights([2, 3, 1]) # generate random weights\n",
    "    out = forward_pass(W, X) # run forward pass\n",
    "    return len(out) == 2 # make sure that your function return a tuple\n",
    "\n",
    "def forward_test2():\n",
    "    X, _ = h.generate_data() # generate data\n",
    "    W = initweights([2, 3, 1]) # generate random weights\n",
    "    A, Z = forward_pass(W, X) # run forward pass\n",
    "    return len(A) == 3 and len(Z) == 3 # Make sure that output produced match the length of the weight\n",
    "\n",
    "def forward_test3():\n",
    "    X, _ = h.generate_data() # generate data\n",
    "    n, _ = X.shape\n",
    "    W = initweights([2, 3, 1]) # generate random weights\n",
    "    A, Z = forward_pass(W, X) # run forward pass\n",
    "    return (A[1].shape == (n, 3) and \n",
    "            Z[1].shape == (n, 3)  and\n",
    "            A[2].shape == (n, 1) and\n",
    "            A[2].shape == (n, 1) ) # Make sure the layer produce the right shape output\n",
    "\n",
    "def forward_test4():\n",
    "    X = -1*np.ones((1, 2)) # generate a feature matrix of all negative ones\n",
    "    W = [np.ones((2, 1))] # a single layer network with weights one\n",
    "    A, Z = forward_pass(W, X) # run forward pass\n",
    "    \n",
    "    # check whether you do not apply the transition function to A[-1] \n",
    "    return np.linalg.norm(Z[-1] - X@W[0]) < 1e-7\n",
    "\n",
    "def forward_test5():\n",
    "    X, _ = h.generate_data() # generate data\n",
    "    n, _ = X.shape\n",
    "    W = initweights([2, 3, 1]) # generate random weights\n",
    "    A, Z = forward_pass(W, X) # run your forward pass\n",
    "    A_grader, Z_grader = h.forward_pass_grader(W, X) # run our forward pass\n",
    "    \n",
    "    Adiff = 0\n",
    "    Zdiff = 0\n",
    "    \n",
    "    # compute the difference between your solution and ours\n",
    "    for i in range(1, 3):\n",
    "        Adiff += np.linalg.norm(A[i] - A_grader[i])\n",
    "        Zdiff += np.linalg.norm(Z[i] - Z_grader[i])\n",
    "        \n",
    "    return Adiff < 1e-7 and Zdiff < 1e-7\n",
    "\n",
    "h.runtest(forward_test1, \"forward_test1\")\n",
    "h.runtest(forward_test2, \"forward_test2\")\n",
    "h.runtest(forward_test3, \"forward_test3\")\n",
    "h.runtest(forward_test4, \"forward_test4\")\n",
    "h.runtest(forward_test5, \"forward_test5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b6d9bcb3c4a8199ef1bbf69cbaf2e2b8",
     "grade": true,
     "grade_id": "cell-131410510db941fe",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# Forward_test1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8a7f881a4422fe44ed9f25566007230e",
     "grade": true,
     "grade_id": "cell-49d3a699f1e73506",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# Forward_test2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d3fec5f7beb46acb8769eda27b98d96a",
     "grade": true,
     "grade_id": "cell-9a6f3b89276ba82f",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# Forward_test3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9fc7ca931756fe635c10a5496c9f8a6c",
     "grade": true,
     "grade_id": "cell-914018b83724610e",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# Forward_test4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "105f323f85182b46a806e7753f38f7d3",
     "grade": true,
     "grade_id": "cell-df0ca708e878a285",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# Forward_test5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "24fb0d45a088e4bddb06e2d7b6a9f497",
     "grade": false,
     "grade_id": "cell-d74c4dde8e4ec273",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3> 2. Loss Function</h3>\n",
    "\n",
    "In this section, you are going to implement the MSE loss function for regression. Recall that for a set of training example $\\{(\\mathbf{x}_1, y_1), ..., (\\mathbf{x}_n, y_n)\\}$, the MSE of the network $h$ is \n",
    "\n",
    "$$L = \\frac{1}{n} \\sum_{i = 1} ^n(h(\\mathbf{x}_i) - y_i)^2$$\n",
    "\n",
    "The <code>MSE</code> function you are going to implement takes the output of the network (<code>out</code>) and the training labels (<code>y</code>) and computes the MSE loss. Also, you are going to implement the <code>MSE_grad</code> function that will calculate the gradient of the MSE loss with respect to each entry of out. This function will be useful when you implement backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b29c9684986acd5a1981a79fdd4772eb",
     "grade": false,
     "grade_id": "cell-6dd92375a32f23f4",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def MSE(out, y):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    out: output of network (n vector)\n",
    "    y: training labels (n vector)\n",
    "    \n",
    "    OUTPUTS:\n",
    "    \n",
    "    loss: the mse loss (a scalar)\n",
    "    \"\"\"\n",
    "    \n",
    "    n = len(y)\n",
    "    loss = 0\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "df5eaf37e290ff87fee5c8ce172bbf18",
     "grade": false,
     "grade_id": "cell-c9a62e7d5e1a53b7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def MSE_test1():\n",
    "    X, y = h.generate_data() # generate data\n",
    "    W = initweights([2, 3, 1]) # generate random weights\n",
    "    A, Z = forward_pass(W, X)\n",
    "    loss = MSE(Z[-1].flatten(), y) # calculate loss\n",
    "    \n",
    "    return np.isscalar(loss) # your loss should be a scalar\n",
    "\n",
    "def MSE_test2():\n",
    "    X, y = h.generate_data() # generate data\n",
    "    W = initweights([2, 3, 1]) # generate random weights\n",
    "    A, Z = forward_pass(W, X)\n",
    "    loss = MSE(Z[-1].flatten(), y) # calculate loss\n",
    "    \n",
    "    return loss >= 0 # your loss should be nonnegative\n",
    "\n",
    "def MSE_test3():\n",
    "    X, y = h.generate_data() # generate data\n",
    "    W = initweights([2, 3, 1]) # generate random weights\n",
    "    A, Z = forward_pass(W, X)\n",
    "    loss = MSE(Z[-1].flatten(), y) # calculate loss\n",
    "    loss_grader = h.MSE_grader(Z[-1].flatten(), y)\n",
    "    \n",
    "    # your loss should not deviate too much from ours\n",
    "    # If you fail this test case, check whether you divide your loss by 1/n\n",
    "    return np.absolute(loss - loss_grader) < 1e-7 \n",
    "\n",
    "h.runtest(MSE_test1, \"MSE_test1\")\n",
    "h.runtest(MSE_test2, \"MSE_test2\")\n",
    "h.runtest(MSE_test3, \"MSE_test3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bf3b62f7b063f02d811ea4da7b138691",
     "grade": true,
     "grade_id": "cell-6bd2bed88d4a1da2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# MSE_test1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3bcedf2cf1c40f87101a0712078123d5",
     "grade": true,
     "grade_id": "cell-b58fe55cb556a158",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# MSE_test2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c96a48c910d961722e695fcfe52f4348",
     "grade": true,
     "grade_id": "cell-63cef93910e2ac1e",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# MSE_test3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a146ee80b93bd89847231cc7559ed9c6",
     "grade": false,
     "grade_id": "cell-5dd2453b4b9f889c",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def MSE_grad(out, y):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    out: output of network (n vector)\n",
    "    y: training labels (n vector)\n",
    "    \n",
    "    OUTPUTS:\n",
    "    \n",
    "    grad: the gradient of the MSE loss with respect to out (nx1 vector)\n",
    "    \"\"\"\n",
    "    \n",
    "    n = len(y)\n",
    "    grad = np.zeros(n)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2920ea5c3f453ce4728a0901fb6bc1b2",
     "grade": false,
     "grade_id": "cell-649b459c6f2dd39f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def MSE_grad_test1():\n",
    "    X, y = h.generate_data() # generate data\n",
    "    \n",
    "    n, _ = X.shape\n",
    "    W = initweights([2, 3, 1]) # generate random weights\n",
    "    A, Z = forward_pass(W, X)\n",
    "    \n",
    "    grad = MSE_grad(Z[-1].flatten(), y)\n",
    "    return grad.shape == (n, )\n",
    "\n",
    "def MSE_grad_test2():\n",
    "    out = np.array([1])\n",
    "    y = np.array([1.2])\n",
    "    \n",
    "    # calculate numerical gradient using finite difference\n",
    "    numerical_grad = (MSE(out + 1e-7, y) - MSE(out - 1e-7, y)) / 2e-7\n",
    "    grad = MSE_grad(out, y)\n",
    "    \n",
    "    # check your gradient is close to the numerical gradient\n",
    "    return np.linalg.norm(numerical_grad - grad) < 1e-7\n",
    "\n",
    "def MSE_grad_test3():\n",
    "    X, y = h.generate_data() # generate data\n",
    "    \n",
    "    n, _ = X.shape\n",
    "    W = initweights([2, 3, 1]) # generate random weights\n",
    "    A, Z = forward_pass(W, X)\n",
    "    \n",
    "    grad = MSE_grad(Z[-1].flatten(), y)\n",
    "    grad_grader = h.MSE_grad_grader(Z[-1].flatten(), y) # compute gradient using our solution\n",
    "    \n",
    "    # your gradient should not deviate too much from ours\n",
    "    return np.linalg.norm(grad_grader - grad) < 1e-7\n",
    "\n",
    "h.runtest(MSE_grad_test1, 'MSE_grad_test1')\n",
    "h.runtest(MSE_grad_test2, 'MSE_grad_test2')\n",
    "h.runtest(MSE_grad_test3, 'MSE_grad_test3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e64b57d3f3dda596ed34efbd8e774efe",
     "grade": true,
     "grade_id": "cell-2d4b2c6f1b01e73e",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# MSE_grad_test1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c77b5039b5e6186e2becbf13f1ba13bd",
     "grade": true,
     "grade_id": "cell-e7874ea815832006",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# MSE_grad_test2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e50cf0b7c3febb93ea9f7377e5919ab2",
     "grade": true,
     "grade_id": "cell-1f54c87a5dda35d9",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# MSE_grad_test3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3. Backpropagation</h3>\n",
    "\n",
    "In this section, you are going to implement the <code>backprop</code> for a ReLU network specified by the weight <code>W</code>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e975169670d7446fcc82729e7188ff1f",
     "grade": false,
     "grade_id": "cell-e30f891ec0f25958",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def backprop(W, A, Z, y):\n",
    "    \"\"\"\n",
    "    \n",
    "    INPUT:\n",
    "    W weights (cell array)\n",
    "    A output of forward pass (cell array)\n",
    "    Z output of forward pass (cell array)\n",
    "    y vector of size n (each entry is a label)\n",
    "    \n",
    "    OUTPUTS:\n",
    "    \n",
    "    gradient = the gradient with respect to W as a cell array of matrices\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert delta to a row vector to make things easier\n",
    "    delta = (MSE_grad(Z[-1].flatten(), y) * 1).reshape(-1, 1)\n",
    "\n",
    "    # compute gradient with backprop\n",
    "    gradients = []\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop_test1():\n",
    "    X, y = h.generate_data() # generate data\n",
    "    \n",
    "    n, _ = X.shape\n",
    "    W = initweights([2, 3, 1]) # generate random weights\n",
    "    A, Z = forward_pass(W, X)\n",
    "    \n",
    "    gradient = backprop(W, A, Z, y) # backprop to calculate the gradient\n",
    "    \n",
    "    # You should return a list with the same len as W\n",
    "    return len(gradient) == len(W)\n",
    "\n",
    "def backprop_test2():\n",
    "    X, y = h.generate_data() # generate data\n",
    "    \n",
    "    n, _ = X.shape\n",
    "    W = initweights([2, 3, 1]) # generate random weights\n",
    "    A, Z = forward_pass(W, X)\n",
    "    \n",
    "    gradient = backprop(W, A, Z, y) # backprop to calculate the gradient\n",
    "    \n",
    "    # gradient[i] should match the shape of W[i]\n",
    "    return np.all([gradient[i].shape == W[i].shape for i in range(len(W))])\n",
    "\n",
    "def backprop_test3():\n",
    "    X, y = h.generate_data() # generate data\n",
    "    \n",
    "    n, _ = X.shape\n",
    "    \n",
    "    # Use a one layer network\n",
    "    # This is essentially the least squares\n",
    "    W = initweights([2, 1]) \n",
    "    \n",
    "    A, Z = forward_pass(W, X)\n",
    "    \n",
    "    # backprop to calculate the gradient\n",
    "    gradient = backprop(W, A, Z, y) \n",
    "    \n",
    "    # calculate the least square gradient\n",
    "    least_square_gradient = 2 *((X.T @ X) @ W[0] - X.T @ y.reshape(-1, 1)) / n\n",
    "    \n",
    "    # gradient[0] should be the least square gradient\n",
    "    return np.linalg.norm(gradient[0] - least_square_gradient) < 1e-7\n",
    "\n",
    "def backprop_test4():\n",
    "    X, y = h.generate_data() # generate data\n",
    "    \n",
    "    n, _ = X.shape\n",
    "    \n",
    "    # Use a one layer network\n",
    "    # This is essentially the least squares\n",
    "    W = initweights([2, 1]) \n",
    "    \n",
    "    A, Z = forward_pass(W, X)\n",
    "    \n",
    "    # backprop to calculate the gradient\n",
    "    gradient = backprop(W, A, Z, y) \n",
    "    \n",
    "    # calculate the least square gradient\n",
    "    gradient_grader = h.backprop_grader(W, A, Z, y)\n",
    "    \n",
    "    # Check whether your gradient matches ours\n",
    "    return np.linalg.norm(gradient[0] - gradient_grader[0]) < 1e-7\n",
    "\n",
    "h.runtest(backprop_test1, 'backprop_test1')\n",
    "h.runtest(backprop_test2, 'backprop_test2')\n",
    "h.runtest(backprop_test3, 'backprop_test3')\n",
    "h.runtest(backprop_test4, 'backprop_test4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8092d2f8f30461b1de9a6b913a00a1dd",
     "grade": true,
     "grade_id": "cell-fe238e88cab5594f",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# backprop_test1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "134296805940f3804e3ee23c2adc15a4",
     "grade": true,
     "grade_id": "cell-adbed70991c98f60",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# backprop_test2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3d71d785230a85b82595f1100d02c88d",
     "grade": true,
     "grade_id": "cell-ba302c9defbf583b",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# backprop_test3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1849561bb8aa0bb9fbb02c8d7d3ce829",
     "grade": true,
     "grade_id": "cell-d6049947c0cde001",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# backprop_test4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "22d12099273cffbe6cf4529e44b90855",
     "grade": false,
     "grade_id": "cell-5bea8cd4b42b1693",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3> 4. Training with Gradient Descent</h3>\n",
    "\n",
    "Run the cell below to train a ReLU Network for the task with Gradient Descent. Feel free to play around with the hyperparameters such learning rate, number of epochs, number of hidden layers, size of each hidden layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate for Gradient Descent\n",
    "lr = 0.001\n",
    "\n",
    "# Number of epochs\n",
    "# one epoch - one full pass through the dataset\n",
    "M = 10000\n",
    "\n",
    "# keep track of the losses\n",
    "losses = np.zeros(M)\n",
    "t0 = time.time()\n",
    "\n",
    "# initialize a neural network with one hidden layer \n",
    "# Try varying the depth and width of the neural networks to see the effect\n",
    "\n",
    "W = initweights([2, 200, 1])\n",
    "\n",
    "# Start training\n",
    "for i in range(M):\n",
    "    \n",
    "    # Do a forward pass\n",
    "    A, Z = forward_pass(W, X)\n",
    "    \n",
    "    # Calculate the loss\n",
    "    losses[i] = MSE(Z[-1].flatten(), y)\n",
    "    \n",
    "    # Calculate the loss using backprop\n",
    "    gradients = backprop(W, A, Z, y)\n",
    "    \n",
    "    # Update he parameters\n",
    "    for j in range(len(W)):\n",
    "        W[j] -= lr * gradients[j]\n",
    "t1 = time.time()\n",
    "print('Elapsed time: %.2fs' % (t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d9c1913097b5a9271f79c26174ccc632",
     "grade": false,
     "grade_id": "cell-db58a28187a02b84",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "h.plot_results(X[:, 0], y, Z, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
