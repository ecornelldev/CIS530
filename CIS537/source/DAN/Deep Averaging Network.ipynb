{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this Project\n",
    "\n",
    "In this project, you will implement a Deep Averaging Network to do sentiment classification.\n",
    "\n",
    "<h3>Evaluation</h3>\n",
    "\n",
    "<p><strong>This project must be successfully completed and submitted in order to receive credit for this course. Your score on this project will be included in your final grade calculation.</strong><p>\n",
    "    \n",
    "<p>You are expected to write code where you see <em># YOUR CODE HERE</em> within the cells of this notebook. Not all cells will be graded; code input cells followed by cells marked with <em>#Autograder test cell</em> will be graded. Upon submitting your work, the code you write at these designated positions will be assessed using an \"autograder\" that will run all test cells to assess your code. You will receive feedback from the autograder that will identify any errors in your code. Use this feedback to improve your code if you need to resubmit. Be sure not to change the names of any provided functions, classes, or variables within the existing code cells, as this will interfere with the autograder. Also, remember to execute all code cells sequentially, not just those you’ve edited, to ensure your code runs properly.</p>\n",
    "    \n",
    "<p>You can resubmit your work as many times as necessary before the submission deadline. If you experience difficulty or have questions about this exercise, use the Q&A discussion board to engage with your peers or seek assistance from the instructor.<p>\n",
    "\n",
    "<p>Before starting your work, please review <a href=\"https://s3.amazonaws.com/ecornell/global/eCornellPlagiarismPolicy.pdf\">eCornell's policy regarding plagiarism</a> (the presentation of someone else's work as your own without source credit).</p>\n",
    "\n",
    "<h3>Submit Code for Autograder Feedback</h3>\n",
    "\n",
    "<p>Once you have completed your work on this notebook, you will submit your code for autograder review. Follow these steps:</p>\n",
    "\n",
    "<ol>\n",
    "  <li><strong>Save your notebook.</strong></li>\n",
    "  <li><strong>Mark as Completed —</strong> In the blue menu bar along the top of this code exercise window, you’ll see a menu item called <strong>Education</strong>. In the <strong>Education</strong> menu, click <strong>Mark as Completed</strong> to submit your code for autograder/instructor review. This process will take a moment and a progress bar will show you the status of your submission.</li>\n",
    "\t<li><strong>Review your results —</strong> Once your work is marked as complete, the results of the autograder will automatically be presented in a new tab within the code exercise window. You can click on the assessment name in this feedback window to see more details regarding specific feedback/errors in your code submission.</li>\n",
    "  <li><strong>Repeat, if necessary —</strong> The Jupyter notebook will always remain accessible in the first tabbed window of the exercise. To reattempt the work, you will first need to click <strong>Mark as Uncompleted</strong> in the <strong>Education</strong> menu and then proceed to make edits to the notebook. Once you are ready to resubmit, follow steps one through three. You can repeat this procedure as many times as necessary.</li>\n",
    "\n",
    "<p>You can also download a copy of this notebook in multiple formats using the <strong>Download as</strong> option in the <strong>File</strong> menu above.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-212ad70811103da8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### We will do the following steps in order:\n",
    "1. Load training and test datasets\n",
    "2. Define a deep averaging network (TODO)\n",
    "3. Define a loss function and optimizer \n",
    "4. Train the network on the training data \n",
    "5. Test the network on the test data\n",
    "\n",
    "For this project, we have done most of the steps for you. What you need to do is to implement the forward pass of the deep averaging network in step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-614a1b162b50f1ed",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-18e299b53d44e9c6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Step 0: Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8a982dd433cb6b32",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "# import PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.utils.data\n",
    "\n",
    "# Set the seed for PyTorch random number generator\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# If gpu is supported, then seed the gpu random number generator as well\n",
    "gpu_available = torch.cuda.is_available()\n",
    "if gpu_available:\n",
    "    torch.cuda.manual_seed(1)\n",
    "    \n",
    "\n",
    "import helper as h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-69c08fd91b471012",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Step 1: Load Dataset\n",
    "\n",
    "We use the <a href=https://www.cs.jhu.edu/~mdredze/datasets/sentiment/>multi-domain sentiment</a> dataset created by Professor <a href=https://www.cs.jhu.edu/~mdredze/>Mark Dredze</a> for our project. This dataset contains product reviews taken from Amazon.com from many product types and the reviews are labeled positive and negative. In particular, we only consider the reviews for book for our project. To make things easier for you, we also created a dictionary where you will only consider the words in this dictionary when you are constructing the word embedding for your deep averaging network. Run the following two cells to load the data and see a positive and a negative review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cae72470a03f1d0b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Training Reviews:  1787\n",
      "Number of Test Reviews:  200\n",
      "Number of Words in the Vocabulary:  4380\n"
     ]
    }
   ],
   "source": [
    "# First load data\n",
    "# Calling h.load_data return the training review, test review and vocabulary\n",
    "# review_train and review_test are stored as pandas dataframe\n",
    "# vocabulary is dictionary with key-value pairs (word, index)\n",
    "# vocabulary[word] = index\n",
    "# We will use this vocabulary to construct bag-of-word (bow) features\n",
    "review_train, review_test, vocab = h.load_data()\n",
    "\n",
    "# label 0 == Negative reviews\n",
    "# label 1 == Positive reviews\n",
    "label_meaning = ['Negative', 'Positive']\n",
    "\n",
    "print('Number of Training Reviews: ', review_train.shape[0])\n",
    "print('Number of Test Reviews: ', review_test.shape[0])\n",
    "print('Number of Words in the Vocabulary: ', len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c05892644a77a852",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Positive Training Review:  This was perhaps the best of Johannes Steinhoff's books, since it does not  deal with his own stellar yet tragic WW II and post war career. The  insights of the average person living in Germany are of great importance to  both social and military historians alike. Steinhoff offered this  collective testament as a warning to all of us regarding war and the rise  of a dictator. As Johannes said in an interview, &quot;It is always the  civilians who suffer the most, yet are remembered the least.&quot\n",
      "\n",
      "A Negative Training Review:  I got to page 26 and gave up.  Lockes writings lack focus and are void of humour.  I read as much as I could with patience until it became clear this book was simply someone rambling on about nothing.  Save your money for something worth reading\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print some training review\n",
    "print('A Positive Training Review: ', review_train.iloc[0]['review'])\n",
    "print('A Negative Training Review: ', review_train.iloc[-1]['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8209f25427ee055e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We also created a function <code>h.generate_featurizer</code> takes in a vocabulary and return a bow featurizer based on the vocabulary. Using the returned featurizer, you can convert a sentence into a bag of word feature vector. See the following cell for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bf9a446569cfd0bc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 2, 1, 1]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a simple vocabulary\n",
    "simple_vocab = {'learn': 0, 'machine': 1, 'learning': 2, 'teach': 3}\n",
    "\n",
    "# Create a simple sentence that will be converted into bag of words features\n",
    "simple_sentence = ' I learn machine learning to teach machine how to learn.'\n",
    "\n",
    "# Create a featurizer by passing in the vocabulary\n",
    "simple_featurizer = h.generate_featurizer(simple_vocab)\n",
    "\n",
    "# Call simple_featurizer.transform to transform the sentence to its bag of word features\n",
    "simple_featurizer.transform([simple_sentence]).toarray()\n",
    "\n",
    "# You should get array([[2, 2, 1, 1]]) as output.\n",
    "# This means that the sentence has 2 occurences of 'learn', 2 occurences of 'machine', \n",
    "# 1 occurence of 'learning' and 1 occurence of 'teach'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2477750b684a6115",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now we will use <code>h.generate_featurizer</code> to generate a featurizer based on the vocabulary we provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6d837743526c112c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Use the given vocab to generate bag of word featurizer\n",
    "# See the next cell for an example\n",
    "bow_featurizer = h.generate_featurizer(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-65b53c8818c65572",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Using the featurizer, we will convert the training reviews and test reviews into bag of word representation and PyTorch Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2d941e1cd5976a2d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# convert the reviews to bow representation and torch Tensor\n",
    "X_train = torch.Tensor(bow_featurizer.transform(review_train['review'].values).toarray())\n",
    "y_train = torch.LongTensor(review_train['label'].values.flatten())\n",
    "\n",
    "X_test = torch.Tensor(bow_featurizer.transform(review_test['review'].values).toarray())\n",
    "y_test = torch.LongTensor(review_test['label'].values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4488693f8d463c80",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Generate PyTorch Datasets \n",
    "trainset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "testset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "\n",
    "# Generate PyTorch Dataloaders\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, drop_last=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2f35ad9e435c7f99",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Step 2: Implement a Deep Averaging Network [Graded]\n",
    "\n",
    "We have defined a PyTorch network class for you. What you need to do is to implement the forward pass for your deep averaging network. To start, first implement <code>average</code> that averages the words in a review and then implement <code>forward</code> that passes the \"averaged\" review to a linear layer to produce the model's belief.\n",
    "\n",
    "- For `average` recall that multiplying the matrix of the bag-of-words with the word embeddings will get you the embedded representations for the reviews. You then want to average over all the different words in a review to get an \"average\" embedding for each review.\n",
    "- For `forward`, pass the output of `average` through the linear layer stored in `self.fc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-59a3f21d71cce127",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Create a Deep Averaging network model class\n",
    "# embedding_size is the size of the word_embedding we are going to learn\n",
    "class DAN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size=32):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create a word-embedding of dimension embedding_size\n",
    "        self.embeds = torch.nn.Parameter(torch.randn(vocab_size, embedding_size))\n",
    "        self.embeds.requires_grad_(True)\n",
    "        \n",
    "        self.fc = nn.Linear(embedding_size, 2)\n",
    "        \n",
    "    def average(self, x):\n",
    "        '''\n",
    "        This function takes in a bag of word representation of reviews. For each review, it retrieves the word \n",
    "        embedding of each word in the review and average them. \n",
    "        \n",
    "        Input: \n",
    "            x: nxd torch Tensor where each row corresponds to bag of word representation of a review\n",
    "        \n",
    "        Output:\n",
    "            n x (embedding_size) torch Tensor for the averaged reivew \n",
    "        '''\n",
    "        \n",
    "        ### BEGIN SOLUTION\n",
    "        count = torch.sum(x, dim=1, keepdim=True)\n",
    "        emb = torch.matmul(x, self.embeds) / count\n",
    "        ### END SOLUTION\n",
    "        return emb\n",
    "          \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        This function takes in a bag of word representation of reviews. It calls the self.average to get the\n",
    "        averaged review and pass it through the linear layer to produce the model's belief.\n",
    "        \n",
    "        Input: \n",
    "            x: nxd torch Tensor where each row corresponds to bag of word representation of reviews\n",
    "        \n",
    "        Output:\n",
    "            nx2 torch Tensor that corresponds to model belief of the input. For instance, output[0][0] is\n",
    "            is the model belief that the 1st review is negative\n",
    "        '''\n",
    "        review_averaged = self.average(x)\n",
    "        \n",
    "        out = None\n",
    "        ### BEGIN SOLUTION\n",
    "        out = self.fc(review_averaged)\n",
    "        ### END SOLUTION\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Test: average_test1 ... ✔ Passed!\n",
      "Running Test: average_test2 ... ✔ Passed!\n",
      "Running Test: average_test3 ... ✔ Passed!\n",
      "Running Test: forward_test1 ... ✔ Passed!\n",
      "Running Test: forward_test2 ... ✔ Passed!\n"
     ]
    }
   ],
   "source": [
    "def average_test1():\n",
    "    n = 10 # number of reviews\n",
    "    vocab_size = 5 # vocab size\n",
    "    embedding_size = 32 # embedding size\n",
    "    model = DAN(vocab_size, embedding_size)\n",
    "    X = torch.rand(n, vocab_size)\n",
    "    \n",
    "    output_size = model.average(X).shape\n",
    "    \n",
    "    # the output of your forward function should be nx2\n",
    "    return output_size[0] == n and output_size[1] == embedding_size\n",
    "\n",
    "def average_test2():\n",
    "    n = 10 # number of reviews\n",
    "    vocab_size = 3 # vocab size\n",
    "    embedding_size = 5 # embedding size\n",
    "    model = DAN(vocab_size, embedding_size)\n",
    "    \n",
    "    # generate a simple input\n",
    "    X = torch.FloatTensor([\n",
    "        [1, 0, 0],\n",
    "        [0, 1, 0],\n",
    "        [0, 0, 1],\n",
    "        [1, 1, 1],\n",
    "    ])\n",
    "    \n",
    "    # Get the averaged reviews\n",
    "    averaged_reviews = model.average(X)\n",
    "    \n",
    "    # Given the input, we know that the first 3 rows corresponds to the first three words\n",
    "    # The last row should be the average of the three words\n",
    "    # The diff between the last row and the average of the first three rows should be small\n",
    "    diff = torch.sum((torch.mean(averaged_reviews[:3], dim=0) - averaged_reviews[3]) ** 2).item()\n",
    "    \n",
    "    return diff < 1e-5\n",
    "\n",
    "def average_test3():\n",
    "    n = 10 # number of reviews\n",
    "    vocab_size = 3 # vocab size\n",
    "    embedding_size = 5 # embedding size\n",
    "    model = DAN(vocab_size, embedding_size)\n",
    "    \n",
    "    # generate a simple input\n",
    "    X = torch.FloatTensor([\n",
    "        [1, 1, 1],\n",
    "        [2, 2, 2]\n",
    "    ])\n",
    "    \n",
    "    # Get the averaged reviews\n",
    "    averaged_reviews = model.average(X)\n",
    "    \n",
    "    # Since the 2nd review is a multiple of the first,\n",
    "    # The two averaged review should be the same\n",
    "    diff = torch.sum((averaged_reviews[0] - averaged_reviews[1])**2).item()\n",
    "    \n",
    "    return diff < 1e-5\n",
    "\n",
    "def forward_test1():\n",
    "    n = 10 # number of reviews\n",
    "    vocab_size = 5 # vocab size\n",
    "    embedding_size = 32 # embedding size\n",
    "    model = DAN(vocab_size, embedding_size)\n",
    "    \n",
    "    # call the forward function\n",
    "    X = torch.rand(n, vocab_size)\n",
    "    \n",
    "    output_size = model(X).shape\n",
    "    \n",
    "    # the output of your forward function should be nx2\n",
    "    return output_size[0] == n and output_size[1] == 2\n",
    "\n",
    "def forward_test2():\n",
    "    n = 10 # number of reviews\n",
    "    vocab_size = 5 # vocab size\n",
    "    embedding_size = 32 # embedding size\n",
    "    model = DAN(vocab_size, embedding_size)\n",
    "    X = torch.rand(n, vocab_size)\n",
    "    \n",
    "    logits = model(X) # get the output of your forward pass\n",
    "    \n",
    "    averaged_reviews = model.average(X) # get the intermediate averaged review\n",
    "    logits2 = model.fc(averaged_reviews) # get the model belief using your intermediate average reviews\n",
    "    \n",
    "    return torch.sum((logits - logits2)**2).item() < 1e-5 # Check whether your forward pass is implemented correctly\n",
    "\n",
    "h.runtest(average_test1, 'average_test1')\n",
    "h.runtest(average_test2, 'average_test2')\n",
    "h.runtest(average_test3, 'average_test3')\n",
    "h.runtest(forward_test1, 'forward_test1')\n",
    "h.runtest(forward_test2, 'forward_test2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-33cd7ef8f81ca3f3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# average_test1\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert average_test1()\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-519ac82b6d884e8c",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# average_test1\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert average_test2()\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-09c1b1e775a95146",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# average_test1\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert average_test3()\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f843a28a46679fea",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# forward_test1\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert forward_test1()\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-43e4534aa26dbdfe",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# forward_test1\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert forward_test2()\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5efa7383fb4c7bb7",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Create a model\n",
    "model = DAN(len(vocab), embedding_size=32)\n",
    "\n",
    "if gpu_available:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f34ae8bcfb938028",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Step 3: Define the loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-06935b2f38a60afb",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Create optimizer and loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-756fe2729112ef19",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Step 4: Train the network\n",
    "\n",
    "Run the following cell to train your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3374e25d9a304abf",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100 / 1000] Average Training Accuracy: 0.816106\n",
      "Epoch [100 / 1000] Average Training loss: 0.450116\n",
      "Epoch [200 / 1000] Average Training Accuracy: 0.996394\n",
      "Epoch [200 / 1000] Average Training loss: 0.025855\n",
      "Epoch [300 / 1000] Average Training Accuracy: 1.000000\n",
      "Epoch [300 / 1000] Average Training loss: 0.006785\n",
      "Epoch [400 / 1000] Average Training Accuracy: 1.000000\n",
      "Epoch [400 / 1000] Average Training loss: 0.003494\n",
      "Epoch [500 / 1000] Average Training Accuracy: 1.000000\n",
      "Epoch [500 / 1000] Average Training loss: 0.002226\n",
      "Epoch [600 / 1000] Average Training Accuracy: 1.000000\n",
      "Epoch [600 / 1000] Average Training loss: 0.001599\n",
      "Epoch [700 / 1000] Average Training Accuracy: 1.000000\n",
      "Epoch [700 / 1000] Average Training loss: 0.001239\n",
      "Epoch [800 / 1000] Average Training Accuracy: 1.000000\n",
      "Epoch [800 / 1000] Average Training loss: 0.001007\n",
      "Epoch [900 / 1000] Average Training Accuracy: 1.000000\n",
      "Epoch [900 / 1000] Average Training loss: 0.000838\n",
      "Epoch [1000 / 1000] Average Training Accuracy: 1.000000\n",
      "Epoch [1000 / 1000] Average Training loss: 0.000718\n"
     ]
    }
   ],
   "source": [
    "# Start Training\n",
    "num_epochs = 1000\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # Define the following variables to keep track of the running losses and accuracies\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    count = 0\n",
    "    \n",
    "    for i, (X, y) in enumerate(trainloader):\n",
    "        \n",
    "        # use gpu if necessary\n",
    "        if gpu_available:\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "        \n",
    "        # clear the gradient buffer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Do forward propagation to get the model's belief\n",
    "        logits = model(X)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = loss_fn(logits, y)\n",
    "        \n",
    "        # Run a backward propagation to get the gradient\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the model's parameter\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Get the model's prediction \n",
    "        pred = torch.argmax(logits,dim=1)\n",
    "        \n",
    "        # Update the running statistics\n",
    "        running_acc += torch.sum((pred == y).float()).item()\n",
    "        running_loss += loss.item()\n",
    "        count += X.size(0)\n",
    "        \n",
    "    # print the running statistics after training for 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print('Epoch [{} / {}] Average Training Accuracy: {:4f}'.format(epoch + 1, num_epochs, running_acc / count))\n",
    "        print('Epoch [{} / {}] Average Training loss: {:4f}'.format(epoch + 1, num_epochs, running_loss / len(trainloader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d8c31d9dc298f8dd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Step 5: Evaluate your model on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7a88e5112d7584a3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Test Accuracy is 0.9150\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "model.eval()\n",
    "\n",
    "# Define the following variable to keep track of the accuracy\n",
    "running_acc = 0.0\n",
    "count = 0.0\n",
    "\n",
    "for (X, y) in testloader:\n",
    "    # Use gpu if available\n",
    "    if gpu_available:\n",
    "        X = X.cuda()\n",
    "        y = y.cuda()\n",
    "\n",
    "    # Do a forward pass and tell PyTorch that no gradient is necessary to save memory\n",
    "    with torch.no_grad():\n",
    "        logits = model(X)\n",
    "    \n",
    "    # Calculate the prediction\n",
    "    pred = torch.argmax(logits,dim=1)\n",
    "    \n",
    "    # Update the running stats\n",
    "    running_acc += torch.sum((pred == y).float()).item()\n",
    "    count += X.size(0)\n",
    "\n",
    "print('Your Test Accuracy is {:.4f}'. format(running_acc / count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-60dca0660c0ba2c2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Run the following cells to see the a random test review and the model prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-705df6c45ad63992",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review:  ...if you're in the bathroom with time on your hands! I must admit that I did finish this book and was glad that the ordeal was over.  Puzo (like Leon Uris) definitely lost the touch near the end of his career and this book is no exception.  In just a few hundred pages, President Kennedy II is shown as a tyrant, a socialist, a reactionary, a humanitarian, a political bumbler, a visionary of USA utopia, a man who would kill a few thousand citizens to save himself from impeachment, a liar who can beat the ultimate lie detector, a dear and loving husband/father, etc....how many characters must one man be?  And what's this odd sub-plot of a former-Mormon/Hollywood oddball who on a whim (in less than 10 seconds of consideration when the opportunity presents itself) decides to kill the president of the United States.  Once again another character with fourteen different personalities!.  Puzo even got his basic facts wrong about the Mormons (he should have stuck with the Catholic Church). I would hate to be washed up on a deserted island and find this book washed up on the shore next to me! I would go crazy reading it again\n",
      "\n",
      "Ground Truth:  Negative\n",
      "Prediction:  Negative\n"
     ]
    }
   ],
   "source": [
    "target = torch.randint(high=len(testset), size=(1,)).item()\n",
    "\n",
    "review_target, label_target = review_test.iloc[target]\n",
    "bog_target = testset[target][0].unsqueeze(0).cuda()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits_target = model(bog_target)\n",
    "\n",
    "pred = torch.argmax(logits_target, dim=1)\n",
    "\n",
    "print('Review: ', review_target)\n",
    "print('Ground Truth: ', label_meaning[int(label_target)])\n",
    "print('Prediction: ', label_meaning[pred.item()])"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
