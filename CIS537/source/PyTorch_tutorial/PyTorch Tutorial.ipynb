{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>About this Exercise</h1>\n",
    "\n",
    "This tutorial will provide you with enough information to finish CIS537 projects that use [PyTorch](https://pytorch.org/) but it is not meant to be a comprehensive tutorial for PyTorch. For a complete tutorial on Pytorch, review PyTorch's official tutorial:\n",
    "https://pytorch.org/tutorials/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>What is PyTorch?</h3>\n",
    "\n",
    "PyTorch is a python-based deep learning platform that provides GPU support. To start, first import the PyTorch packages and some of its some packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PyTorch and its subpackages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Also other packages for convenience\n",
    "import numpy as np\n",
    "import helper as h\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>0. Tensors</h3>\n",
    "\n",
    "Tensors are the basic building blocks of PyTorch. They are similar to NumPy's ndarrays. For example, to create a tensor of all zeros with size (3, 2), you can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.zeros(3, 2) # this syntax is similar to NumPy. In Numpy, we would do np.zero(3,2)\n",
    "\n",
    "print(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also convert a NumPy array to a torch Tensor: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_numpy = np.arange(15)\n",
    "print('NumPy Array: ', X_numpy)\n",
    "\n",
    "X_tensor = torch.tensor(X_numpy)\n",
    "print('Pytorch Tensor: ', X_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also convert a torch Tensor to a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor_numpy = X_tensor.numpy()\n",
    "\n",
    "print(X_tensor_numpy)\n",
    "print(type(X_tensor_numpy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like NumPy, you can do arithmetic operations on torch Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two numpy arrays\n",
    "A = np.array([1, 2, 3])\n",
    "B = np.array([1, 5, 6])\n",
    "\n",
    "# Convert the two numpy arrays into torch Tensor\n",
    "A_tensor = torch.Tensor(A)\n",
    "B_tensor = torch.Tensor(B)\n",
    "\n",
    "# addition / subtraction\n",
    "print('Addition in Numpy', A + B)\n",
    "print('Addition in PyTorch', A_tensor + B_tensor)\n",
    "print()\n",
    "\n",
    "# scalar multiplication\n",
    "print('Scalar Multiplication in Numpy', 3*A)\n",
    "print('Scalar Multiplication in PyTorch', 3*A_tensor)\n",
    "print()\n",
    "\n",
    "# elementwise multiplication\n",
    "print('Elementwise Multiplication in Numpy', A*B)\n",
    "print('Elementiwse Multiplication in PyTorch', A_tensor*B_tensor)\n",
    "print()\n",
    "\n",
    "# matrix multiplication\n",
    "# this is slightly different from NumPy\n",
    "print('Elementwise Multiplication in Numpy', A@B)\n",
    "print('Elementiwse Multiplication in PyTorch', torch.matmul(A_tensor, B_tensor))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some other useful tensor operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elementwise comparison\n",
    "print('NumPy: ', A == B)\n",
    "print('PyTorch: ', A_tensor == B_tensor)\n",
    "print()\n",
    "\n",
    "# Generate a random matrix\n",
    "C = np.array([[10, 9, 8], [6, 7, 5], [1, 2, 3]])\n",
    "C_tensor = torch.Tensor(C)\n",
    "\n",
    "print('C', C)\n",
    "print()\n",
    "\n",
    "# Sum along the row\n",
    "# In NumPy, we specify the axis.\n",
    "# In PyTorch, we specify the dim\n",
    "print('NumPy: ', np.sum(C, axis=0))\n",
    "print('PyTorch: ' ,torch.sum(C_tensor, dim=0))\n",
    "print()\n",
    "\n",
    "# Find the mean along the column\n",
    "# In NumPy, we specify the axis.\n",
    "# In PyTorch, we specify the dim\n",
    "print('NumPy: ', np.mean(C, axis=1))\n",
    "print('PyTorch: ', torch.mean(C_tensor, dim=1))\n",
    "print()\n",
    "\n",
    "\n",
    "# Find the argmax along the column\n",
    "# In NumPy, we specify the axis.\n",
    "# In PyTorch, we specify the dim\n",
    "print('NumPy: ', np.argmax(C, axis=1))\n",
    "print('PyTorch: ', torch.argmax(C_tensor, dim=1))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1. Creating a model</h3>\n",
    "\n",
    "In the following sections, we are going to develop a multilayer perceptron for our favorite spiral dataset. To start, let's generate and visualize the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the following code with generate\n",
    "# 600 datapoints\n",
    "# 300 in class 0 \n",
    "# 300 in class 1\n",
    "X, y = h.spiraldata(N=300)\n",
    "\n",
    "h.visualize_2D(X.numpy(), y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will create a multilayer perceptron. In Pytorch, to create a model, we need to create a model class that overloads <code>nn.Module</code> and implement the <code>forward()</code> function that performs forward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model class named MLP\n",
    "# overloads nn.Module \n",
    "\n",
    "# This MLP is a simple multilayer perceptron with 3-hidden layers \n",
    "# with ReLU transition function\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        # call the constructor of the inherited module\n",
    "        # this line is always necessary\n",
    "        super().__init__()\n",
    "        \n",
    "        # All the layers with trainable parameters have to be \n",
    "        # defined in the constructor\n",
    "        # Create a linear layer that takes input of dimension 2\n",
    "        # and outputs a vector of dimension 64\n",
    "        # Similarly, create the consequent layers\n",
    "        self.fc1 = nn.Linear(in_features=2, out_features=64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=32)\n",
    "        self.fc3 = nn.Linear(in_features=32, out_features=16)\n",
    "        \n",
    "        # The last layer should output a two dimensional vector\n",
    "        # where the first dimension corresponds to the network's\n",
    "        # belief that the input belongs to class 0\n",
    "        # and similarly, second dimension corresponds to class 1\n",
    "        self.fc4 = nn.Linear(in_features=16, out_features=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # apply the first linear transformation to input x\n",
    "        # then apply the ReLU transition to get the output\n",
    "        # of the first hidden layer o1\n",
    "        # Similar procedures are done to get o2, o3\n",
    "        o1 = F.relu(self.fc1(x))\n",
    "        o2 = F.relu(self.fc2(o1))\n",
    "        o3 = F.relu(self.fc3(o2))\n",
    "        \n",
    "        # We do not apply any transition function to the output\n",
    "        # of the final hidden layer\n",
    "        o4 = self.fc4(o3)\n",
    "        return o4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can instantiate a model and do forward propagation by running the following cell: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a model\n",
    "model = MLP()\n",
    "\n",
    "# Run forward propagation\n",
    "logits = model(X)\n",
    "\n",
    "# Print the output of the model\n",
    "# This should be (600, 2)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2. Define a loss function</h3>\n",
    "\n",
    "To train a neural network, we need a loss function. Since we are doing a classification problem, we are going to use the cross entropy loss. Luckily, Pytorch has implemented this for us. For a comprehensive list of PyTorch implemented loss function, please check out: https://pytorch.org/docs/1.1.0/nn.html#loss-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the cross entropy loss function\n",
    "# this loss function takes in logits and labels and return the loss value\n",
    "# Concretely, loss_fn(logits, y) returns the cross entropy loss between\n",
    "# the prediction and ground truth\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3. Define an optimizer </h3>\n",
    "We are going to use an SGD optimizer to optimize our MLP. Conveniently, PyTorch has implemented SGD for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.optim.SGD is the optimizer that PyTorch implemented\n",
    "# it takes the model's parameter, which you can get by calling\n",
    "# model.parameters()\n",
    "# and learning rate lr\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>4. Training the Network </h3>\n",
    "\n",
    "With all the ingredients we have, we are ready to train our multilayer perceptrons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Num_epochs is the number we are going to go through the dataset\n",
    "# This is a simple dataset so we are just going to do a vanilla Gradient Descent\n",
    "# rather than minibatch Stochastic Gradient Descent\n",
    "num_epochs = 2000\n",
    "\n",
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    # Set the model to training mode\n",
    "    # This step is always necessary during training \n",
    "    model.train()\n",
    "    \n",
    "    # PyTorch keeps track of all the gradients generated \n",
    "    # throughout the whole training procedure\n",
    "    # But we only need the gradient of the current time step\n",
    "    # so we need to zero-out the gradient buffer\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Do forward propagation\n",
    "    logits = model(X)\n",
    "    \n",
    "    # Calculate the loss \n",
    "    loss = loss_fn(logits, y)\n",
    "    \n",
    "    # Do backward propagation\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the model parameters \n",
    "    optimizer.step()\n",
    "    \n",
    "    # Get the prediction\n",
    "    prediction = torch.argmax(logits, dim=1)\n",
    "    \n",
    "    # PyTorch supports elements elementwise comparison\n",
    "    acc = torch.mean((prediction == y).float())\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # print the loss value every 50 epochs\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print('Epoch [{}/ {}], Loss: {:.4f} '.format(epoch + 1, num_epochs, losses[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training loss\n",
    "plt.plot(range(1, num_epochs + 1), losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>5. Evaluate the Network </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to go into eval mode\n",
    "model.eval()\n",
    "\n",
    "# Do a forward pass and tell PyTorch to not keep track of\n",
    "# the gradient calculation to save memory\n",
    "with torch.no_grad():\n",
    "    logits = model(X)\n",
    "\n",
    "# make prediction\n",
    "prediction = torch.argmax(logits, dim=1)\n",
    "\n",
    "# Calculate the accracuy\n",
    "print('Accuracy: {:.2f}%'.format(torch.mean((prediction == y).float()).item() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualize the model\n",
    "h.visclassifier(model, X.numpy(), y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>(Optional) Exercise</h3>\n",
    "\n",
    "In the following cell, you will create another multilayer perceptron with 2 hidden layers. The first hidden layer has 256 neurons and the second hidden layer has 128 neurons. We have provided the following code stub. What you need to is to fill in the blanks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_v2(nn.Module):\n",
    "    def __init__(self):\n",
    "        # call the constructor of the inherited module\n",
    "        # this line is always necessary\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: fill in the blank with the right layer\n",
    "        self.fc1 = None\n",
    "        self.fc2 = None\n",
    "        self.fc3 = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        o1 = F.relu(self.fc1(x))\n",
    "        o2 = F.relu(self.fc2(o1))\n",
    "        o3 = self.fc3(o2)\n",
    "        return o3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v2 = MLP_v2()\n",
    "\n",
    "# If you create the right model, you should be able to pass the following test\n",
    "\n",
    "# Checking the size of the parameters in the first layer\n",
    "fc1_parameters = list(model_v2.fc1.parameters())\n",
    "\n",
    "# the weight should have size (256, 2)\n",
    "assert fc1_parameters[0].shape == (256, 2)\n",
    "# the bias should have size (256, )\n",
    "assert fc1_parameters[1].shape == (256, )\n",
    "\n",
    "# Checking the size of the parameters in the second layer\n",
    "fc2_parameters = list(model_v2.fc2.parameters())\n",
    "assert fc2_parameters[0].shape == (128, 256)\n",
    "assert fc2_parameters[1].shape == (128, )\n",
    "\n",
    "# Checking the size of the parameters in the last layer\n",
    "fc3_parameters = list(model_v2.fc3.parameters())\n",
    "assert fc3_parameters[0].shape == (2, 128)\n",
    "assert fc3_parameters[1].shape == (2, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a new optimizer\n",
    "# Feel free to play around with the learning rate\n",
    "optimizer_v2 = torch.optim.SGD(model_v2.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "num_epochs = 2000\n",
    "\n",
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    # Set the model to training mode\n",
    "    # This step is always necessary during training \n",
    "    model_v2.train()\n",
    "    \n",
    "    # PyTorch keeps track of all the gradients generated \n",
    "    # throughout the whole training procedure\n",
    "    # But we only need the gradient of the current time step\n",
    "    # so we need to zero-out the gradient buffer\n",
    "    optimizer_v2.zero_grad()\n",
    "    \n",
    "    # Do forward propagation\n",
    "    logits = model_v2(X)\n",
    "    \n",
    "    # Calculate the loss \n",
    "    loss = loss_fn(logits, y)\n",
    "    \n",
    "    # Do backward propagation\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the model parameters \n",
    "    optimizer_v2.step()\n",
    "    \n",
    "    # Get the prediction\n",
    "    prediction = torch.argmax(logits, dim=1)\n",
    "    \n",
    "    # PyTorch supports elements elementwise comparison\n",
    "    acc = torch.mean((prediction == y).float())\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # print the loss value every 50 epochs\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print('Epoch [{}/ {}], Loss: {:.4f} '.format(epoch + 1, num_epochs, losses[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "# Need to go into eval mode\n",
    "model_v2.eval()\n",
    "\n",
    "# Do a forward pass and tell PyTorch to not keep track of\n",
    "# the gradient calculation to save memory\n",
    "with torch.no_grad():\n",
    "    logits = model_v2(X)\n",
    "\n",
    "# make prediction\n",
    "prediction = torch.argmax(logits, dim=1)\n",
    "\n",
    "# Calculate the accracuy\n",
    "print('Accuracy: {:.2f}%'.format(torch.mean((prediction == y).float()).item() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the model\n",
    "h.visclassifier(model_v2, X.numpy(), y.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
