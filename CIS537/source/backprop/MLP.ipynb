{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>About this Project</h2>\n",
    "\n",
    "<p>In this project, you will implement a simple multilayer perceptron for a regression problem.</p>\n",
    "\n",
    "<h3>Evaluation</h3>\n",
    "\n",
    "<p><strong>This project must be successfully completed and submitted in order to receive credit for this course. Your score on this project will be included in your final grade calculation.</strong><p>\n",
    "    \n",
    "<p>You are expected to write code where you see <em># YOUR CODE HERE</em> within the cells of this notebook. Not all cells will be graded; code input cells followed by cells marked with <em>#Autograder test cell</em> will be graded. Upon submitting your work, the code you write at these designated positions will be assessed using an \"autograder\" that will run all test cells to assess your code. You will receive feedback from the autograder that will identify any errors in your code. Use this feedback to improve your code if you need to resubmit. Be sure not to change the names of any provided functions, classes, or variables within the existing code cells, as this will interfere with the autograder. Also, remember to execute all code cells sequentially, not just those you’ve edited, to ensure your code runs properly.</p>\n",
    "    \n",
    "<p>You can resubmit your work as many times as necessary before the submission deadline. If you experience difficulty or have questions about this exercise, use the Q&A discussion board to engage with your peers or seek assistance from the instructor.<p>\n",
    "\n",
    "<p>Before starting your work, please review <a href=\"https://s3.amazonaws.com/ecornell/global/eCornellPlagiarismPolicy.pdf\">eCornell's policy regarding plagiarism</a> (the presentation of someone else's work as your own without source credit).</p>\n",
    "\n",
    "<h3>Submit Code for Autograder Feedback</h3>\n",
    "\n",
    "<p>Once you have completed your work on this notebook, you will submit your code for autograder review. Follow these steps:</p>\n",
    "\n",
    "<ol>\n",
    "  <li><strong>Save your notebook.</strong></li>\n",
    "  <li><strong>Mark as Completed —</strong> In the blue menu bar along the top of this code exercise window, you’ll see a menu item called <strong>Education</strong>. In the <strong>Education</strong> menu, click <strong>Mark as Completed</strong> to submit your code for autograder/instructor review. This process will take a moment and a progress bar will show you the status of your submission.</li>\n",
    "\t<li><strong>Review your results —</strong> Once your work is marked as complete, the results of the autograder will automatically be presented in a new tab within the code exercise window. You can click on the assessment name in this feedback window to see more details regarding specific feedback/errors in your code submission.</li>\n",
    "  <li><strong>Repeat, if necessary —</strong> The Jupyter notebook will always remain accessible in the first tabbed window of the exercise. To reattempt the work, you will first need to click <strong>Mark as Uncompleted</strong> in the <strong>Education</strong> menu and then proceed to make edits to the notebook. Once you are ready to resubmit, follow steps one through three. You can repeat this procedure as many times as necessary.</li>\n",
    "    \n",
    "<p>You can also download a copy of this notebook in multiple formats using the <strong>Download as</strong> option in the <strong>File</strong> menu above.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Getting Started</h2>\n",
    "<h3>Python Initialization</h3> \n",
    "\n",
    "Please run the following code to initialize your Python kernel. You should be running a version of Python 3.x. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3fe0f85f2526afff",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-05ec67dd9c38e8fd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "import numpy as np\n",
    "from numpy.matlib import repmat\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "import time\n",
    "\n",
    "from helper import *\n",
    "\n",
    "%matplotlib notebook\n",
    "print('You\\'re running python %s' % sys.version.split(' ')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d5f8224f284306d3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Visualizing Data</h3>\n",
    "<p>In the cell below, you generate a simple 1-dimensional toy dataset by calling <code>generate_data</code>. This function returns the data $\\mathbf{X}$ and label $\\mathbf{y}$. Note that $X$ is of shape (N, 2). We append 1 to each example to introduce bias.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-07e24f138d476ac4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "X, y = generate_data()\n",
    "\n",
    "print(f'The shape of X is {X.shape}. This is because we append 1 to each feature vector to introduce bias!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-22f2e1e5eea4b07b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(X[:, 0], y, '*')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6aae1a46a88cfb3d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2>Multilayer Perceptron</h2>\n",
    "\n",
    "<h3>Part Zero: Transition Function and Transition Function's gradient</h3>\n",
    "<p>Transition functions are the key component of a neural network that contributes to its nonlinearity. For our neural network, we are going to use the ReLU transition function. Recall that the ReLU transition is as follows:</p>\n",
    "$$\\sigma(z) = \\max(z, 0)$$\n",
    "\n",
    "<p>We have implemented the <code>ReLU</code> and <code>ReLU_grad</code> functions as follow: (As its name suggests,  <code>ReLU_grad</code> computes the gradient of the ReLU function.)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e29d6d493186387e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def ReLU(z):\n",
    "    return np.maximum(z, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0e334778f4924bd7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def ReLU_grad(z):\n",
    "    return (z > 0).astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d719110877776b79",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h4>Visualize the Activation Function</h4>\n",
    "We can now visualize the activation function in the plot above by running the code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ea052350528f5135",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(-4, 4, 1000), ReLU(np.linspace(-4, 4, 1000)),'b-')\n",
    "plt.plot(np.linspace(-4, 4, 1000), ReLU_grad(np.linspace(-4, 4, 1000)),'r-')\n",
    "plt.xlabel('z')\n",
    "plt.ylabel(r'$\\max$ (z, 0)')\n",
    "plt.legend(['ReLU','ReLU_grad'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also display the function and its gradient on a small example vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array([2.7,-0.5,-3.2])\n",
    "print(\"X:\",x)\n",
    "print(\"ReLU(X):\",ReLU(x))\n",
    "print(\"ReLU_grad(X):\",ReLU_grad(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a07168b188ef1922",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Part One: Forward Pass [Graded]</h3>\n",
    "\n",
    "In this section, you will implement the forward pass function <code>forward_pass(W, xTr)</code>. Note that $\\mathbf{W}$ specifies the weights of the network at each layer. More specifically, $W[0]$ stores the weights for the first layer of the network, $W[i]$ stores the weights of the (i + 1)-th layer and $W[l-1]$ stores the weights of the last layer.\n",
    "\n",
    "Each layer of the network produces two outputs, $A[i + 1]$ and \n",
    "$Z[i + 1]$, where \n",
    "\n",
    "$$A[i + 1]=Z[i] * W[i]$$ \n",
    "for $i = 0, 1, 2, ..., l-1$ and \n",
    "$$Z[i+1]=\\sigma(A[i+1])$$ for $i = 0, 1, 2, ..., l-2$ and \n",
    "$$Z[l-1]=A[l-1]$$\n",
    "\n",
    "\n",
    "Here, $*$ stands for matrix multiplication and $Z[0], A[0]$ are both initialized to be the the training set.\n",
    "\n",
    "For simplicity, we did not include bias when calculating $A[i + 1]$. For the purpose of this assignment, this is fine since we have appended one in all the raw features and  our dataset is rather simple. In general, bias should always be included when calculating $A[i + 1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7a43a3f723e9b98a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "To visualize the variables `A`, `Z`, and `W` a bit better, consider the following hypothetical neural net layer:\n",
    "![nnlayer.png](nnlayer.png)\n",
    "\n",
    "It is important to note that `W[i]` is an *array of matrices*, as specified in the docstring for the provided method `initweights`. Since layer `i` has 2 nodes and layer `i+1` has 3, `W[i]` is a 2 by 3 matrix.\n",
    "\n",
    "Additionally, $\\sigma$ is applied elementwise to the values in `A[i+1]`, and is `ReLU` for this assignment. The final output at the end of the neural network should be `Z[l]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b652c9590b2cfe79",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The following function will allow you to randomly generate initial weights for the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ce4c1f90a8b0779c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def initweights(specs):\n",
    "    \"\"\"\n",
    "    Given a specification of the neural network, output a random weight array\n",
    "    INPUT:\n",
    "        specs - array of length m+1. specs[0] should be the dimension of the feature and spec[-1] \n",
    "                should be the dimension of output\n",
    "    \n",
    "    OUTPUT:\n",
    "        W - array of length m, each element is a matrix\n",
    "            where size(weights[i]) = (specs[i], specs[i+1])\n",
    "    \"\"\"\n",
    "    W = []\n",
    "    for i in range(len(specs) - 1):\n",
    "        W.append(np.random.randn(specs[i], specs[i+1]))\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-679cd28956259b78",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# If we want to create a network that \n",
    "#   i) takes in feature of dimension 2\n",
    "#   ii) has 1 hidden layer with 3 hidden units\n",
    "#   iii) output a scalar\n",
    "# then we initialize the the weights the following way:\n",
    "\n",
    "W = initweights([2, 3, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1e5948cf4b4cf360",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In the cell below, implement the forward pass function <code>forward_pass(W, xTr)</code>. The below is the pseudocode from the READ module: \n",
    "\n",
    "![fwdpass.png](fwdpass.png)\n",
    "\n",
    "<h3>Important</h3>\n",
    "A forward pass is used to evaluate a neural network on an input. However, here, we want to do a forward pass also to train a neural network, i.e. we will follow it with a backward pass. Therefore, unlike the pseudocode from the READ module, our code requires two changes. We output `A` (a <b>list</b> of the outputs at each layer after multiplying by the weights) and `Z` (a <b>list</b> of all outputs at each layer after passing the results of `A` through our transition function). To be precise, the outputs to your function should be $A=[a_1,a_2,\\dots,a_L]$ and $Z=[z_1,z_2,\\dots,z_L]$. The reason we need these intermediate results is that it is much more efficient to not recompute them when we use them during the back-prop pass. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-forward_pass",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def forward_pass(W, xTr):\n",
    "    \"\"\"\n",
    "    function forward_pass(weights,xTr)\n",
    "    \n",
    "    INPUT:\n",
    "    W - an array of L weight matrices\n",
    "    xTr - nxd matrix. Each row is an input vector\n",
    "    \n",
    "    OUTPUTS:\n",
    "    A - a list of matrices (of length L) that stores result of matrix multiplication at each layer \n",
    "    Z - a list of matrices (of length L) that stores result of transition function at each layer \n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize A and Z\n",
    "    A = [xTr]\n",
    "    Z = [xTr]\n",
    "    ### BEGIN SOLUTION\n",
    "    for i in range(len(W)):\n",
    "        a = Z[i] @ W[i]\n",
    "        A.append(a)\n",
    "        \n",
    "        if i < len(W) - 1:\n",
    "            z = ReLU(a)\n",
    "        else:\n",
    "            z = a\n",
    "\n",
    "        Z.append(z)\n",
    "    ### END SOLUTION\n",
    "    return A, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-forward_pass-selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def forward_test1():\n",
    "    X, _ = generate_data() # generate data\n",
    "    W = initweights([2, 3, 1]) # generate random weights\n",
    "    out = forward_pass(W, X) # run forward pass\n",
    "    return len(out) == 2 # make sure that your function return a tuple\n",
    "\n",
    "def forward_test2():\n",
    "    X, _ = generate_data() # generate data\n",
    "    W = initweights([2, 3, 1]) # generate random weights\n",
    "    A, Z = forward_pass(W, X) # run forward pass\n",
    "    return len(A) == 3 and len(Z) == 3 # Make sure that output produced match the length of the weight\n",
    "\n",
    "def forward_test3():\n",
    "    X, _ = generate_data() # generate data\n",
    "    n, _ = X.shape\n",
    "    W = initweights([2, 3, 1]) # generate random weights\n",
    "    A, Z = forward_pass(W, X) # run forward pass\n",
    "    return (A[1].shape == (n, 3) and \n",
    "            Z[1].shape == (n, 3)  and\n",
    "            A[2].shape == (n, 1) and\n",
    "            A[2].shape == (n, 1) ) # Make sure the layer produce the right shape output\n",
    "\n",
    "def forward_test4():\n",
    "    X = -1*np.ones((1, 2)) # generate a feature matrix of all negative ones\n",
    "    W = [np.ones((2, 1))] # a single layer network with weights one\n",
    "    A, Z = forward_pass(W, X) # run forward pass\n",
    "    \n",
    "    # check whether you do not apply the transition function to A[-1] \n",
    "    return np.linalg.norm(Z[-1] - X@W[0]) < 1e-7\n",
    "\n",
    "def forward_test5():\n",
    "    X, _ = generate_data() # generate data\n",
    "    n, _ = X.shape\n",
    "    W = initweights([2, 3, 1]) # generate random weights\n",
    "    A, Z = forward_pass(W, X) # run your forward pass\n",
    "    A_grader, Z_grader = forward_pass_grader(W, X) # run our forward pass\n",
    "    \n",
    "    Adiff = 0\n",
    "    Zdiff = 0\n",
    "    \n",
    "    # compute the difference between your solution and ours\n",
    "    for i in range(1, 3):\n",
    "        Adiff += np.linalg.norm(A[i] - A_grader[i])\n",
    "        Zdiff += np.linalg.norm(Z[i] - Z_grader[i])\n",
    "        \n",
    "    return Adiff < 1e-7 and Zdiff < 1e-7\n",
    "\n",
    "runtest(forward_test1, \"forward_test1\")\n",
    "runtest(forward_test2, \"forward_test2\")\n",
    "runtest(forward_test3, \"forward_test3\")\n",
    "runtest(forward_test4, \"forward_test4\")\n",
    "runtest(forward_test5, \"forward_test5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-Forward_test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# Forward_test1\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert forward_test1()\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-Forward_test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# Forward_test2\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert forward_test2()\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-Forward_test3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# Forward_test3\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert forward_test3()\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-Forward_test4",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# Forward_test4\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert forward_test4()\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-Forward_test5",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# Forward_test5\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert forward_test5()\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d74c4dde8e4ec273",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Part Two: MSE Loss and Regression</h3>\n",
    "<h4>Loss Function [Graded]</h4>\n",
    "\n",
    "In this section, you are going to implement the Mean Squared Error (MSE) loss function for regression. Recall that for a set of training example $\\{(\\mathbf{x}_1, y_1), ..., (\\mathbf{x}_n, y_n)\\}$, the MSE of the network $h$ is \n",
    "\n",
    "$$L = \\frac{1}{n} \\sum_{i = 1} ^n(h(\\mathbf{x}_i) - y_i)^2$$\n",
    "\n",
    "The <code>MSE</code> function you are going to implement takes the output of the network (<code>out</code>) and the training labels (<code>y</code>) and computes the MSE loss. You will also need to implement the <code>MSE_grad</code> function that will calculate the gradient of the MSE loss with respect to each entry of `out`. This function will be useful when you implement backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-MSE",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def MSE(out, y):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    out: output of network (n vector)\n",
    "    y: training labels (n vector)\n",
    "    \n",
    "    OUTPUTS:\n",
    "    \n",
    "    loss: the mse loss (a scalar)\n",
    "    \"\"\"\n",
    "    \n",
    "    n = len(y)\n",
    "    loss = 0\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    loss = np.mean((out - y) ** 2)\n",
    "    ### END SOLUTION\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-MSE-selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def MSE_test1():\n",
    "    X, y = generate_data() # generate data\n",
    "    W = initweights([2, 3, 1]) # generate random weights\n",
    "    A, Z = forward_pass(W, X)\n",
    "    loss = MSE(Z[-1].flatten(), y) # calculate loss\n",
    "    \n",
    "    return np.isscalar(loss) # your loss should be a scalar\n",
    "\n",
    "def MSE_test2():\n",
    "    X, y = generate_data() # generate data\n",
    "    W = initweights([2, 3, 1]) # generate random weights\n",
    "    A, Z = forward_pass(W, X)\n",
    "    loss = MSE(Z[-1].flatten(), y) # calculate loss\n",
    "    \n",
    "    return loss >= 0 # your loss should be nonnegative\n",
    "\n",
    "def MSE_test3():\n",
    "    X, y = generate_data() # generate data\n",
    "    W = initweights([2, 3, 1]) # generate random weights\n",
    "    A, Z = forward_pass(W, X)\n",
    "    loss = MSE(Z[-1].flatten(), y) # calculate loss\n",
    "    loss_grader = MSE_grader(Z[-1].flatten(), y)\n",
    "    \n",
    "    # your loss should not deviate too much from ours\n",
    "    # If you fail this test case, check whether you divide your loss by 1/n\n",
    "    return np.absolute(loss - loss_grader) < 1e-7 \n",
    "\n",
    "runtest(MSE_test1, \"MSE_test1\")\n",
    "runtest(MSE_test2, \"MSE_test2\")\n",
    "runtest(MSE_test3, \"MSE_test3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-MSE_test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# MSE_test1\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert MSE_test1()\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-MSE_test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# MSE_test2\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert MSE_test2()\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-MSE_test3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# MSE_test3\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert MSE_test3()\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MSE gradient [Graded]\n",
    "\n",
    "You will now need to implement the gradient of the MSE loss calculated above. When you take the derivative of the above with respect to the entries of out, you get the following:\n",
    "$$\\nabla L = \\frac{2}{n} * (h(\\mathbf{x}_i) - y_i)$$\n",
    "\n",
    "Implement the above in the function `MSE_grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-MSE_grad",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def MSE_grad(out, y):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    out: output of network (n vector)\n",
    "    y: training labels (n vector)\n",
    "    \n",
    "    OUTPUTS:\n",
    "    \n",
    "    grad: the gradient of the MSE loss with respect to out (nx1 vector)\n",
    "    \"\"\"\n",
    "    \n",
    "    n = len(y)\n",
    "    grad = np.zeros(n)\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    grad = 2 * (out - y) / n\n",
    "    ### END SOLUTION\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-MSE_grad-selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def MSE_grad_test1():\n",
    "    X, y = generate_data() # generate data\n",
    "    \n",
    "    n, _ = X.shape\n",
    "    W = initweights([2, 3, 1]) # generate random weights\n",
    "    A, Z = forward_pass(W, X)\n",
    "    \n",
    "    grad = MSE_grad(Z[-1].flatten(), y)\n",
    "    return grad.shape == (n, ) # check if the gradient has the right shape\n",
    "\n",
    "def MSE_grad_test2():\n",
    "    out = np.array([1])\n",
    "    y = np.array([1.2])\n",
    "    \n",
    "    # calculate numerical gradient using finite difference\n",
    "    numerical_grad = (MSE(out + 1e-7, y) - MSE(out - 1e-7, y)) / 2e-7\n",
    "    grad = MSE_grad(out, y)\n",
    "    \n",
    "    # check your gradient is close to the numerical gradient\n",
    "    return np.linalg.norm(numerical_grad - grad) < 1e-7\n",
    "\n",
    "def MSE_grad_test3():\n",
    "    X, y = generate_data() # generate data\n",
    "    \n",
    "    n, _ = X.shape\n",
    "    W = initweights([2, 3, 1]) # generate random weights\n",
    "    A, Z = forward_pass(W, X)\n",
    "    \n",
    "    grad = MSE_grad(Z[-1].flatten(), y)\n",
    "    grad_grader = MSE_grad_grader(Z[-1].flatten(), y) # compute gradient using our solution\n",
    "    \n",
    "    # your gradient should not deviate too much from ours\n",
    "    return np.linalg.norm(grad_grader - grad) < 1e-7\n",
    "\n",
    "runtest(MSE_grad_test1, 'MSE_grad_test1')\n",
    "runtest(MSE_grad_test2, 'MSE_grad_test2')\n",
    "runtest(MSE_grad_test3, 'MSE_grad_test3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-MSE_grad-test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# MSE_grad_test1\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert MSE_grad_test1()\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-MSE_grad-test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# MSE_grad_test2\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert MSE_grad_test2()\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-MSE_grad-test3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# MSE_grad_test3\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert MSE_grad_test3()\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Part Three: Backpropagation [Graded]</h3>\n",
    "\n",
    "In this section, you are going to implement the <code>backprop</code> for a ReLU network specified by the weight <code>W</code>. To recap, this is the stage that updates all the weights in the network starting from the last layer. \n",
    "\n",
    "Again, our implementation will have slight tweaks from the one in the READ module on back propagation. \n",
    "* Recall that in backpropagation, we alternate between calculating a \"link\" ( $\\frac{\\partial \\mathcal L}{\\partial \\mathbf a_\\ell}$) and updating the weight ($\\mathbf W_\\ell = \\mathbf W_\\ell -\\alpha \\frac{\\partial \\mathcal L}{\\partial \\mathbf W_\\ell}$). In this function, you only calculate the update $\\frac{\\partial \\mathcal L}{\\partial \\mathbf W_\\ell}$, **without applying it**. In other words, you store all the $\\frac{\\partial \\mathcal L}{\\partial \\mathbf W_\\ell}$ for all $1 \\le \\ell \\le L$, and return them all in the list `gradients`.\n",
    "* We have absorbed $b$ into $\\mathbf W$, so you don't need to calculate its gradient explicitly (thus, you should just ignore the second line in the for loop of the pseudocode!) \n",
    "* In our code `delta` = $\\vec \\delta _ \\ell = \\frac{\\partial \\mathcal L}{\\partial \\mathbf a_\\ell}$. The line initializing it has been done for you, so you only need to implement the for loop from the pseudocode (copied below).\n",
    "\n",
    "![backpass.png](backpass.png)\n",
    "\n",
    "Hint: In python you can multiply two matrices <code>D,B</code> element-wise with <code>D*B</code>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-backprop",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def backprop(W, A, Z, y):\n",
    "    \"\"\"\n",
    "    \n",
    "    INPUT:\n",
    "    W weights (cell array)\n",
    "    A output of forward pass (cell array)\n",
    "    Z output of forward pass (cell array)\n",
    "    y vector of size n (each entry is a label)\n",
    "    \n",
    "    OUTPUTS:\n",
    "    \n",
    "    gradient = the gradient with respect to W as a cell array of matrices\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert delta to a row vector to make things easier\n",
    "    delta = (MSE_grad(Z[-1].flatten(), y) * 1).reshape(-1, 1)\n",
    "\n",
    "    # compute gradient with backprop\n",
    "    gradients = []\n",
    "    \n",
    "    # BEGIN SOLUTION\n",
    "    for i in range(len(W)-1, -1, -1):\n",
    "        gradients.append((Z[i].T) @ (delta))\n",
    "        delta = ReLU_grad(A[i]) * (delta@(W[i].T))\n",
    "        \n",
    "    gradients = gradients[::-1]\n",
    "    # END SOLUTION\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-backprop-selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def backprop_test1():\n",
    "    X, y = generate_data() # generate data\n",
    "    \n",
    "    n, _ = X.shape\n",
    "    W = initweights([2, 3, 1]) # generate random weights\n",
    "    A, Z = forward_pass(W, X)\n",
    "    \n",
    "    gradient = backprop(W, A, Z, y) # backprop to calculate the gradient\n",
    "    \n",
    "    # You should return a list with the same len as W\n",
    "    return len(gradient) == len(W)\n",
    "\n",
    "def backprop_test2():\n",
    "    X, y = generate_data() # generate data\n",
    "    \n",
    "    n, _ = X.shape\n",
    "    W = initweights([2, 3, 1]) # generate random weights\n",
    "    A, Z = forward_pass(W, X)\n",
    "    \n",
    "    gradient = backprop(W, A, Z, y) # backprop to calculate the gradient\n",
    "    \n",
    "    # gradient[i] should match the shape of W[i]\n",
    "    return np.all([gradient[i].shape == W[i].shape for i in range(len(W))])\n",
    "\n",
    "def backprop_test3():\n",
    "    X, y = generate_data() # generate data\n",
    "    \n",
    "    n, _ = X.shape\n",
    "    \n",
    "    # Use a one layer network\n",
    "    # This is essentially the least squares\n",
    "    W = initweights([2, 1]) \n",
    "    \n",
    "    A, Z = forward_pass(W, X)\n",
    "    \n",
    "    # backprop to calculate the gradient\n",
    "    gradient = backprop(W, A, Z, y) \n",
    "    \n",
    "    # calculate the least square gradient\n",
    "    least_square_gradient = 2 *((X.T @ X) @ W[0] - X.T @ y.reshape(-1, 1)) / n\n",
    "    \n",
    "    # gradient[0] should be the least square gradient\n",
    "    return np.linalg.norm(gradient[0] - least_square_gradient) < 1e-7\n",
    "\n",
    "def backprop_test4():\n",
    "    X, y = generate_data() # generate data\n",
    "    \n",
    "    n, _ = X.shape\n",
    "    \n",
    "    # Use a one layer network\n",
    "    # This is essentially the least squares\n",
    "    W = initweights([2, 5, 5, 1]) \n",
    "    \n",
    "    A, Z = forward_pass(W, X)\n",
    "    \n",
    "    # backprop to calculate the gradient\n",
    "    gradient = backprop(W, A, Z, y) \n",
    "    \n",
    "    # calculate the backprop gradient\n",
    "    gradient_grader = backprop_grader(W, A, Z, y)\n",
    "    \n",
    "    # Check whether your gradient matches ours\n",
    "    OK=[len(gradient_grader)==len(gradient)] # check if length matches\n",
    "    for (g,gg) in zip(gradient_grader,gradient): # check if each component matches in shape and values\n",
    "        OK.append(gg.shape==g.shape and (np.linalg.norm(g - gg) < 1e-7))\n",
    "    return(all(OK))\n",
    "\n",
    "def backprop_test5():\n",
    "    # Here we reverse your gradient output and check that reverse with ours. It shouldn't match. \n",
    "    # If your reverse gradient matches our gradient, this means you outputted the gradient in reverse order.\n",
    "    # This is a common mistake, as the loop is backwards. \n",
    "    X, y = generate_data() # generate data\n",
    "    \n",
    "    n, _ = X.shape\n",
    "    \n",
    "    # Use a one layer network\n",
    "    # This is essentially the least squares\n",
    "    W = initweights([2, 5, 5, 1]) \n",
    "    \n",
    "    A, Z = forward_pass(W, X)\n",
    "    \n",
    "    # backprop to calculate the gradient\n",
    "    gradient = backprop(W, A, Z, y) \n",
    "    \n",
    "    # calculate the backprop gradient\n",
    "    gradient_grader = backprop_grader(W, A, Z, y)\n",
    "\n",
    "    gradient.reverse() # reverse the gradient. From now on it should NOT match\n",
    "    # Check whether your gradient matches ours\n",
    "    OK=[] # check if length matches\n",
    "    for (g,gg) in zip(gradient_grader,gradient): # check if each component matches\n",
    "        OK.append(gg.shape==g.shape and (np.linalg.norm(g - gg) < 1e-7))\n",
    "    return(not all(OK)) \n",
    "\n",
    "\n",
    "\n",
    "runtest(backprop_test1, 'backprop_test1')\n",
    "runtest(backprop_test2, 'backprop_test2')\n",
    "runtest(backprop_test3, 'backprop_test3')\n",
    "runtest(backprop_test4, 'backprop_test4')\n",
    "runtest(backprop_test5, 'backprop_test5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-backprop_test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# backprop_test1\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert backprop_test1()\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-backprop_test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# backprop_test2\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert backprop_test2()\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-backprop_test3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# backprop_test3\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert backprop_test3()\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-backprop_test4",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# backprop_test4\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert backprop_test4()\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-backprop_test5",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# backprop_test5\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert backprop_test5()\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5bea8cd4b42b1693",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3> Part Four: Training with Gradient Descent</h3>\n",
    "\n",
    "Run the cell below to train a ReLU Network for the task with Gradient Descent. Feel free to play around with the hyperparameters such learning rate, number of epochs, number of hidden layers, size of each hidden layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate data\n",
    "X, y = generate_data() # generate data\n",
    "\n",
    "\n",
    "# learning rate for Gradient Descent\n",
    "lr = 0.001\n",
    "\n",
    "# Number of epochs\n",
    "# one epoch - one full pass through the dataset\n",
    "M = 10000\n",
    "\n",
    "# keep track of the losses\n",
    "losses = np.zeros(M)\n",
    "t0 = time.time()\n",
    "\n",
    "# initialize a neural network with one hidden layer \n",
    "# Try varying the depth and width of the neural networks to see the effect\n",
    "\n",
    "W = initweights([2, 200, 1])\n",
    "\n",
    "# Start training\n",
    "for i in range(M):\n",
    "    \n",
    "    # Do a forward pass\n",
    "    A, Z = forward_pass(W, X)\n",
    "    \n",
    "    # Calculate the loss\n",
    "    losses[i] = MSE(Z[-1].flatten(), y)\n",
    "    \n",
    "    # Calculate the loss using backprop\n",
    "    gradients = backprop(W, A, Z, y)\n",
    "    \n",
    "    # Update he parameters\n",
    "    for j in range(len(W)):\n",
    "        W[j] -= lr * gradients[j]\n",
    "t1 = time.time()\n",
    "print('Elapsed time: %.2fs' % (t1-t0))\n",
    "plot_results(X[:, 0], y, Z, losses)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
