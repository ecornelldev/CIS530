{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7ca0859bbc65faff",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2>Multilayer Perceptron</h2>\n",
    "\n",
    "\n",
    "<h3>Introduction</h3>\n",
    "In this project, you will implement a simple multilayer perceptron for a regression problem.\n",
    "We broke it apart into several functions. Before we start, let's import some packages:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3>Evaluation</h3>\n",
    "\n",
    "<p><strong>This project must be successfully completed and submitted in order to receive credit for this course. Your score on this project will be included in your final grade calculation.</strong><p>\n",
    "    \n",
    "<p>You are expected to write code where you see <em># YOUR CODE HERE</em> within the cells of this notebook. Not all cells will be graded; code input cells followed by cells marked with <em>#Autograder test cell</em> will be graded. Upon submitting your work, the code you write at these designated positions will be assessed using an \"autograder\" that will run all test cells to assess your code. You will receive feedback from the autograder that will identify any errors in your code. Use this feedback to improve your code if you need to resubmit. Be sure not to change the names of any provided functions, classes, or variables within the existing code cells, as this will interfere with the autograder. Also, remember to execute all code cells sequentially, not just those you’ve edited, to ensure your code runs properly.</p>\n",
    "    \n",
    "<p>You can resubmit your work as many times as necessary before the submission deadline. If you experience difficulty or have questions about this exercise, use the Q&A discussion board to engage with your peers or seek assistance from the instructor.<p>\n",
    "\n",
    "<p>Before starting your work, please review <a href=\"https://s3.amazonaws.com/ecornell/global/eCornellPlagiarismPolicy.pdf\">eCornell's policy regarding plagiarism</a> (the presentation of someone else's work as your own without source credit).</p>\n",
    "\n",
    "<h3>Submit Code for Autograder Feedback</h3>\n",
    "\n",
    "<p>Once you have completed your work on this notebook, you will submit your code for autograder review. Follow these steps:</p>\n",
    "\n",
    "<ol>\n",
    "  <li><strong>Save your notebook.</strong></li>\n",
    "  <li><strong>Mark as Completed —</strong> In the blue menu bar along the top of this code exercise window, you’ll see a menu item called <strong>Education</strong>. In the <strong>Education</strong> menu, click <strong>Mark as Completed</strong> to submit your code for autograder/instructor review. This process will take a moment and a progress bar will show you the status of your submission.</li>\n",
    "\t<li><strong>Review your results —</strong> Once your work is marked as complete, the results of the autograder will automatically be presented in a new tab within the code exercise window. You can click on the assessment name in this feedback window to see more details regarding specific feedback/errors in your code submission.</li>\n",
    "  <li><strong>Repeat, if necessary —</strong> The Jupyter notebook will always remain accessible in the first tabbed window of the exercise. To reattempt the work, you will first need to click <strong>Mark as Uncompleted</strong> in the <strong>Education</strong> menu and then proceed to make edits to the notebook. Once you are ready to resubmit, follow steps one through three. You can repeat this procedure as many times as necessary.</li>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3fe0f85f2526afff",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-05ec67dd9c38e8fd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "import numpy as np\n",
    "from numpy.matlib import repmat\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "import time\n",
    "\n",
    "import helper as h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d5f8224f284306d3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Visualizing Data</h3>\n",
    "We will generate a simple 1-dimensional toy dataset by calling <code>h.generate_data</code>. This function returns the data $\\mathbf{X}$ and label $\\mathbf{y}$. Note that $X$ is of shape (N, 2). We append 1 to each example to introduce bias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-07e24f138d476ac4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "X, y = h.generate_data()\n",
    "\n",
    "print(f'The shape of X is {X.shape}. This is because we append 1 to each feature vector to introduce bias!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-22f2e1e5eea4b07b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(X[:, 0], y, '*')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6aae1a46a88cfb3d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Part Zero: Transition Function</h3>\n",
    "Transition function is the key component of a neural network that contributes to its nonlinearity. For our neural network, we are going to use the ReLU transition function. Recall that the ReLU transition is as follows:\n",
    "$$\\sigma(z) = \\max(z, 0)$$\n",
    "\n",
    "We have implemented the <code>ReLU</code> and <code>ReLU_grad</code> functions as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e29d6d493186387e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def ReLU(z):\n",
    "    return np.maximum(z, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0e334778f4924bd7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def ReLU_grad(z):\n",
    "    return (z > 0).astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d719110877776b79",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We visualize the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ea052350528f5135",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(-4, 4, 100), ReLU(np.linspace(-4, 4, 100)))\n",
    "plt.xlabel('z')\n",
    "plt.ylabel(r'$\\max$ (z, 0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a07168b188ef1922",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Part One: Forward Pass [Graded]</h3>\n",
    "\n",
    "In this section, you will implement the forward pass function <code>forward_pass(W, xTr)</code>. Note that $\\mathbf{W}$ specifies the weights of the network at each layer. More specifically, $W[0]$ stores the weights for the first layer of the network, $W[i]$ stores the weights of the (i + 1)-th layer and $W[l-1]$ stores the weights of the last layer.\n",
    "\n",
    "Each layer of the network produces two outputs, $A[i + 1]$ and \n",
    "$Z[i + 1]$, where \n",
    "\n",
    "$$A[i + 1]=Z[i] * W[i]$$ \n",
    "for $i = 0, 1, 2, ..., l-1$ and \n",
    "$$Z[i+1]=\\sigma(A[i+1])$$ for $i = 0, 1, 2, ..., l-2$ and \n",
    "$$Z[l-1]=A[l-1]$$\n",
    "\n",
    "\n",
    "Here, $*$ stands for matrix multiplication and $Z[0], A[0]$ are both initialized to be the the training set.\n",
    "\n",
    "For simplicity, we did not include bias when calculating $A[i + 1]$. For the purpose of this assignment, this is fine since we have appended one in all the raw features and  our dataset is rather simple. In general, bias should always be included when calculating $A[i + 1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the variables `A`, `Z`, and `W` a bit better, consider the following hypothetical neural net layer:\n",
    "![nnlayer.png](nnlayer.png)\n",
    "\n",
    "It is important to note that `W[i]` is an *array of matrices*, as specified in the docstring for the provided method `initweights`. Since layer `i` has 2 nodes and layer `i+1` has 3, `W[i]` is a 2 by 3 matrix.\n",
    "\n",
    "Additionally, $\\sigma$ is applied elementwise to the values in `A[i+1]`, and is `ReLU` for this assignment. The final output at the end of the neural network should be `Z[l]`.\n",
    "\n",
    "Finally, the following pseudocode from the READ module may be helpful:\n",
    "\n",
    "![fwdpass.png](fwdpass.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b652c9590b2cfe79",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The following function will randomly generate initial weights for the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ce4c1f90a8b0779c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def initweights(specs):\n",
    "    \"\"\"\n",
    "    Given a specification of the neural network, output a random weight array\n",
    "    INPUT:\n",
    "        specs - array of length m+1. specs[0] should be the dimension of the feature and spec[-1] \n",
    "                should be the dimension of output\n",
    "    \n",
    "    OUTPUT:\n",
    "        W - array of length m, each element is a matrix\n",
    "            where size(weights[i]) = (specs[i], specs[i+1])\n",
    "    \"\"\"\n",
    "    W = []\n",
    "    for i in range(len(specs) - 1):\n",
    "        W.append(np.random.randn(specs[i], specs[i+1]))\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-679cd28956259b78",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# If we want to create a network that \n",
    "#   i) takes in feature of dimension 2\n",
    "#   ii) has 1 hidden layer with 3 hidden units\n",
    "#   iii) output a scalar\n",
    "# then we initialize the the weights the following way:\n",
    "\n",
    "W = initweights([2, 3, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0ee91db35bb678d4",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def forward_pass(W, xTr):\n",
    "    \"\"\"\n",
    "    function forward_pass(weights,xTr)\n",
    "    \n",
    "    INPUT:\n",
    "    W - an array of l weight matrices\n",
    "    xTr - nxd matrix. Each row is an input vector\n",
    "    \n",
    "    OUTPUTS:\n",
    "    A - an array (of length l) that stores result of matrix multiplication at each layer \n",
    "    Z - an array (of length l) that stores result of transition function at each layer \n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize A and Z\n",
    "    A = [xTr]\n",
    "    Z = [xTr]\n",
    "    ### BEGIN SOLUTION\n",
    "    for i in range(len(W)):\n",
    "        a = Z[i] @ W[i]\n",
    "        A.append(a)\n",
    "        \n",
    "        if i < len(W) - 1:\n",
    "            z = ReLU(a)\n",
    "        else:\n",
    "            z = a\n",
    "\n",
    "        Z.append(z)\n",
    "    ### END SOLUTION\n",
    "    return A, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-291fd4186629ceb1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def forward_test1():\n",
    "    X, _ = h.generate_data() # generate data\n",
    "    W = initweights([2, 3, 1]) # generate random weights\n",
    "    out = forward_pass(W, X) # run forward pass\n",
    "    return len(out) == 2 # make sure that your function return a tuple\n",
    "\n",
    "def forward_test2():\n",
    "    X, _ = h.generate_data() # generate data\n",
    "    W = initweights([2, 3, 1]) # generate random weights\n",
    "    A, Z = forward_pass(W, X) # run forward pass\n",
    "    return len(A) == 3 and len(Z) == 3 # Make sure that output produced match the length of the weight\n",
    "\n",
    "def forward_test3():\n",
    "    X, _ = h.generate_data() # generate data\n",
    "    n, _ = X.shape\n",
    "    W = initweights([2, 3, 1]) # generate random weights\n",
    "    A, Z = forward_pass(W, X) # run forward pass\n",
    "    return (A[1].shape == (n, 3) and \n",
    "            Z[1].shape == (n, 3)  and\n",
    "            A[2].shape == (n, 1) and\n",
    "            A[2].shape == (n, 1) ) # Make sure the layer produce the right shape output\n",
    "\n",
    "def forward_test4():\n",
    "    X = -1*np.ones((1, 2)) # generate a feature matrix of all negative ones\n",
    "    W = [np.ones((2, 1))] # a single layer network with weights one\n",
    "    A, Z = forward_pass(W, X) # run forward pass\n",
    "    \n",
    "    # check whether you do not apply the transition function to A[-1] \n",
    "    return np.linalg.norm(Z[-1] - X@W[0]) < 1e-7\n",
    "\n",
    "def forward_test5():\n",
    "    X, _ = h.generate_data() # generate data\n",
    "    n, _ = X.shape\n",
    "    W = initweights([2, 3, 1]) # generate random weights\n",
    "    A, Z = forward_pass(W, X) # run your forward pass\n",
    "    A_grader, Z_grader = h.forward_pass_grader(W, X) # run our forward pass\n",
    "    \n",
    "    Adiff = 0\n",
    "    Zdiff = 0\n",
    "    \n",
    "    # compute the difference between your solution and ours\n",
    "    for i in range(1, 3):\n",
    "        Adiff += np.linalg.norm(A[i] - A_grader[i])\n",
    "        Zdiff += np.linalg.norm(Z[i] - Z_grader[i])\n",
    "        \n",
    "    return Adiff < 1e-7 and Zdiff < 1e-7\n",
    "\n",
    "h.runtest(forward_test1, \"forward_test1\")\n",
    "h.runtest(forward_test2, \"forward_test2\")\n",
    "h.runtest(forward_test3, \"forward_test3\")\n",
    "h.runtest(forward_test4, \"forward_test4\")\n",
    "h.runtest(forward_test5, \"forward_test5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-131410510db941fe",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# Forward_test1\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert forward_test1()\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-49d3a699f1e73506",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# Forward_test2\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert forward_test2()\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9a6f3b89276ba82f",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# Forward_test3\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert forward_test3()\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-914018b83724610e",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# Forward_test4\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert forward_test4()\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-df0ca708e878a285",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# Forward_test5\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert forward_test5()\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d74c4dde8e4ec273",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3> Part Two: Loss Function [Graded]</h3>\n",
    "\n",
    "In this section, you are going to implement the Mean Squared Error (MSE) loss function for regression. Recall that for a set of training example $\\{(\\mathbf{x}_1, y_1), ..., (\\mathbf{x}_n, y_n)\\}$, the MSE of the network $h$ is \n",
    "\n",
    "$$L = \\frac{1}{n} \\sum_{i = 1} ^n(h(\\mathbf{x}_i) - y_i)^2$$\n",
    "\n",
    "The <code>MSE</code> function you are going to implement takes the output of the network (<code>out</code>) and the training labels (<code>y</code>) and computes the MSE loss. You will also need to implement the <code>MSE_grad</code> function that will calculate the gradient of the MSE loss with respect to each entry of `out`. This function will be useful when you implement backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6dd92375a32f23f4",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def MSE(out, y):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    out: output of network (n vector)\n",
    "    y: training labels (n vector)\n",
    "    \n",
    "    OUTPUTS:\n",
    "    \n",
    "    loss: the mse loss (a scalar)\n",
    "    \"\"\"\n",
    "    \n",
    "    n = len(y)\n",
    "    loss = 0\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    loss = np.mean((out - y) ** 2)\n",
    "    ### END SOLUTION\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c9a62e7d5e1a53b7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def MSE_test1():\n",
    "    X, y = h.generate_data() # generate data\n",
    "    W = initweights([2, 3, 1]) # generate random weights\n",
    "    A, Z = forward_pass(W, X)\n",
    "    loss = MSE(Z[-1].flatten(), y) # calculate loss\n",
    "    \n",
    "    return np.isscalar(loss) # your loss should be a scalar\n",
    "\n",
    "def MSE_test2():\n",
    "    X, y = h.generate_data() # generate data\n",
    "    W = initweights([2, 3, 1]) # generate random weights\n",
    "    A, Z = forward_pass(W, X)\n",
    "    loss = MSE(Z[-1].flatten(), y) # calculate loss\n",
    "    \n",
    "    return loss >= 0 # your loss should be nonnegative\n",
    "\n",
    "def MSE_test3():\n",
    "    X, y = h.generate_data() # generate data\n",
    "    W = initweights([2, 3, 1]) # generate random weights\n",
    "    A, Z = forward_pass(W, X)\n",
    "    loss = MSE(Z[-1].flatten(), y) # calculate loss\n",
    "    loss_grader = h.MSE_grader(Z[-1].flatten(), y)\n",
    "    \n",
    "    # your loss should not deviate too much from ours\n",
    "    # If you fail this test case, check whether you divide your loss by 1/n\n",
    "    return np.absolute(loss - loss_grader) < 1e-7 \n",
    "\n",
    "h.runtest(MSE_test1, \"MSE_test1\")\n",
    "h.runtest(MSE_test2, \"MSE_test2\")\n",
    "h.runtest(MSE_test3, \"MSE_test3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-6bd2bed88d4a1da2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# MSE_test1\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert MSE_test1()\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b58fe55cb556a158",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# MSE_test2\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert MSE_test2()\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-63cef93910e2ac1e",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# MSE_test3\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert MSE_test3()\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now need to implement the gradient of the MSE loss calculated above. When you take the derivative of the above with respect to the entries of out, you get the following:\n",
    "$$\\nabla L = \\frac{2}{n} * (h(\\mathbf{x}_i) - y_i)$$\n",
    "\n",
    "Implement the above in the function `MSE_grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5dd2453b4b9f889c",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def MSE_grad(out, y):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    out: output of network (n vector)\n",
    "    y: training labels (n vector)\n",
    "    \n",
    "    OUTPUTS:\n",
    "    \n",
    "    grad: the gradient of the MSE loss with respect to out (nx1 vector)\n",
    "    \"\"\"\n",
    "    \n",
    "    n = len(y)\n",
    "    grad = np.zeros(n)\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    grad = 2 * (out - y) / n\n",
    "    ### END SOLUTION\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-649b459c6f2dd39f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def MSE_grad_test1():\n",
    "    X, y = h.generate_data() # generate data\n",
    "    \n",
    "    n, _ = X.shape\n",
    "    W = initweights([2, 3, 1]) # generate random weights\n",
    "    A, Z = forward_pass(W, X)\n",
    "    \n",
    "    grad = MSE_grad(Z[-1].flatten(), y)\n",
    "    return grad.shape == (n, )\n",
    "\n",
    "def MSE_grad_test2():\n",
    "    out = np.array([1])\n",
    "    y = np.array([1.2])\n",
    "    \n",
    "    # calculate numerical gradient using finite difference\n",
    "    numerical_grad = (MSE(out + 1e-7, y) - MSE(out - 1e-7, y)) / 2e-7\n",
    "    grad = MSE_grad(out, y)\n",
    "    \n",
    "    # check your gradient is close to the numerical gradient\n",
    "    return np.linalg.norm(numerical_grad - grad) < 1e-7\n",
    "\n",
    "def MSE_grad_test3():\n",
    "    X, y = h.generate_data() # generate data\n",
    "    \n",
    "    n, _ = X.shape\n",
    "    W = initweights([2, 3, 1]) # generate random weights\n",
    "    A, Z = forward_pass(W, X)\n",
    "    \n",
    "    grad = MSE_grad(Z[-1].flatten(), y)\n",
    "    grad_grader = h.MSE_grad_grader(Z[-1].flatten(), y) # compute gradient using our solution\n",
    "    \n",
    "    # your gradient should not deviate too much from ours\n",
    "    return np.linalg.norm(grad_grader - grad) < 1e-7\n",
    "\n",
    "h.runtest(MSE_grad_test1, 'MSE_grad_test1')\n",
    "h.runtest(MSE_grad_test2, 'MSE_grad_test2')\n",
    "h.runtest(MSE_grad_test3, 'MSE_grad_test3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-2d4b2c6f1b01e73e",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# MSE_grad_test1\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert MSE_grad_test1()\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-e7874ea815832006",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# MSE_grad_test2\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert MSE_grad_test2()\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1f54c87a5dda35d9",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# MSE_grad_test3\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert MSE_grad_test3()\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Part Three: Backpropagation [Graded]</h3>\n",
    "\n",
    "In this section, you are going to implement the <code>backprop</code> for a ReLU network specified by the weight <code>W</code>. To recap, this is the stage that updates all the weights in the network starting from the last layer. Refer to the following pseudocode from the READ module on back propagation:\n",
    "\n",
    "![backpass.png](backpass.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e30f891ec0f25958",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def backprop(W, A, Z, y):\n",
    "    \"\"\"\n",
    "    \n",
    "    INPUT:\n",
    "    W weights (cell array)\n",
    "    A output of forward pass (cell array)\n",
    "    Z output of forward pass (cell array)\n",
    "    y vector of size n (each entry is a label)\n",
    "    \n",
    "    OUTPUTS:\n",
    "    \n",
    "    gradient = the gradient with respect to W as a cell array of matrices\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert delta to a row vector to make things easier\n",
    "    delta = (MSE_grad(Z[-1].flatten(), y) * 1).reshape(-1, 1)\n",
    "\n",
    "    # compute gradient with backprop\n",
    "    gradients = []\n",
    "    \n",
    "    # BEGIN SOLUTION\n",
    "    for i in range(len(W)-1, -1, -1):\n",
    "        gradients.append((Z[i].T) @ (delta))\n",
    "        delta = ReLU_grad(A[i]) * (delta@(W[i].T))\n",
    "        \n",
    "    gradients = gradients[::-1]\n",
    "    # END SOLUTION\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def backprop_test1():\n",
    "    X, y = h.generate_data() # generate data\n",
    "    \n",
    "    n, _ = X.shape\n",
    "    W = initweights([2, 3, 1]) # generate random weights\n",
    "    A, Z = forward_pass(W, X)\n",
    "    \n",
    "    gradient = backprop(W, A, Z, y) # backprop to calculate the gradient\n",
    "    \n",
    "    # You should return a list with the same len as W\n",
    "    return len(gradient) == len(W)\n",
    "\n",
    "def backprop_test2():\n",
    "    X, y = h.generate_data() # generate data\n",
    "    \n",
    "    n, _ = X.shape\n",
    "    W = initweights([2, 3, 1]) # generate random weights\n",
    "    A, Z = forward_pass(W, X)\n",
    "    \n",
    "    gradient = backprop(W, A, Z, y) # backprop to calculate the gradient\n",
    "    \n",
    "    # gradient[i] should match the shape of W[i]\n",
    "    return np.all([gradient[i].shape == W[i].shape for i in range(len(W))])\n",
    "\n",
    "def backprop_test3():\n",
    "    X, y = h.generate_data() # generate data\n",
    "    \n",
    "    n, _ = X.shape\n",
    "    \n",
    "    # Use a one layer network\n",
    "    # This is essentially the least squares\n",
    "    W = initweights([2, 1]) \n",
    "    \n",
    "    A, Z = forward_pass(W, X)\n",
    "    \n",
    "    # backprop to calculate the gradient\n",
    "    gradient = backprop(W, A, Z, y) \n",
    "    \n",
    "    # calculate the least square gradient\n",
    "    least_square_gradient = 2 *((X.T @ X) @ W[0] - X.T @ y.reshape(-1, 1)) / n\n",
    "    \n",
    "    # gradient[0] should be the least square gradient\n",
    "    return np.linalg.norm(gradient[0] - least_square_gradient) < 1e-7\n",
    "\n",
    "def backprop_test4():\n",
    "    X, y = h.generate_data() # generate data\n",
    "    \n",
    "    n, _ = X.shape\n",
    "    \n",
    "    # Use a one layer network\n",
    "    # This is essentially the least squares\n",
    "    W = initweights([2, 1]) \n",
    "    \n",
    "    A, Z = forward_pass(W, X)\n",
    "    \n",
    "    # backprop to calculate the gradient\n",
    "    gradient = backprop(W, A, Z, y) \n",
    "    \n",
    "    # calculate the least square gradient\n",
    "    gradient_grader = h.backprop_grader(W, A, Z, y)\n",
    "    \n",
    "    # Check whether your gradient matches ours\n",
    "    return np.linalg.norm(gradient[0] - gradient_grader[0]) < 1e-7\n",
    "\n",
    "h.runtest(backprop_test1, 'backprop_test1')\n",
    "h.runtest(backprop_test2, 'backprop_test2')\n",
    "h.runtest(backprop_test3, 'backprop_test3')\n",
    "h.runtest(backprop_test4, 'backprop_test4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-fe238e88cab5594f",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# backprop_test1\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert backprop_test1()\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-adbed70991c98f60",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# backprop_test2\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert backprop_test2()\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ba302c9defbf583b",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# backprop_test3\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert backprop_test3()\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d6049947c0cde001",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# backprop_test4\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert backprop_test4()\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5bea8cd4b42b1693",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3> Part Four: Training with Gradient Descent</h3>\n",
    "\n",
    "Run the cell below to train a ReLU Network for the task with Gradient Descent. Feel free to play around with the hyperparameters such learning rate, number of epochs, number of hidden layers, size of each hidden layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate for Gradient Descent\n",
    "lr = 0.001\n",
    "\n",
    "# Number of epochs\n",
    "# one epoch - one full pass through the dataset\n",
    "M = 10000\n",
    "\n",
    "# keep track of the losses\n",
    "losses = np.zeros(M)\n",
    "t0 = time.time()\n",
    "\n",
    "# initialize a neural network with one hidden layer \n",
    "# Try varying the depth and width of the neural networks to see the effect\n",
    "\n",
    "W = initweights([2, 200, 1])\n",
    "\n",
    "# Start training\n",
    "for i in range(M):\n",
    "    \n",
    "    # Do a forward pass\n",
    "    A, Z = forward_pass(W, X)\n",
    "    \n",
    "    # Calculate the loss\n",
    "    losses[i] = MSE(Z[-1].flatten(), y)\n",
    "    \n",
    "    # Calculate the loss using backprop\n",
    "    gradients = backprop(W, A, Z, y)\n",
    "    \n",
    "    # Update he parameters\n",
    "    for j in range(len(W)):\n",
    "        W[j] -= lr * gradients[j]\n",
    "t1 = time.time()\n",
    "print('Elapsed time: %.2fs' % (t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-db58a28187a02b84",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "h.plot_results(X[:, 0], y, Z, losses)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
