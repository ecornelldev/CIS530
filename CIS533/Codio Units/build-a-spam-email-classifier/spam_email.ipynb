{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bb88f0a35b56d0ae",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2>About this Project</h2>\n",
    "<p>In this Project, you will find the gradient of the logistic loss function and implement gradient descent in order to create a linear classifier that can \"filter\" spam email messages.</p>\n",
    "\n",
    "<h3>Evaluation</h3>\n",
    "\n",
    "<p><strong>This project must be successfully completed and submitted in order to receive credit for this course. Your score on this project will be included in your final grade calculation.</strong><p>\n",
    "    \n",
    "<p>You are expected to write code where you see <em># YOUR CODE HERE</em> within the cells of this notebook. Not all cells will be graded; code input cells followed by cells marked with <em>#Autograder test cell</em> will be graded. Upon submitting your work, the code you write at these designated positions will be assessed using an \"autograder\" that will run all test cells to assess your code. You will receive feedback from the autograder that will identify any errors in your code. Use this feedback to improve your code if you need to resubmit. Be sure not to change the names of any provided functions, classes, or variables within the existing code cells, as this will interfere with the autograder. Also, remember to execute all code cells sequentially, not just those you’ve edited, to ensure your code runs properly.</p>\n",
    "    \n",
    "<p>You can resubmit your work as many times as necessary before the submission deadline. If you experience difficulty or have questions about this exercise, use the Q&A discussion board (found in the Live Labs section of this course) to engage with your peers or seek assistance from the instructor.<p>\n",
    "\n",
    "<p>Before starting your work, please review <a href=\"https://s3.amazonaws.com/ecornell/global/eCornellPlagiarismPolicy.pdf\">eCornell's policy regarding plagiarism</a> (the presentation of someone else's work as your own without source credit).</p>\n",
    "\n",
    "<h3>Submit Code for Autograder Feedback</h3>\n",
    "\n",
    "<p>Once you have completed your work on this notebook, you will submit your code for autograder review. Follow these steps:</p>\n",
    "\n",
    "<ol>\n",
    "    <li><strong>Save your notebook —</strong> Click <strong>Save and Checkpoint</strong> in the \"File\" menu.</li>\n",
    "  <li><strong>Mark as Completed —</strong> In the blue menu bar along the top of this code exercise window, you’ll see a menu item called <strong>Education</strong>. In the <strong>Education</strong> menu, click <strong>Mark as Completed</strong> to submit your code for autograder/instructor review. This process will take a moment and a progress bar will show you the status of your submission.</li>\n",
    "\t<li><strong>Review your results —</strong> Once your work is marked as complete, the results of the autograder will automatically be presented in a new tab within the code exercise window. You can click on the assessment name in this feedback window to see more details regarding specific feedback/errors in your code submission.</li>\n",
    "  <li><strong>Repeat, if necessary —</strong> The Jupyter notebook will always remain accessible in the first tabbed window of the exercise. To reattempt the work, you will first need to click <strong>Mark as Uncompleted</strong> in the <strong>Education</strong> menu and then proceed to make edits to the notebook. Once you are ready to resubmit, follow steps one through three. You can repeat this procedure as many times as necessary.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ae461e465b7f800e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2>Getting Started</h2>\n",
    "<h3>Python Initialization</h3> \n",
    "\n",
    "Please run the following code to initialize your Python kernel. You should be running a version of Python 3.x. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a801a09f2b1fc82b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're running python 3.7.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from pylab import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "from helper import *\n",
    "\n",
    "print('You\\'re running python %s' % sys.version.split(' ')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5e40cd81b8f0dd4a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Next, let's simulate some 2D data with binary (0/1) labels.  You'll be generating this data from non-overlapping multivariate normal distributions that should be very easily separable for logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-48afa45c97dc36fb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "n_samples = 500\n",
    "\n",
    "class_one = np.random.multivariate_normal([5, 10], [[1, .25],[.25, 1]], n_samples)\n",
    "class_one_labels = -np.ones(n_samples)\n",
    "\n",
    "class_two = np.random.multivariate_normal([0, 5], [[1, .25],[.25, 1]], n_samples)\n",
    "class_two_labels = np.ones(n_samples)\n",
    "\n",
    "features = np.vstack((class_one, class_two))\n",
    "labels = np.hstack((class_one_labels, class_two_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a262f12a83fdbc86",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's see what what our feature arrays look like. \n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ff1f0296d7835ca5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# We can visualize these data distributions\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.scatter(features[:, 0], features[:, 1],\n",
    "            c = labels, alpha = .6);\n",
    "\n",
    "plt.title(\"Binary labeled data in 2D\", size=15);\n",
    "plt.xlabel(\"Feature 1\", size=13);\n",
    "plt.ylabel(\"Feature 2\", size=13);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ff5ae8402f789a19",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In logistic regression, we use gradient ascent to solve for the weight vector that maximizes the (log) likelihood of observing the data.  (Equivalently, we can use gradient _descent_ to solve for the weight vector that _minimizes_ the _negative_ log likelihood - refer to Module 3 if you need a review of this derivation!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Gradient Descent and Email Spam Classification</h2>\n",
    "\n",
    "<h3>Part One: Sigmoid [Graded]</h3>\n",
    "\n",
    "<p>To begin, you must first implement the sigmoid function:    $\\sigma(z)=\\frac{1}{1+e^{-z}}$ </p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-sigmoid",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    # Input: \n",
    "    # z : scalar or array of dimension n \n",
    "    # Output:\n",
    "    # sgmd: scalar or array of dimension n\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    sgmd = 1 / (1 + np.exp(-z))\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    return sgmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-sigmoid_selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def test_sigmoid1():\n",
    "    h = np.random.rand(10) # input is an 10-dimensional array\n",
    "    sgmd1 = sigmoid(h)\n",
    "    return sgmd1.shape == h.shape # output should be a 10-dimensional array\n",
    "\n",
    "def test_sigmoid2():\n",
    "    h = np.random.rand(10) # input is an 10-dimensional array\n",
    "    sgmd1 = sigmoid(h) # compute the sigmoids with your function\n",
    "    sgmd2 = sigmoid_grader(h) # compute the sigmoids with ground truth funcdtion\n",
    "    return (np.linalg.norm(sgmd1 - sgmd2) < 1e-5) # check if they agree\n",
    "\n",
    "def test_sigmoid3():\n",
    "    x = np.random.rand(1) # input is a scalar\n",
    "    sgmd1 = sigmoid(x) # compute the sigmoids with your function\n",
    "    sgmd2 = sigmoid_grader(x) # compute the sigmoids with ground truth function\n",
    "    return (np.linalg.norm(sgmd1 - sgmd2) < 1e-5) # check if they agree\n",
    "\n",
    "def test_sigmoid4():\n",
    "    x = np.array([-1e10,1e10,0]) # three input points: very negative, very positive, and 0\n",
    "    sgmds = sigmoid(x) # compute the sigmoids with your function\n",
    "    truth = np.array([0,1,0.5]) # the truth should be 0, 1, 0.5 exactly\n",
    "    return (np.linalg.norm(sgmds - truth) < 1e-8) # test if this is true\n",
    "\n",
    "\n",
    "runtest(test_sigmoid1, 'test_sigmoid1')\n",
    "runtest(test_sigmoid2, 'test_sigmoid2')\n",
    "runtest(test_sigmoid3, 'test_sigmoid3')\n",
    "runtest(test_sigmoid4, 'test_sigmoid4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-test_sigmoid1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_sigmoid1\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "h = np.random.rand(10) # input is an 10-dimensional array\n",
    "sgmd1 = sigmoid(h)\n",
    "assert sgmd1.shape == h.shape\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-test_sigmoid2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_sigmoid2\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "n = 10\n",
    "h = np.random.rand(n)\n",
    "sgmd1 = sigmoid(h)\n",
    "sgmd2 = sigmoid_grader(h)\n",
    "assert (np.linalg.norm(sgmd1 - sgmd2) < 1e-5)\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-test_sigmoid3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_sigmoid3\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "x = np.random.rand(1)\n",
    "sgmd1 = sigmoid(x)\n",
    "sgmd2 = sigmoid_grader(x)\n",
    "assert (np.linalg.norm(sgmd1 - sgmd2) < 1e-5)\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-cell-test_sigmoid4",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_sigmoid4\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "x = np.array([-1e10,1e10,0]) # three input points: very negative, very positive, and 0\n",
    "sgmds = sigmoid(x) # compute the sigmoids with your function\n",
    "truth = np.array([0,1,0.5]) # the truth should be 0,1,0.5 exactly\n",
    "assert (np.linalg.norm(sgmds - truth) < 1e-8) # test if this is true\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a2e7bdf679a2c694",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Part Two: Implement <code>y_pred</code> [Graded]</h3>\n",
    "\n",
    "<p>We consider binary logistic regression with class labels $y\\in\\{+1,-1\\}$. Implement a function <code>y_pred(X,w)</code> that computes $P(y=1\\;|\\;\\mathbf{x};\\mathbf{w}, b)$ for each row-vector $\\mathbf{x}$ in the matrix <code>X</code>.</p>\n",
    "<p>Recall that:\n",
    "$$P(y\\;|\\;\\mathbf{x};\\mathbf{w})=\\sigma(y (\\mathbf{w}^\\top \\mathbf{x} + b))$$\n",
    "</p>\n",
    "<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ypred",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def y_pred(X, w, b=0):\n",
    "    # Input:\n",
    "    # X: nxd matrix\n",
    "    # w: d-dimensional vector\n",
    "    # b: scalar (optional, if not passed on is treated as 0)\n",
    "    # Output:\n",
    "    # prob: n-dimensional vector\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    prob=sigmoid(X@w + b)\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ypred-selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def test_ypred1():\n",
    "    n = 20\n",
    "    d = 5\n",
    "    X = np.random.rand(n,d) # generate n random vectors with d dimensions\n",
    "    w = np.random.rand(5) # define a random weight vector\n",
    "    probs=y_pred(X,w,0) # compute the probabilities of P(y=1|x;w) using your y_pred function\n",
    "    return probs.shape == (n, ) # check if all outputs are >=0 and <=1\n",
    "\n",
    "\n",
    "def test_ypred2():\n",
    "    n = 20\n",
    "    d = 5\n",
    "    X = np.random.rand(n,d) # generate n random vectors with d dimensions\n",
    "    w = np.random.rand(5) # define a random weight vector\n",
    "    probs=y_pred(X, w, 0) # compute the probabilities of P(y=1|x;w) using your y_pred function\n",
    "    return all(probs>=0) and all(probs<=1) # check if all outputs are >=0 and <=1\n",
    "\n",
    "def test_ypred3():\n",
    "    n = 20\n",
    "    d = 5\n",
    "    X = np.random.rand(n,d) # generate n random vectors with d dimensions\n",
    "    w = np.random.rand(5) # define a random weight vector\n",
    "    probs1=y_pred(X, w, 0) # compute the probabilities of P(y=1|x;w) using your y_pred function\n",
    "    probs2=y_pred(X,-w, 0) # compute the probabilities of P(y=1|x;w) using your y_pred function\n",
    "    return np.linalg.norm(probs1+probs2-1)<1e-08 # check if P(y|x;w)+P(y|x;-w)=1\n",
    "\n",
    "\n",
    "\n",
    "def test_ypred4():\n",
    "    X=np.random.rand(25,4) # define random input\n",
    "    w=np.array([1,0,0,0]) # all-zeros weight vector\n",
    "    prob=y_pred(X, w, 0) # compute P(y|X;w)\n",
    "    truth=sigmoid(X[:,0]) # should simply be the sigmoid of the first feature\n",
    "    return np.linalg.norm(prob-truth)<1e-08 # see if they match\n",
    "\n",
    "\n",
    "def test_ypred5(): \n",
    "    X=np.array([[0.61793598, 0.09367891], # define 3 inputs (2D)\n",
    "               [0.79447745, 0.98605996],\n",
    "               [0.53679997, 0.4253885 ]])\n",
    "    w=np.array([0.9822789 , 0.16017851]); # define weight vector\n",
    "    prob=y_pred(X, w, 3) # compute P(y|X;w)\n",
    "    truth=np.array([0.97396645,0.98089179,0.97328431]) # this is the grount truth\n",
    "    return np.linalg.norm(prob-truth)<1e-08 # see if they match\n",
    "\n",
    "runtest(test_ypred1, 'test_ypred1')\n",
    "runtest(test_ypred2, 'test_ypred2')\n",
    "runtest(test_ypred3, 'test_ypred3')\n",
    "runtest(test_ypred4, 'test_ypred4')\n",
    "runtest(test_ypred5, 'test_ypred5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-test_ypred1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_ypred1\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "n = 20\n",
    "d = 5\n",
    "X = np.random.rand(n,d) # generate n random vectors with d dimensions\n",
    "w = np.random.rand(5) # define a random weight vector\n",
    "probs=y_pred(X,w,0) # compute the probabilities of P(y=1|x;w) using your y_pred function\n",
    "assert probs.shape == (n, ) # check if all outputs are >=0 and <=1\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-test_ypred2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_ypred2\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "n = 20\n",
    "d = 5\n",
    "X = np.random.rand(n,d) # generate n random vectors with d dimensions\n",
    "w = np.random.rand(5) # define a random weight vector\n",
    "probs=y_pred(X, w, 0) # compute the probabilities of P(y=1|x;w) using your y_pred function\n",
    "assert all(probs>=0) and all(probs<=1) # check if all outputs are >=0 and <=1\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-test_ypred3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_ypred3\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "n = 20\n",
    "d = 5\n",
    "X = np.random.rand(n,d) # generate n random vectors with d dimensions\n",
    "w = np.random.rand(5) # define a random weight vector\n",
    "probs1=y_pred(X, w, 0) # compute the probabilities of P(y=1|x;w) using your y_pred function\n",
    "probs2=y_pred(X,-w, 0) # compute the probabilities of P(y=1|x;w) using your y_pred function\n",
    "assert np.linalg.norm(probs1+probs2-1)<1e-08 # check if P(y|x;w)+P(y|x;-w)=1\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-test_ypred4",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_ypred4\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "X=np.random.rand(25,4) # define random input\n",
    "w=np.array([1,0,0,0]) # all-zeros weight vector\n",
    "prob=y_pred(X, w, 0) # compute P(y|X;w)\n",
    "truth=sigmoid(X[:,0]) # should simply be the sigmoid of the first feature\n",
    "assert np.linalg.norm(prob-truth)<1e-08 # see if they match\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-test_ypred5",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_ypred5\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "X=np.array([[0.61793598, 0.09367891], # define 3 inputs (2D)\n",
    "           [0.79447745, 0.98605996],\n",
    "           [0.53679997, 0.4253885 ]])\n",
    "w=np.array([0.9822789 , 0.16017851]); # define weight vector\n",
    "prob=y_pred(X, w, 3) # compute P(y|X;w)\n",
    "truth=np.array([0.97396645,0.98089179,0.97328431]) # this is the grount truth\n",
    "assert np.linalg.norm(prob-truth)<1e-08 # see if they match\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-00d32abe379c13b2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Part Three: Implement <code>log_loss</code> [Graded]</h3>\n",
    "\n",
    "<p>Assume you are given a data set $\\mathbf{x}_1,\\dots,\\mathbf{x}_n$ stored as row-vectors in a matrix <code>X</code> with labels $y_1,\\dots,y_n$ stored in vector <code>y</code>. Compute the <b>negative</b> log-likelihood (<code>log_loss</code>) of all inputs\n",
    "    $$NLL=-\\log P(\\mathbf{y}\\;|\\;\\mathbf{X};\\mathbf{w}, b)=-\\sum_{i=1}^n \\log\\left((P(y_i\\;|\\; \\mathbf{x}_i;\\mathbf{w}, b))\\right)=-\\sum_{i=1}^n \\log\\left(\\sigma(y_i(\\mathbf{w}^\\top \\mathbf{x}_i + b))\\right).$$\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-log_loss",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def log_loss(X, y, w, b=0):\n",
    "    # Input:\n",
    "    # X: nxd matrix\n",
    "    # y: n-dimensional vector with labels (+1 or -1)\n",
    "    # w: d-dimensional vector\n",
    "    # Output:\n",
    "    # a scalar\n",
    "    assert np.sum(np.abs(y))==len(y) # check if all labels in y are either +1 or -1\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    nll=  np.sum(np.log(1+np.exp(-y*(X@w + b))))\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    return nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-log_loss-selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def test_logloss1():\n",
    "    X = np.random.rand(25,5) # generate n random vectors with d dimensions\n",
    "    w = np.random.rand(5) # define a random weight vector\n",
    "    b = np.random.rand(1) # define a bias\n",
    "    y = np.ones(25) # set labels all-(+1)\n",
    "    ll=log_loss(X,y,w, b) # compute the probabilities of P(y=1|x;w) using your y_pred function\n",
    "    # if labels are all-ones function becomes simply the sum of log of y_pred\n",
    "    return np.isscalar(ll) # check whether the output is a scalar\n",
    "\n",
    "def test_logloss2():\n",
    "    X = np.random.rand(25,5) # generate n random vectors with d dimensions\n",
    "    w = np.random.rand(5) # define a random weight vector\n",
    "    b = np.random.rand(1) # define a bias\n",
    "    y = np.ones(25) # set labels all-(+1)\n",
    "    ll=log_loss(X,y,w, b) # compute the probabilities of P(y=1|x;w) using your y_pred function\n",
    "    ll2=-np.sum(np.log(y_pred(X, w, b))) # if labels are all-ones function becomes simply the sum of log of y_pred\n",
    "    return np.linalg.norm(ll-ll2)<1e-05\n",
    "\n",
    "def test_logloss3():\n",
    "    X = np.random.rand(25,5) # generate n random vectors with d dimensions\n",
    "    w = np.random.rand(5) # define a random weight vector\n",
    "    b = np.random.rand(1) # define a bias\n",
    "    y = -np.ones(25) # set labels all-(-1)\n",
    "    ll=log_loss(X,y,w,b) # compute the probabilities of P(y=1|x;w) using your y_pred function\n",
    "    ll2=-np.sum(np.log(1-y_pred(X,w, b))) # if labels are all-ones function becomes simply the sum of log of 1-y_pred\n",
    "    return np.linalg.norm(ll-ll2)<1e-05\n",
    "\n",
    "def test_logloss4():\n",
    "    X = np.random.rand(20,5) # generate n random vectors with d dimensions\n",
    "    w = np.array([0,0,0,0,0]) # define an all-zeros weight vector\n",
    "    y = (np.random.rand(20)>0.5)*2-1; # define n random labels (+1 or -1)\n",
    "    ll=log_loss(X,y,w,0) # compute the probabilities of P(y=1|x;w) using your y_pred function\n",
    "    # the log-likelihood for each of the 20 examples should be exactly 0.5:\n",
    "    return np.linalg.norm(ll+20*np.log(0.5))<1e-08 \n",
    "\n",
    "def test_logloss5():\n",
    "    X = np.random.rand(500,15) # generate n random vectors with d dimensions\n",
    "    w = np.random.rand(15) # define a random weight vector\n",
    "    b = np.random.rand(1) # define a bias\n",
    "    y = (np.random.rand(500)>0.5)*2-1; # define n random labels (+1 or -1)\n",
    "    ll=log_loss(X,y,w,b) # compute the probabilities of P(y=1|x;w) using your y_pred function\n",
    "    ll2=log_loss_grader(X,y,w,b) # compute the probabilities of P(y=1|x;w) using your y_pred function\n",
    "    return np.linalg.norm(ll-ll2)<1e-05\n",
    "\n",
    "runtest(test_logloss1, 'test_logloss1')\n",
    "runtest(test_logloss2, 'test_logloss2')\n",
    "runtest(test_logloss3, 'test_logloss3')\n",
    "runtest(test_logloss4, 'test_logloss4')\n",
    "runtest(test_logloss5, 'test_logloss5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-test_logloss1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_logloss1\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "X = np.random.rand(25,5) # generate n random vectors with d dimensions\n",
    "w = np.random.rand(5) # define a random weight vector\n",
    "b = np.random.rand(1) # define a bias\n",
    "y = np.ones(25) # set labels all-(+1)\n",
    "ll=log_loss(X,y,w, b) # compute the probabilities of P(y=1|x;w) using your y_pred function\n",
    "# if labels are all-ones function becomes simply the sum of log of y_pred\n",
    "assert np.isscalar(ll) # check whether the output is a scalar\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-test_logloss2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_logloss2\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "X = np.random.rand(25,5) # generate n random vectors with d dimensions\n",
    "w = np.random.rand(5) # define a random weight vector\n",
    "b = np.random.rand(1) # define a bias\n",
    "y = np.ones(25) # set labels all-(+1)\n",
    "ll=log_loss(X,y,w, b) # compute the probabilities of P(y=1|x;w) using your y_pred function\n",
    "ll2=-np.sum(np.log(y_pred(X, w, b))) # if labels are all-ones function becomes simply the sum of log of y_pred\n",
    "assert np.linalg.norm(ll-ll2)<1e-05\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-test_logloss3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_logloss3\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "X = np.random.rand(25,5) # generate n random vectors with d dimensions\n",
    "w = np.random.rand(5) # define a random weight vector\n",
    "b = np.random.rand(1) # define a bias\n",
    "y = -np.ones(25) # set labels all-(-1)\n",
    "ll=log_loss(X,y,w,b) # compute the probabilities of P(y=1|x;w) using your y_pred function\n",
    "ll2=-np.sum(np.log(1-y_pred(X,w, b))) # if labels are all-ones function becomes simply the sum of log of 1-y_pred\n",
    "assert np.linalg.norm(ll-ll2)<1e-05\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-test_logloss4",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_logloss4\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "X = np.random.rand(20,5) # generate n random vectors with d dimensions\n",
    "w = np.array([0,0,0,0,0]) # define an all-zeros weight vector\n",
    "y = (np.random.rand(20)>0.5)*2-1; # define n random labels (+1 or -1)\n",
    "ll=log_loss(X,y,w,0) # compute the probabilities of P(y=1|x;w) using your y_pred function\n",
    "# the log-likelihood for each of the 20 examples should be exactly 0.5:\n",
    "assert np.linalg.norm(ll+20*np.log(0.5))<1e-08 \n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-test_logloss5",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_logloss5\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "X = np.random.rand(500,15) # generate n random vectors with d dimensions\n",
    "w = np.random.rand(15) # define a random weight vector\n",
    "b = np.random.rand(1) # define a bias\n",
    "y = (np.random.rand(500)>0.5)*2-1; # define n random labels (+1 or -1)\n",
    "ll=log_loss(X,y,w,b) # compute the probabilities of P(y=1|x;w) using your y_pred function\n",
    "ll2=log_loss_grader(X,y,w,b) # compute the probabilities of P(y=1|x;w) using your y_pred function\n",
    "assert np.linalg.norm(ll-ll2)<1e-05\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-af8fd1a81a6e6b97",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Part Four: Compute Gradient [Graded]</h3>\n",
    "\n",
    "Now, verify that the gradient of the log-loss with respect to the weight vector is:\n",
    "$$\\frac{\\partial NLL(\\mathbf{X},\\mathbf{y},\\mathbf{w}, b)}{\\partial \\mathbf{w}}=\\sum_{i=1}^n -y_i\\sigma(-y_i(\\mathbf{w}^\\top\\mathbf{x}_i + b))\\mathbf{x}_i.$$ \n",
    "\n",
    "$$\\frac{\\partial NLL(\\mathbf{X},\\mathbf{y},\\mathbf{w}, b)}{\\partial b}=\\sum_{i=1}^n -y_i\\sigma(-y_i(\\mathbf{w}^\\top\\mathbf{x}_i + b)).$$\n",
    "Implement the function <code>gradient</code> which returns the first derivative with respect to <code>w, b</code> for a given <code>X, y, w, b</code>.\n",
    "\n",
    "Hint: remember that you derived earlier that $\\sigma'(z)=\\sigma(z)(1-\\sigma(z))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-gradient",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def gradient(X, y, w, b):\n",
    "    # Input:\n",
    "    # X: nxd matrix\n",
    "    # y: n-dimensional vector with labels (+1 or -1)\n",
    "    # w: d-dimensional vector\n",
    "    # b: a scalar bias term\n",
    "    # Output:\n",
    "    # wgrad: d-dimensional vector with gradient\n",
    "    # bgrad: a scalar with gradient\n",
    "    \n",
    "    n, d = X.shape\n",
    "    wgrad = np.zeros(d)\n",
    "    bgrad = 0.0\n",
    "    ### BEGIN SOLUTION\n",
    "    temp = (-y*sigmoid(-y*(X@w + b)))\n",
    "    wgrad = X.T@temp\n",
    "    bgrad = np.sum(temp)\n",
    "    ### END SOLUTION\n",
    "    return wgrad, bgrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-gradient-selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def test_grad1():\n",
    "    X = np.random.rand(25,5) # generate n random vectors with d dimensions\n",
    "    w = np.random.rand(5) # define a random weight vector\n",
    "    b = np.random.rand(1) # define a bias\n",
    "    y = (np.random.rand(25)>0.5)*2-1 # set labels all-(+1)\n",
    "    wgrad, bgrad = gradient(X, y, w, b) # compute the gradient using your function\n",
    "    \n",
    "    return wgrad.shape == w.shape and np.isscalar(bgrad)\n",
    "\n",
    "\n",
    "def test_grad2():\n",
    "    X = np.random.rand(25,5) # generate n random vectors with d dimensions\n",
    "    w = np.random.rand(5) # define a random weight vector\n",
    "    b = np.random.rand(1) # define a bias\n",
    "    y = (np.random.rand(25)>0.5)*2-1 # set labels all-(+1)\n",
    "    wgrad, bgrad = gradient(X, y, w, b) # compute the gradient using your function\n",
    "    wgrad2, bgrad2 = gradient_grader(X, y, w, b) # compute the gradient using ground truth\n",
    "    return np.linalg.norm(wgrad - wgrad2)<1e-06 and np.linalg.norm(bgrad - bgrad2) < 1e-06 # test if they match\n",
    "\n",
    "def test_grad3():\n",
    "    X = np.random.rand(25,5) # generate n random vectors with d dimensions\n",
    "    y = (np.random.rand(25)>0.5)*2-1 # set labels all-(+1)\n",
    "    w = np.random.rand(5) # define a random weight vector\n",
    "    b = np.random.rand(1) \n",
    "\n",
    "    w_s = np.random.rand(5)*1e-05 # define tiny random step \n",
    "    b_s = np.random.rand(1)*1e-05 # define tiny random step \n",
    "    ll1 = log_loss(X,y,w+w_s, b+b_s)  # compute log-likelihood after taking a step\n",
    "    \n",
    "    ll = log_loss(X,y,w,b) # use Taylor's expansion to approximate new loss with gradient\n",
    "    wgrad, bgrad =gradient(X,y,w,b) # compute gradient\n",
    "    ll2=ll+ wgrad @ w_s + bgrad * b_s # take linear step with Taylor's approximation\n",
    "    return np.linalg.norm(ll1-ll2)<1e-05 # test if they match\n",
    "\n",
    "def test_grad4():\n",
    "    w1, b1, losses1 = logistic_regression_grader(features, labels, 1000, 1e-03, gradient)\n",
    "    w2, b2, losses2 = logistic_regression_grader(features, labels, 1000, 1e-03)\n",
    "    return(np.abs(losses1[-1]-losses2[-1])<0.1)\n",
    "\n",
    "runtest(test_grad1, 'test_grad1')\n",
    "runtest(test_grad2, 'test_grad2')\n",
    "runtest(test_grad3, 'test_grad3')\n",
    "runtest(test_grad4, 'test_grad4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-test_grad1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_grad 1\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "X = np.random.rand(25,5) # generate n random vectors with d dimensions\n",
    "w = np.random.rand(5) # define a random weight vector\n",
    "b = np.random.rand(1) # define a bias\n",
    "y = (np.random.rand(25)>0.5)*2-1 # set labels all-(+1)\n",
    "wgrad, bgrad = gradient(X, y, w, b) # compute the gradient using your function\n",
    "\n",
    "assert wgrad.shape == w.shape and np.isscalar(bgrad)\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-test_grad2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_grad2\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "X = np.random.rand(25,5) # generate n random vectors with d dimensions\n",
    "w = np.random.rand(5) # define a random weight vector\n",
    "b = np.random.rand(1) # define a bias\n",
    "y = (np.random.rand(25)>0.5)*2-1 # set labels all-(+1)\n",
    "wgrad, bgrad = gradient(X, y, w, b) # compute the gradient using your function\n",
    "wgrad2, bgrad2 = gradient_grader(X, y, w, b) # compute the gradient using ground truth\n",
    "assert np.linalg.norm(wgrad - wgrad2)<1e-06 and np.linalg.norm(bgrad - bgrad2) < 1e-06 # test if they match\n",
    "\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-test_grad3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_grad3\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "X = np.random.rand(25,5) # generate n random vectors with d dimensions\n",
    "y = (np.random.rand(25)>0.5)*2-1 # set labels all-(+1)\n",
    "w = np.random.rand(5) # define a random weight vector\n",
    "b = np.random.rand(1) \n",
    "\n",
    "w_s = np.random.rand(5)*1e-05 # define tiny random step \n",
    "b_s = np.random.rand(1)*1e-05 # define tiny random step \n",
    "ll1 = log_loss(X,y,w+w_s, b+b_s)  # compute log-likelihood after taking a step\n",
    "\n",
    "ll = log_loss(X,y,w,b) # use Taylor's expansion to approximate new loss with gradient\n",
    "wgrad, bgrad =gradient(X,y,w,b) # compute gradient\n",
    "ll2=ll+ wgrad @ w_s + bgrad * b_s # take linear step with Taylor's approximation\n",
    "assert np.linalg.norm(ll1-ll2)<1e-05 # test if they match\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-test_grad4",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_grad4\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "w1, b1, losses1 = logistic_regression_grader(features, labels, 1000, 1e-03, gradient)\n",
    "w2, b2, losses2 = logistic_regression_grader(features, labels, 1000, 1e-03)\n",
    "assert(np.abs(losses1[-1]-losses2[-1])<0.1)\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-692e12ddff9a0aaa",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Part Five: Weight Update of Gradient Ascent [Graded]</h3>\n",
    "    \n",
    "Write code below to implement the weight update of gradient descent on the log_loss function.  Hint: use the `gradient` and `log_loss` functions from above. Please use a <b>constant</b> learning rate throughout (i.e. do not decrease the learning rate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-logistic_regression",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def logistic_regression(X, y, max_iter, alpha):\n",
    "    n, d = X.shape\n",
    "    w = np.zeros(d)\n",
    "    b = 0.0\n",
    "    losses = np.zeros(max_iter)    \n",
    "    \n",
    "    for step in range(max_iter):\n",
    "        ### BEGIN SOLUTION\n",
    "        wgrad, bgrad=gradient(X, y, w, b)\n",
    "        w -= alpha * wgrad\n",
    "        b -= alpha * bgrad\n",
    "        losses[step] = log_loss(X, y, w, b)\n",
    "        ### END SOLUTION\n",
    "    return w, b, losses\n",
    "\n",
    "weight, b, losses = logistic_regression(features, labels, 1000, 1e-04)\n",
    "plot(losses)\n",
    "xlabel('iterations')\n",
    "ylabel('log_loss')\n",
    "# your loss should go down :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-logistic_regression-selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def test_logistic_regression1():\n",
    "\n",
    "    XUnit = np.array([[-1,1],[-1,0],[0,-1],[-1,2],[1,-2],[1,-1],[1,0],[0,1],[1,-2],[-1,2]])\n",
    "    YUnit = np.hstack((np.ones(5), -np.ones(5)))\n",
    "\n",
    "    w1, b1, _ = logistic_regression(XUnit, YUnit, 30000, 5e-5)\n",
    "    w2, b2, _ = logistic_regression_grader(XUnit, YUnit, 30000, 5e-5)\n",
    "    return (np.linalg.norm(w1 - w2) < 1e-5) and (np.linalg.norm(b1 - b2) < 1e-5)\n",
    "\n",
    "def test_logistic_regression2():\n",
    "    X = np.vstack((np.random.randn(50, 5), np.random.randn(50, 5) + 2))\n",
    "    Y = np.hstack((np.ones(50), -np.ones(50)))\n",
    "    max_iter = 300\n",
    "    alpha = 1e-5\n",
    "    w1, b1, _ = logistic_regression(X, Y, max_iter, alpha)\n",
    "    w2, b2, _ = logistic_regression_grader(X, Y, max_iter, alpha)\n",
    "    return (np.linalg.norm(w1 - w2) < 1e-5) and (np.linalg.norm(b1 - b2) < 1e-5)\n",
    "\n",
    "runtest(test_logistic_regression1, 'test_logistic_regression1')\n",
    "runtest(test_logistic_regression2, 'test_logistic_regression2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-test_logistic_regression1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_logistic_regression1\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "XUnit = np.array([[-1,1],[-1,0],[0,-1],[-1,2],[1,-2],[1,-1],[1,0],[0,1],[1,-2],[-1,2]])\n",
    "YUnit = np.hstack((np.ones(5), -np.ones(5)))\n",
    "\n",
    "w1, b1, _ = logistic_regression(XUnit, YUnit, 30000, 5e-5)\n",
    "w2, b2, _ = logistic_regression_grader(XUnit, YUnit, 30000, 5e-5)\n",
    "assert (np.linalg.norm(w1 - w2) < 1e-5) and (np.linalg.norm(b1 - b2) < 1e-5)\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-test_logistic_regression_2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_logistic_regression2\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "X = np.vstack((np.random.randn(50, 5), np.random.randn(50, 5) + 2))\n",
    "Y = np.hstack((np.ones(50), -np.ones(50)))\n",
    "max_iter = 300\n",
    "alpha = 1e-5\n",
    "w1, b1, _ = logistic_regression(X, Y, max_iter, alpha)\n",
    "w2, b2, _ = logistic_regression_grader(X, Y, max_iter, alpha)\n",
    "assert (np.linalg.norm(w1 - w2) < 1e-5) and (np.linalg.norm(b1 - b2) < 1e-5)\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f874275c305249ec",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2>Original Data</h2>\n",
    "<p>Now run your implementation on the binary classification data from the top of the notebook.  Check your code by plotting the values of the negative log likelihood - should these values increase or decrease as the number of iterations grows?  Do your values move in the right direction?</p>\n",
    "\n",
    "You can tune `max_iter` and `alpha` to see how they affect convergence!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a8f0e2256f9dc7a7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_iter = 10000\n",
    "alpha = 1e-4\n",
    "final_w, final_b, losses = logistic_regression(features, labels, max_iter, alpha)\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.plot(losses)\n",
    "plt.title(\"Loss vs. iteration\", size=15)\n",
    "plt.xlabel(\"Num iteration\", size=13)\n",
    "plt.ylabel(\"Loss value\", size=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-334b2b1ff6a97606",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Below, we'll take the final weights from the logistic solver and predict labels for the entire dataset.  By plotting the results, we can get a sense of where the linear decision boundary lies.  What do you notice?  What could be changed to further improve the accuracy of the classifier? (_Hint: take a look at the second video in Module 1._)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d3c49e5cdaebdd3d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scores = y_pred(features, final_w, final_b)\n",
    "\n",
    "pred_labels = (scores > 0.5).astype(int)\n",
    "pred_labels[pred_labels != 1] = -1\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "\n",
    "# plot the decision boundary \n",
    "x = np.linspace(np.amin(features[:, 0]), np.amax(features[:, 0]), 10)\n",
    "y = -(final_w[0] * x + final_b)/ final_w[1] \n",
    "plt.plot(x, y)\n",
    "\n",
    "plt.scatter(features[:, 0], features[:, 1],\n",
    "            c = pred_labels, alpha = .6)\n",
    "plt.title(\"Predicted labels\", size=15)\n",
    "plt.xlabel(\"Feature 1\", size=13)\n",
    "plt.ylabel(\"Feature 2\", size=13)\n",
    "plt.axis([-3,10,0,15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-888232b67e0c4279",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h1>Build an Email Spam Filter</h1>\n",
    "<p> With logistic regression implemented, you can now build an email spam filter using logistic regression. The functions below load in pre-processed email data, where emails are represented as one-hot vectors.</p>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c8c8b48777d4cb55",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# tokenize the email and hashes the symbols into a vector\n",
    "def extract_features_naive(path, B):\n",
    "    with open(path, 'r') as email_file:\n",
    "        # initialize all-zeros feature vector\n",
    "        v = np.zeros(B)\n",
    "        email = email_file.read()\n",
    "        # breaks for non-ascii characters\n",
    "        tokens = email.split()\n",
    "        for token in tokens:\n",
    "            v[hash(token) % B] = 1\n",
    "    return v\n",
    "\n",
    "def load_spam_data(extract_features, B=512, path=\"data_train/\"):\n",
    "    '''\n",
    "    INPUT:\n",
    "    extractfeatures : function to extract features\n",
    "    B               : dimensionality of feature space\n",
    "    path            : the path of folder to be processed\n",
    "    \n",
    "    OUTPUT:\n",
    "    X, Y\n",
    "    '''\n",
    "    \n",
    "    with open(os.path.join(path, 'index'), 'r') as f:\n",
    "        all_emails = [x for x in f.read().split('\\n') if ' ' in x]\n",
    "    \n",
    "    xs = np.zeros((len(all_emails), B))\n",
    "    ys = np.zeros(len(all_emails))\n",
    "    for i, line in enumerate(all_emails):\n",
    "        label, filename = line.split(' ')\n",
    "        # make labels +1 for \"spam\" and -1 for \"ham\"\n",
    "        ys[i] = (label == 'spam') * 2 - 1\n",
    "        xs[i, :] = extract_features(os.path.join(path, filename), B)\n",
    "    print('Loaded %d input emails.' % len(ys))\n",
    "    return xs, ys\n",
    "\n",
    "Xspam, Yspam = load_spam_data(extract_features_naive)\n",
    "Xspam.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-01440c3c0c02e9a7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Split The Dataset</h3>\n",
    "\n",
    "<p>Now that you have loaded the dataset, it's time to split it into training and testing. To evaluate your algorithm, run the code below to split off 20% of this data into a testing set, leaving 80% as your training set.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-afd447bf48aa9770",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Split data into training (xTr and yTr) \n",
    "# and testing (xTv and yTv)\n",
    "n, d = Xspam.shape\n",
    "# Allocate 80% of the data for training and 20% for testing\n",
    "cutoff = int(np.ceil(0.8 * n))\n",
    "# indices of training samples\n",
    "xTr = Xspam[:cutoff,:]\n",
    "yTr = Yspam[:cutoff]\n",
    "# indices of testing samples\n",
    "xTv = Xspam[cutoff:]\n",
    "yTv = Yspam[cutoff:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1dce047da83653a9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Training and Evaluating </h3>\n",
    "\n",
    "<p> Running the following cell will produce a logistic regression model that can classify unseen emails at roughly 88% accuracy</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f52ed14dba328199",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "max_iter = 5000\n",
    "alpha = 1e-5\n",
    "final_w_spam, final_b_spam, losses = logistic_regression(xTr, yTr, max_iter, alpha)\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.plot(losses)\n",
    "plt.title(\"Loss vs. iteration\", size=15)\n",
    "plt.xlabel(\"Num iteration\", size=13)\n",
    "plt.ylabel(\"Loss value\", size=13)\n",
    "\n",
    "# evaluate training accuracy\n",
    "scoresTr = y_pred(xTr, final_w_spam, final_b_spam)\n",
    "pred_labels = (scoresTr > 0.5).astype(int)\n",
    "pred_labels[pred_labels != 1] = -1\n",
    "trainingacc = np.mean(pred_labels == yTr)\n",
    "\n",
    "# evaluate testing accuracy\n",
    "scoresTv = y_pred(xTv, final_w_spam, final_b_spam)\n",
    "pred_labels = (scoresTv > 0.5).astype(int)\n",
    "pred_labels[pred_labels != 1] = -1\n",
    "validationacc = np.mean(pred_labels==yTv)\n",
    "print(\"Training accuracy %2.2f%%\\nValidation accuracy %2.2f%%\\n\" % (trainingacc*100,validationacc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7f69eb68a943ead0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2>Challenge: Improve Your Spam Classifier <b>[Ungraded]</b></h2>\n",
    "\n",
    "<p>You can improve your classifier in two ways:</p>\n",
    "\n",
    "<ol>\n",
    "<li><b>Feature Extraction</b>:\n",
    "Modify the function <code>extract_features_comp()</code>.\n",
    "This function takes in a file path <code>path</code> and\n",
    "a feature dimension <code>B</code> and should output a feature vector of dimension <code>B</code>.\n",
    "The autograder will pass in a file path pointing to a file that contains an email,\n",
    "and set <code>B</code> = <code>feature_dimension</code>.\n",
    "We provide a naive feature extraction as an example.\n",
    "</li>\n",
    "<li><b>Model Training</b>:\n",
    "Modify the function <code>train_spam_filter_comp()</code>.\n",
    "This function takes in training data <code>xTr</code> and training labels <code>yTr</code> and\n",
    "should output a weight vector <code>w</code> and bias <code>b</code> for linear classification. The predictions will be calculated exactly the same way as we have demonstrated in the previous cell. \n",
    "We provide an initial implementation using gradient descent and logistic regression.\n",
    "</li>\n",
    "</ol>\n",
    "\n",
    "<p>Your model will be trained on the same training set above (loaded by <code>load_spam_data()</code>), but we will test its accuracy on a secret dataset of emails.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-extract_features_comp",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "feature_dimension = 512\n",
    "def extract_features_comp(path, B=feature_dimension):\n",
    "    '''\n",
    "    INPUT:\n",
    "    path : file path of email\n",
    "    B    : dimensionality of feature vector\n",
    "    \n",
    "    OUTPUTS:\n",
    "    x    : B dimensional vector\n",
    "    '''\n",
    "    x = np.zeros(B)\n",
    "    with open(path, 'r') as email_file:\n",
    "        email = email_file.read()\n",
    "        # breaks for non-ascii characters\n",
    "        tokens = email.split()\n",
    "        for token in tokens:\n",
    "            x[hash(token) % B] = 1\n",
    "    ### BEGIN SOLUTION\n",
    "    ### END SOLUTION\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-train_spam_filter_comp",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def train_spam_filter_comp(xTr, yTr):\n",
    "    '''\n",
    "    INPUT:\n",
    "    xTr : nxd dimensional matrix (each row is an input vector)\n",
    "    yTr : d   dimensional vector (each entry is a label)\n",
    "    \n",
    "    OUTPUTS:\n",
    "    w : d dimensional vector for linear classification\n",
    "    '''\n",
    "    n, d = xTr.shape\n",
    "    \n",
    "    max_iter = 100\n",
    "    alpha = 1e-5\n",
    "    w, b, losses = logistic_regression(xTr, yTr, max_iter, alpha)\n",
    "    ### BEGIN SOLUTION\n",
    "    ### END SOLUTION\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-comptest",
     "locked": true,
     "points": 0,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Challenge test cell - no point value\n",
    "### BEGIN HIDDEN TESTS\n",
    "def competition_test():\n",
    "    xTr,yTr = load_spam_data(extract_features_comp, feature_dimension, 'data_train')\n",
    "    w, b = train_spam_filter_comp(xTr, yTr)\n",
    "    xTe,yTe = load_spam_data(extract_features_comp, feature_dimension, 'data_test')\n",
    "    scoresTe = sigmoid_grader(xTe @ w + b)\n",
    "    \n",
    "    preds = (scoresTe > 0.5).astype(int)\n",
    "    preds[preds != 1] = -1\n",
    "    \n",
    "    pos_ind = (yTe == 1)\n",
    "    neg_ind = (yTe == -1)\n",
    "    \n",
    "    pos_acc = np.mean(yTe[pos_ind] == preds[pos_ind])\n",
    "    neg_acc = np.mean(yTe[neg_ind] == preds[neg_ind])\n",
    "    \n",
    "    test_accuracy = 0.5*pos_acc + 0.5*neg_acc\n",
    "    \n",
    "    scoresTr =  sigmoid_grader(xTr @ w + b)\n",
    "    preds_Tr = (scoresTr > 0.5).astype(int)\n",
    "    preds_Tr[preds_Tr != 1] = -1\n",
    "    \n",
    "    training_accuracy = np.mean(preds_Tr == yTr)\n",
    "    return training_accuracy, test_accuracy\n",
    "\n",
    "training_acc, test_acc = competition_test()\n",
    "print(\"Your features and model achieved training accuracy: {:.2f}% and test accuracy: {:.2f}%\".format(training_acc*100, test_acc*100))\n",
    "### END HIDDEN TESTS"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
