{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d89a9eaca8f89db2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2>About this Project</h2>\n",
    "<p>In this project, you will implement your own perceptron. You'll implement a linear classifier and the perceptron update functions. You'll also have a chance to visualize your perceptron.</p>\n",
    "\n",
    "<h3>Evaluation</h3>\n",
    "\n",
    "<p><strong>This project must be successfully completed and submitted in order to receive credit for this course. Your score on this project will be included in your final grade calculation.</strong><p>\n",
    "    \n",
    "<p>You are expected to write code where you see <em># YOUR CODE HERE</em> within the cells of this notebook. Not all cells will be graded; code input cells followed by cells marked with <em>#Autograder test cell</em> will be graded. Upon submitting your work, the code you write at these designated positions will be assessed using an \"autograder\" that will run all test cells to assess your code. You will receive feedback from the autograder that will identify any errors in your code. Use this feedback to improve your code if you need to resubmit. Be sure not to change the names of any provided functions, classes, or variables within the existing code cells, as this will interfere with the autograder. Also, remember to execute all code cells sequentially, not just those you’ve edited, to ensure your code runs properly.</p>\n",
    "    \n",
    "<p>You can resubmit your work as many times as necessary before the submission deadline. If you experience difficulty or have questions about this exercise, use the Q&A discussion board to engage with your peers or seek assistance from the instructor.<p>\n",
    "\n",
    "<p>Before starting your work, please review <a href=\"https://s3.amazonaws.com/ecornell/global/eCornellPlagiarismPolicy.pdf\">eCornell's policy regarding plagiarism</a> (the presentation of someone else's work as your own without source credit).</p>\n",
    "\n",
    "<h3>Submit Code for Autograder Feedback</h3>\n",
    "\n",
    "<p>Once you have completed your work on this notebook, you will submit your code for autograder review. Follow these steps:</p>\n",
    "\n",
    "<ol>\n",
    "  <li><strong>Save your notebook.</strong></li>\n",
    "  <li><strong>Mark as Completed —</strong> In the blue menu bar along the top of this code exercise window, you’ll see a menu item called <strong>Education</strong>. In the <strong>Education</strong> menu, click <strong>Mark as Completed</strong> to submit your code for autograder/instructor review. This process will take a moment and a progress bar will show you the status of your submission.</li>\n",
    "\t<li><strong>Review your results —</strong> Once your work is marked as complete, the results of the autograder will automatically be presented in a new tab within the code exercise window. You can click on the assessment name in this feedback window to see more details regarding specific feedback/errors in your code submission.</li>\n",
    "  <li><strong>Repeat, if necessary —</strong> The Jupyter notebook will always remain accessible in the first tabbed window of the exercise. To reattempt the work, you will first need to click <strong>Mark as Uncompleted</strong> in the <strong>Education</strong> menu and then proceed to make edits to the notebook. Once you are ready to resubmit, follow steps one through three. You can repeat this procedure as many times as necessary.</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-07165ae949d0e7dc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2>Getting Started</h2>\n",
    "<h3>Python Initialization</h3> \n",
    "\n",
    "Please run the following code to initialize your Python kernel. You should be running a version of Python 3.x. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-347e83ddfe5848d2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib \n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "sys.path.append('/home/codio/workspace/.guides/hf')\n",
    "from helper import *\n",
    "\n",
    "\n",
    "%matplotlib notebook\n",
    "print('You\\'re running python %s' % sys.version.split(' ')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9259529aa911a4e4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2>The Perceptron</h2>\n",
    "\n",
    "<p>The perceptron is a basic linear classifier. The following cells will walk you through steps and ask you to finish the necessary functions in a pre-defined order. Code cells requiring your input will display # YOUR CODE HERE and graded portions will be adequately labeled. Unless specified otherwise, do not use loops.<br></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-30625fcf06df5461",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Part One [Graded]</h3>\n",
    "<p>Implement the function below to update the perceptron given an input vector, label, and weight vector. Do <b>not</b> check if an update is necessary. This function can assume that it is only called when an update should be performed. Hint: Refer to the perceptron pseudocode page for help with implementing the update step.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-perceptron_update",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def perceptron_update(x,y,w):\n",
    "    \"\"\"\n",
    "    function w=perceptron_update(x,y,w);\n",
    "    \n",
    "    Implementation of Perceptron weights updating\n",
    "    Input:\n",
    "    x : input vector of d dimensions (d)\n",
    "    y : corresponding label (-1 or +1)\n",
    "    w : weight vector of d dimensions\n",
    "    \n",
    "    Output:\n",
    "    w : weight vector after updating (d)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    w += y*x.flatten()\n",
    "    return w.flatten()\n",
    "    ### END SOLUTION\n",
    "    \n",
    "# little test\n",
    "x = np.random.rand(10)\n",
    "y = -1\n",
    "w = np.random.rand(10)\n",
    "w1 = perceptron_update(x,y,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-perceptron_update_selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This self test will check that your perceptron_update function returns the correct values for input vector [0,1], label -1, and weight vector [1,1]\n",
    "\n",
    "def test_perceptron_update1():\n",
    "    x = np.array([0,1])\n",
    "    y = -1\n",
    "    w = np.array([1,1])\n",
    "    w1 = perceptron_update(x,y,w)\n",
    "    return (w1.reshape(-1,) == np.array([1,0])).all()\n",
    "\n",
    "def test_perceptron_update2(): \n",
    "    x = np.random.rand(25)\n",
    "    y = 1\n",
    "    w = np.zeros(25)\n",
    "    w1 = perceptron_update(x,y,w)\n",
    "    return np.linalg.norm(w1-x)<1e-8\n",
    "\n",
    "\n",
    "def test_perceptron_update3():\n",
    "    x = np.random.rand(25)\n",
    "    y = -1\n",
    "    w = np.zeros(25)\n",
    "    w1 = perceptron_update(x,y,w)\n",
    "    return np.linalg.norm(w1+x)<1e-8\n",
    "\n",
    "\n",
    "runtest(test_perceptron_update1, 'test_perceptron_update1')\n",
    "runtest(test_perceptron_update2, 'test_perceptron_update2')\n",
    "runtest(test_perceptron_update3, 'test_perceptron_update3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-test_perceptron_update1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell- worth 1 point\n",
    "# runs test_perceptron_update1\n",
    "### BEGIN HIDDEN TESTS\n",
    "x = np.array([0,1])\n",
    "y = -1\n",
    "w = np.array([1,1])\n",
    "w1 = perceptron_update(x,y,w)\n",
    "assert (w1.reshape(-1,) == np.array([1,0])).all()\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-test_perceptron_update2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell- worth 1 point\n",
    "# runs test_perceptron_update2\n",
    "### BEGIN HIDDEN TESTS\n",
    "# Test the perceptron update function on a simple matrix\n",
    "x = np.random.rand(25)\n",
    "y = 1\n",
    "w = np.zeros(25)\n",
    "w1 = perceptron_update(x,y,w)\n",
    "assert np.linalg.norm(w1-x)<1e-8\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-test_perceptron_update3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell- worth 1 point\n",
    "# runs test_perceptron_update3\n",
    "### BEGIN HIDDEN TESTS\n",
    "# Test the perceptron update function on a simple matrix\n",
    "x = np.random.rand(25)\n",
    "y = -1\n",
    "w = np.zeros(25)\n",
    "w1 = perceptron_update(x,y,w)\n",
    "assert np.linalg.norm(w1+x)<1e-8\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8c2c41b6b1bbec23",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Part Two [Graded]</h3>\n",
    "\n",
    "<p>Implement function <b><code>perceptron</code></b>. This should contain a loop that calls \n",
    "<b><code>perceptron_update</code></b>\n",
    " until it converges or the maximum iteration count, 100, has been reached.\n",
    " Make sure you randomize the order of the training data on each iteration (you can use <b><code>np.random.permutation()</code></b> to do this.)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-perceptron",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def perceptron(xs,ys):\n",
    "    \"\"\"\n",
    "    function w=perceptron(xs,ys);\n",
    "    \n",
    "    Implementation of a Perceptron classifier\n",
    "    Input:\n",
    "    xs : n input vectors of d dimensions (nxd)\n",
    "    ys : n labels (-1 or +1)\n",
    "    \n",
    "    Output:\n",
    "    w : weight vector (1xd)\n",
    "    b : bias term\n",
    "    \"\"\"\n",
    "\n",
    "    n, d = xs.shape     # so we have n input vectors, of d dimensions each\n",
    "    w = np.zeros(d)\n",
    "    b = 0.0\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    iter_count = 0\n",
    "    while True:\n",
    "        m = 0\n",
    "        # Shuffle the indices of the datapoints\n",
    "        for i in np.random.permutation(n):\n",
    "            if ys[i]*(np.dot(w, xs[i]) + b) <= 0:\n",
    "                perceptron_update(xs[i], ys[i], w)\n",
    "                b += ys[i]\n",
    "                m += 1\n",
    "        \n",
    "        iter_count += 1\n",
    "        if m == 0 or iter_count == 100:\n",
    "            break\n",
    "    ### END SOLUTION\n",
    "    return (w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-perceptron_selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# These self tests will check that your perceptron function successfully classifies points in two different linearly separable dataset \n",
    "\n",
    "def test_Perceptron1():\n",
    "    N = 100;\n",
    "    d = 10;\n",
    "    x = np.random.rand(N,d)\n",
    "    w = np.random.rand(1,d)\n",
    "    y = np.sign(w.dot(x.T))[0]\n",
    "    w, b = perceptron(x,y)\n",
    "    preds = classify_linear_grader(x,w,b)\n",
    "    return np.array_equal(preds.reshape(-1,),y.reshape(-1,))\n",
    "\n",
    "\n",
    "\n",
    "def test_Perceptron2():\n",
    "    x = np.array([ [-0.70072, -1.15826],  [-2.23769, -1.42917],  [-1.28357, -3.52909],  [-3.27927, -1.47949],  [-1.98508, -0.65195],  [-1.40251, -1.27096],  [-3.35145,-0.50274],  [-1.37491,-3.74950],  [-3.44509,-2.82399],  [-0.99489,-1.90591],   [0.63155,1.83584],   [2.41051,1.13768],  [-0.19401,0.62158],   [2.08617,4.41117],   [2.20720,1.24066],   [0.32384,3.39487],   [1.44111,1.48273],   [0.59591,0.87830],   [2.96363,3.00412],   [1.70080,1.80916]])\n",
    "    y = np.array([1]*10 + [-1]*10)\n",
    "    w, b =perceptron(x,y)\n",
    "    preds = classify_linear_grader(x,w,b)\n",
    "    return np.array_equal(preds.reshape(-1,),y.reshape(-1,))\n",
    "\n",
    "runtest(test_Perceptron1, 'test_Perceptron1')\n",
    "runtest(test_Perceptron2, 'test_Perceptron2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-test_Perceptron1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell- worth 1 point\n",
    "# runs test_Perceptron1\n",
    "### BEGIN HIDDEN TESTS\n",
    "# Test perceptron on a simple 10 dimensional data set\n",
    "N = 100;\n",
    "d = 10;\n",
    "x = np.random.rand(N,d)\n",
    "w = np.random.rand(1,d)\n",
    "y = np.sign(w.dot(x.T))[0]\n",
    "w, b = perceptron(x,y)\n",
    "preds = classify_linear_grader(x,w,b)\n",
    "assert np.array_equal(preds.reshape(-1,),y.reshape(-1,))\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-test_Perceptron2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell- worth 1 point\n",
    "# runs test_Perceptron2\n",
    "### BEGIN HIDDEN TESTS\n",
    "#Test the Perceptron on a linearly separable data set (this must lead to a solution with 100% accuracy):\n",
    "x = np.array([ [-0.70072, -1.15826],  [-2.23769, -1.42917],  [-1.28357, -3.52909],  [-3.27927, -1.47949],  [-1.98508, -0.65195],  [-1.40251, -1.27096],  [-3.35145,-0.50274],  [-1.37491,-3.74950],  [-3.44509,-2.82399],  [-0.99489,-1.90591],   [0.63155,1.83584],   [2.41051,1.13768],  [-0.19401,0.62158],   [2.08617,4.41117],   [2.20720,1.24066],   [0.32384,3.39487],   [1.44111,1.48273],   [0.59591,0.87830],   [2.96363,3.00412],   [1.70080,1.80916]])\n",
    "y = np.array([1]*10 + [-1]*10)\n",
    "w, b = perceptron(x,y)\n",
    "preds = classify_linear_grader(x,w,b)\n",
    "assert np.array_equal(preds.reshape(-1,),y.reshape(-1,))\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2795fac657fc44f7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Visualize Your Perceptron</h3>\n",
    "\n",
    "<p> You can use the following script to test your code and visualize your perceptron on linearly separable data in 2 dimensions. Your classifier should find a separating hyperplane on such data.   </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dfa3c34a4174dea1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# number of input vectors\n",
    "N = 100\n",
    "\n",
    "# generate random (linarly separable) data\n",
    "xs = np.random.rand(N, 2)*10-5\n",
    "\n",
    "# defining random hyperplane\n",
    "w0 = np.random.rand(2)\n",
    "b0 = np.random.rand()*2-1;\n",
    "\n",
    "# assigning labels +1, -1 labels depending on what side of the plane they lie on\n",
    "ys = np.sign(xs.dot(w0)+b0)\n",
    "\n",
    "# call perceptron to find w from data\n",
    "w,b = perceptron(xs.copy(),ys.copy())\n",
    "\n",
    "# test if all points are classified correctly\n",
    "assert (all(np.sign(ys*(xs.dot(w)+b))==1.0))  # yw'x should be +1.0 for every input\n",
    "print(\"Looks like you passed the Perceptron test! :o)\")\n",
    "\n",
    "# we can make a pretty visualization\n",
    "from helperfunctions import visboundary\n",
    "visboundary(w,b,xs,ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-50afb80324faafd6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def onclick(event):\n",
    "    global w,b,ldata,ax,line,xydata\n",
    "\n",
    "    pos=np.array([[event.xdata],[event.ydata]])\n",
    "    if event.key == 'shift': # add positive point\n",
    "        color='or'\n",
    "        label=1\n",
    "    else: # add negative point\n",
    "        color='ob'\n",
    "        label=-1    \n",
    "    ax.plot(pos[0],pos[1],color)\n",
    "    ldata.append(label);\n",
    "    xydata=np.vstack((xydata,pos.T))\n",
    "\n",
    "    # call Perceptron function\n",
    "    w,b=perceptron(xydata,np.array(ldata).flatten())\n",
    "\n",
    "    # draw decision boundary\n",
    "    q=-b/(w**2).sum() *w;\n",
    "    if line is None:\n",
    "        line, = ax.plot([q[0]-w[1],q[0]+w[1]],[q[1]+w[0],q[1]-w[0]],'b--')\n",
    "    else:\n",
    "        line.set_data([q[0]-w[1],q[0]+w[1]],[q[1]+w[0],q[1]-w[0]])\n",
    "    return \n",
    "        \n",
    "xydata=np.random.rand(0,2)\n",
    "ldata=[]\n",
    "w=np.zeros(2)\n",
    "b=np.zeros(1)\n",
    "line=None\n",
    "\n",
    "%matplotlib notebook\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "cid = fig.canvas.mpl_connect('button_press_event', onclick)\n",
    "plt.title('Use shift-click to add negative points.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-341e13b424f0e6bd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Part Three [Graded]</h3>\n",
    "\n",
    "<p>Implement <b><code>classify_linear</code></b> that applies the weight vector and bias to the input vector. (The bias is an optional parameter. If it is not passed in, assume it is zero.) Make sure that the predictions returned are either 1 or -1.</p> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-classify_linear",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def classify_linear(xs,w,b=None):\n",
    "    \"\"\"\n",
    "    function preds=classify_linear(xs,w,b)\n",
    "    \n",
    "    Make predictions with a linear classifier\n",
    "    Input:\n",
    "    xs : n input vectors of d dimensions (nxd) [could also be a single vector of d dimensions]\n",
    "    w : weight vector of dimensionality d\n",
    "    b : bias (scalar)\n",
    "    \n",
    "    Output:\n",
    "    preds: predictions (1xn)\n",
    "    \"\"\"    \n",
    "    w = w.flatten()    \n",
    "    predictions=np.zeros(xs.shape[0])\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    if b is None:\n",
    "        b = 0\n",
    "    predictions = np.sign(xs.dot(w) + b)\n",
    "    ### END SOLUTION\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-classify_linear_selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this self-test to check that your linear classifier correctly classifies the points in a linearly separable dataset\n",
    "\n",
    "def test_linear1():\n",
    "    xs = np.random.rand(50000,20)-0.5 # draw random data \n",
    "    w0 = np.random.rand(20)\n",
    "    b0 =- 0.1 # with bias -0.1\n",
    "    ys = classify_linear(xs,w0,b0)\n",
    "    uniquepredictions=np.unique(ys) # check if predictions are only -1 or 1\n",
    "    return set(uniquepredictions)==set([-1,1])\n",
    "\n",
    "def test_linear2():\n",
    "    xs = np.random.rand(1000,2)-0.5 # draw random data \n",
    "    w0 = np.array([0.5,-0.3]) # define a random hyperplane \n",
    "    b0 =- 0.1 # with bias -0.1\n",
    "    ys = np.sign(xs.dot(w0)+b0) # assign labels according to this hyperplane (so you know it is linearly separable)\n",
    "    return (all(np.sign(ys*classify_linear(xs,w0,b0))==1.0))  # the original hyperplane (w0,b0) should classify all correctly\n",
    "\n",
    "runtest(test_linear1, 'test_linear1')\n",
    "runtest(test_linear2, 'test_linear2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-test_linear1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell- worth 1 point\n",
    "# runs test_linear1\n",
    "### BEGIN HIDDEN TESTS\n",
    "xs = np.random.rand(1000,2)-0.5 # draw random data \n",
    "w0 = np.array([0.5,-0.3]) # define a random hyperplane \n",
    "b0 =- 0.1 # with bias -0.1\n",
    "ys = np.sign(xs.dot(w0)+b0) # assign labels according to this hyperplane (so you know it is linearly separable)\n",
    "assert (all(np.sign(ys*classify_linear(xs,w0,b0))==1.0))  # the original hyperplane (w0,b0) should classify all correctly\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-test_linear2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell- worth 1 point\n",
    "# runs test_linear2\n",
    "### BEGIN HIDDEN TESTS\n",
    "xs = np.random.rand(50000,20)-0.5 # draw random data \n",
    "w0 = np.random.rand(20)\n",
    "b0 =- 0.1 # with bias -0.1\n",
    "ys = classify_linear(xs,w0,b0)\n",
    "uniquepredictions=np.unique(ys) # check if predictions are only -1 or 1\n",
    "assert (set(uniquepredictions)==set([-1,1]))\n",
    "### END HIDDEN TESTS"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
