{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-da909ed2b39a0d76",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2>About this Project</h2>\n",
    "<p>In this project, you will build an email spam filter by implementing ridge regression loss and gradient descent. You will also have an opportunity to adjust the feature extraction and model training to improve your spam filter and test it against a hiddent dataset.</p>\n",
    "\n",
    "<h3>Evaluation</h3>\n",
    "\n",
    "<p>You are expected to write code where you see <em># YOUR CODE HERE</em> within the cells of this notebook. Upon submitting your work, the code you write at these designated positions will be assessed using an \"autograder\" that will run a series of tests on your code. You will receive instant feedback from the autograder that will identify issues with and errors in your code. Use this feedback to improve your code if you need to resubmit. Be sure not to change the names of any provided functions, classes, or variables within the existing code cells, as this will interfere with the autograder. Also, remember to execute all code cells, not just those you’ve edited, to ensure the code runs properly.</p>\n",
    "    \n",
    "<p>You can resubmit your work as many times as necessary before the submission deadline. If you experience difficulty or have questions about this exercise, use the discussion board to engage with your peers or seek assistance from the instructor.<p>\n",
    "\n",
    "<p><strong>This exercise must be successfully completed in order to receive credit for this course.</strong><p>\n",
    "\n",
    "<p>Before starting your work, please review <a href=\"https://s3.amazonaws.com/ecornell/global/eCornellPlagiarismPolicy.pdf\">eCornell's policy regarding plagiarism</a> (the presentation of someone else's work as your own without source credit).</p>\n",
    "\n",
    "<h3>Submit Code for Autograder/Instructor Feedback</h3>\n",
    "\n",
    "<p>Once you have completed your work on this notebook, you will submit your code for autograder/instructor review. Follow these steps:</p>\n",
    "\n",
    "<ol>\n",
    "<li>Save your notebook. Though the system should automatically save your progress, you should ensure the latest version of your work is saved before submitting. </li>\n",
    "  <li>In the blue menu bar along the top of the code exercise window, you’ll see a menu item called <strong>Education</strong>. In the <strong>Education</strong> menu, click <strong>Mark as Completed</strong> to submit your code for autograder/instructor review. This process will take a moment and a progress bar will show you the status of your submission.</li>\n",
    "\t<li>Once your work is marked as complete, the results of the autograder will automatically be presented in a new tab within the code exercise window. You can click on the assessment name in this feedback window to see more details regarding specific feedback/errors in your code submission.</li>\n",
    "  <li>The Jupyter notebook will always remain accessible in the first tabbed window of the exercise. To reattempt the work, you will first need to click <strong>Mark as Uncompleted</strong> in the <strong>Education</strong> menu and then proceed to make edits to the notebook. Once you are ready to resubmit, follow steps one through three. You can repeat this procedure as many times as necessary.</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e18e4b15769199ac",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2>Getting Started</h2>\n",
    "    \n",
    "<h3>Computing Derivatives</h3>\n",
    "\n",
    "<p>  In this project you will need the gradient for several loss functions with respect to the weight vector $\\mathbf{w}$:\n",
    "</p>\n",
    "\n",
    "<ol>\n",
    "    <li> Ridge Regression: ${\\cal L}(\\mathbf{w})=\\frac{1}{n}\\sum_{i=1}^n (\\mathbf{w}^\\top \\mathbf{x}_i-y_i)^2+\\lambda \\|\\mathbf{w}\\|_2^2$ </li>\n",
    "    <li> L2-regularized Logistic Regression (assumes $y_i\\in\\{+1,-1\\}$): ${\\cal L}(\\mathbf{w})=\\frac{1}{n} \\sum_{i=1}^n \\log(1+\\exp{(-y_i \\mathbf{w}^\\top \\mathbf{x}_i)}) +\\lambda \\|\\mathbf{w}\\|_2^2$ \n",
    "</ol>  \n",
    "\n",
    "<p> Note:    $\\|\\mathbf{w}\\|_2^2=\\mathbf{w}^\\top \\mathbf{w}$ and  $\\lambda$ are non-negative constants. You have either seen or derived the gradients of these two functions before. You can either refer back to the previous sections or derive the gradients again!</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-41b1914a70420412",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Python Initialization</h3> \n",
    "\n",
    "<p>Please run the following code to initialize your Python kernel. You should be running a version of Python 3.x.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.3 (default, Mar 27 2019, 16:54:48) \n",
      "[Clang 4.0.1 (tags/RELEASE_401/final)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.matlib import repmat\n",
    "import sys\n",
    "from scipy.io import loadmat\n",
    "import time\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-01c849716c016538",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2>Building an Email Spam Filter</h2>\n",
    "<p>You will now implement ridge loss and gradient descent. The following cells will walk you through steps and ask you to finish the necessary functions in a pre-defined order. Code cells requiring your input will display # YOUR CODE HERE and graded portions will be adequately labeled.<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1b391de438f78ea9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Load the Email Data</h3>\n",
    "<p>The function below loads in pre-processed email data, where emails are represented as one-hot vectors.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5000 input emails.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5000, 512)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize the email and hashes the symbols into a vector\n",
    "def extract_features_naive(path, B):\n",
    "    with open(path, 'r') as email_file:\n",
    "        # initialize all-zeros feature vector\n",
    "        v = np.zeros(B)\n",
    "        email = email_file.read()\n",
    "        # breaks for non-ascii characters\n",
    "        tokens = email.split()\n",
    "        for token in tokens:\n",
    "            v[hash(token) % B] = 1\n",
    "    return v\n",
    "\n",
    "def load_spam_data(extract_features, B=512, path=\"data_train/\"):\n",
    "    '''\n",
    "    INPUT:\n",
    "    extractfeatures : function to extract features\n",
    "    B               : dimensionality of feature space\n",
    "    path            : the path of folder to be processed\n",
    "    \n",
    "    OUTPUT:\n",
    "    X, Y\n",
    "    '''\n",
    "    \n",
    "    with open(os.path.join(path, 'index'), 'r') as f:\n",
    "        all_emails = [x for x in f.read().split('\\n') if ' ' in x]\n",
    "    \n",
    "    xs = np.zeros((len(all_emails), B))\n",
    "    ys = np.zeros(len(all_emails))\n",
    "    for i, line in enumerate(all_emails):\n",
    "        label, filename = line.split(' ')\n",
    "        # make labels +1 for \"spam\" and -1 for \"ham\"\n",
    "        ys[i] = (label == 'spam') * 2 - 1\n",
    "        xs[i, :] = extract_features(os.path.join(path, filename), B)\n",
    "    print('Loaded %d input emails.' % len(ys))\n",
    "    return xs, ys\n",
    "\n",
    "X,Y = load_spam_data(extract_features_naive)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7b6213a523d63a49",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Split Your Dataset</h3>\n",
    "\n",
    "<p>Now that you have loaded the dataset, it's time to split it into training and testing. To evaluate your algorithm you should split off 20% of this data into a testing set, leaving 80% as your training set.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training (xTr and yTr) and testing (xTv and yTv)\n",
    "n, d = X.shape\n",
    "# Allocate 80% of the data for training and 20% for testing\n",
    "cutoff = int(np.ceil(0.8 * n))\n",
    "# indices of training samples\n",
    "xTr = X[:cutoff,:]\n",
    "yTr = Y[:cutoff]\n",
    "# indices of testing samples\n",
    "xTv = X[cutoff:,:]\n",
    "yTv = Y[cutoff:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a1ed2fcdc58d8269",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Part 1 [Graded]</h3>\n",
    "\n",
    "<p>The code above should generate a training data set <code>xTr</code>, <code>yTr</code> and a validation set <code>xTv</code>, <code>yTv</code> for you. </p>\n",
    "\n",
    "<p>It is now time to implement your classifiers. You will always use the gradient descent algorithm, but with various loss functions. First implement the function <code>ridge</code>, which computes the ridge regression loss and gradient for a particular data set <code>xTr</code>, <code>yTr</code> and a weight vector <code>w</code>. Make sure you don't forget to incorporate your regularization constant $\\lambda$. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-36f54318a528d2a6",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def ridge(w,xTr,yTr,lmbda):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    w     : d   dimensional weight vector\n",
    "    xTr   : nxd dimensional matrix (each row is an input vector)\n",
    "    yTr   : n   dimensional vector (each entry is a label)\n",
    "    lmbda : regression constant (scalar)\n",
    "    \n",
    "    OUTPUTS:\n",
    "    loss     : the total loss obtained with w on xTr and yTr (scalar)\n",
    "    gradient : d dimensional gradient at w\n",
    "    \"\"\"\n",
    "    n, d = xTr.shape\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    preds = xTr @ (w)\n",
    "    diff = preds - yTr\n",
    "    loss = np.mean(diff ** 2) + lmbda * w.dot(w)\n",
    "    grad = 2.0 * (xTr.T) @(diff) /n  + 2*lmbda * w\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    return loss, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5b1c7d699aa7ae90",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Estimating Gradient Numerically</h3>\n",
    "<p>An  alternative to  deriving the gradient analytically is to estimate it numerically. This is very slow, but it is a convenient  way to check your code for correctness.  The following function  uses numerical differentiation to evaluate the correctness of ridge.  If your code is correct, the norm difference between the two should be very small (smaller than $10^{-8}$).</p> \n",
    "<p>\n",
    "Keep in mind that this only checks if the gradient corresponds to the loss, but not if the loss is correct. The function also plots an image of the gradient values (blue) and their estimates (red). If everything is correct, these two should be right on top of each other.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(fun,w,e):\n",
    "    # get dimensionality\n",
    "    d = len(w)\n",
    "    # initialize numerical derivative\n",
    "    dh = np.zeros(d)\n",
    "    # go through dimensions to compute directional derivative\n",
    "    for i in range(d):\n",
    "        # copy the weight vector\n",
    "        nw = w.copy()\n",
    "        # perturb dimension i\n",
    "        nw[i] += e\n",
    "        # compute loss\n",
    "        l1, temp = fun(nw)\n",
    "        # perturb dimension i again\n",
    "        nw[i] -= 2*e\n",
    "        # compute loss\n",
    "        l2, temp = fun(nw)\n",
    "        # the gradient is the slope of the loss\n",
    "        dh[i] = (l1 - l2) / (2*e)\n",
    "    return dh\n",
    "\n",
    "def check_grad(fun,w,e, plot=True):\n",
    "    # evaluate symbolic gradient from fun()\n",
    "    loss, dy = fun(w)\n",
    "    # estimate gradient numerically from fun()\n",
    "    dh = numerical_gradient(fun,w,e)\n",
    "    \n",
    "    ii = np.arange(len(dy))\n",
    "    if plot:\n",
    "        plt.figure(figsize=(10,6))\n",
    "        plt.scatter(ii, dh[ii], c='b', marker='o', s=60)\n",
    "        plt.scatter(ii, dy[ii], c='r', marker='.', s=50)\n",
    "        plt.xlabel('Dimension')\n",
    "        plt.ylabel('Gradient value')\n",
    "        plt.legend([\"numeric\",\"symbolic\"])\n",
    "    \n",
    "    # return the norm of the difference scaled by the norm of the sum\n",
    "    return np.linalg.norm(dh - dy) / np.linalg.norm(dh + dy)\n",
    "\n",
    "# set lmbda (λ) arbitrarily\n",
    "lmbda = 0.1\n",
    "# dimensionality of the input\n",
    "_, d = xTr.shape\n",
    "# evaluate loss on random vector\n",
    "w = np.random.rand(d)\n",
    "# the lambda function notation is an inline way to define a function with only a single argument.\n",
    "ratio = check_grad(lambda weight: ridge(weight,xTr,yTr,lmbda), w, 1e-05)\n",
    "print(\"The norm ratio is {}.\".format(ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-23331122645c7df8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Part Two [Graded]</h3>\n",
    "\n",
    "<p>Implement the function <code>grad_descent</code> which performs adaptive gradient descent. \n",
    "Make sure to include the tolerance variable to stop early if the norm of the gradient is less than the tolerance value (you can use the function <code>np.linalg.norm(x)</code>). When the norm of the gradient is tiny it means that you have arrived at a minimum.</p>\n",
    "\n",
    "<p>\n",
    "The first parameter of <code>grad_descent</code> is a function which takes a weight vector and returns loss and gradient.\n",
    "</p>                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-690521a733faab76",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def grad_descent(func,w,alpha,maxiter,delta=1e-02):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    func    : function to minimize\n",
    "              (loss, gradient = func(w))\n",
    "    w       : d dimensional initial weight vector \n",
    "    alpha   : initial gradient descent stepsize (scalar)\n",
    "    maxiter : maximum amount of iterations (scalar)\n",
    "    delta   : if norm(gradient)<delta, it quits (scalar)\n",
    "    \n",
    "    OUTPUTS:\n",
    "     \n",
    "    w      : d dimensional final weight vector\n",
    "    losses : vector containing loss at each iteration\n",
    "    \"\"\"\n",
    "    \n",
    "    losses = np.zeros(maxiter)\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    for i in range(maxiter):\n",
    "        losses[i], grad = func(w)\n",
    "        w -= alpha * grad\n",
    "        if np.linalg.norm(grad) < delta: \n",
    "            losses = losses[:i+1]\n",
    "            break\n",
    "    ### END SOLUTION\n",
    "    return w, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, d = xTr.shape\n",
    "lmbda = 5\n",
    "w, losses = grad_descent(lambda weight: ridge(weight, xTr, yTr, lmbda), np.random.rand(d), 0.005, 4000)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.semilogy(losses, c='r', linestyle='-')\n",
    "plt.xlabel(\"Gradient updates\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Gradient Descent Convergence\")\n",
    "print(\"Final Loss: %f\" % losses[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0bec2e6204d4eaf4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Part Three [Graded]</h3>\n",
    "\n",
    "<p>Now, write the function <code>linclassify</code>, which returns the predictions for a vector <code>w</code> and a data set <code>xTv</code>. (You can take it from a previous project.)</p>\n",
    "\n",
    "<p>After this, you can check your training and validation accuracy by running the cell below.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d0184a4067129bc9",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def linclassify(w,xTr):\n",
    "    ### BEGIN SOLUTION\n",
    "    return np.sign(xTr.dot(w))\n",
    "    ### END SOLUTION\n",
    "\n",
    "# evaluate training accuracy\n",
    "preds = linclassify(w,xTr)\n",
    "trainingacc = np.mean(preds==yTr)\n",
    "# evaluate testing accuracy\n",
    "preds = linclassify(w,xTv)\n",
    "validationacc = np.mean(preds==yTv)\n",
    "print(\"Training accuracy %2.2f%%\\nValidation accuracy %2.2f%%\\n\" % (trainingacc*100,validationacc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-45b68a6b60864f7f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Part Four [Graded]</h3>\n",
    "\n",
    "<p>Now implement the other loss function, <code>logistic</code>.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2a6283cd82a37648",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def logistic(w,xTr,yTr, lmbda):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    w     : d   dimensional weight vector\n",
    "    xTr   : nxd dimensional matrix (each row is an input vector)\n",
    "    yTr   : n   dimensional vector (each entry is a label)\n",
    "    \n",
    "    OUTPUTS:\n",
    "    loss     : the total loss obtained with w on xTr and yTr (scalar)\n",
    "    gradient : d dimensional gradient at w\n",
    "    \"\"\"\n",
    "    n, d = xTr.shape\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    e = np.exp((xTr@w).flatten() * - yTr)\n",
    "    loss = np.sum(np.log(1+e)) / n + lmbda * w.dot(w)\n",
    "    grad = np.dot(e * -yTr/(1+e), xTr) / n + 2*lmbda * w\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    return loss,grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-128295826d3a4770",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Test Your Loss Function</h3>\n",
    "\n",
    "<p>You can use the two cells below to test how well this loss function performs.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient sanity check\n",
    "_, d = xTr.shape\n",
    "w = np.random.rand(d)\n",
    "ratio = check_grad(lambda weight: logistic(weight,xTr,yTr, lmbda),w,1e-05)\n",
    "print(\"The norm ratio is {}\".format(ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmbda = 0.5\n",
    "w, losses = grad_descent(lambda weight: logistic(weight, xTr, yTr, lmbda), np.random.rand(d), 0.001, 4000)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.semilogy(losses, c='r', linestyle='-')\n",
    "plt.xlabel(\"Gradient updates\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Gradient Descent Convergence\")\n",
    "print(\"Final Loss: %f\" % losses[-1])\n",
    "\n",
    "# evaluate training accuracy\n",
    "preds = linclassify(w,xTr)\n",
    "trainingacc = np.mean(preds==yTr)\n",
    "# evaluate testing accuracy\n",
    "preds = linclassify(w,xTv)\n",
    "validationacc = np.mean(preds==yTv)\n",
    "print(\"Training accuracy %2.2f%%\\nValidation accuracy %2.2f%%\\n\" % (trainingacc*100,validationacc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-81ac7561e7957cd8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2>Challenge: Improve Your Spam Classifier <b>[Ungraded]</b></h2>\n",
    "\n",
    "<p>You can improve your classifier in two ways:</p>\n",
    "\n",
    "<ol>\n",
    "<li><b>Feature Extraction</b>:\n",
    "Modify the function <code>extract_features_comp()</code>.\n",
    "This function takes in a file path <code>path</code> and\n",
    "a feature dimension <code>B</code> and should output a feature vector of dimension <code>B</code>.\n",
    "The autograder will pass in a file path pointing to a file that contains an email,\n",
    "and set <code>B</code> = <code>feature_dimension</code>.\n",
    "We provide a naive feature extraction as an example.\n",
    "</li>\n",
    "<li><b>Model Training</b>:\n",
    "Modify the function <code>train_spam_filter_comp()</code>.\n",
    "This function takes in training data <code>xTr</code> and training labels <code>yTr</code> and\n",
    "should output a weight vector <code>w</code> for linear classification.\n",
    "We provide an initial implementation using gradient descent and ridge regression.\n",
    "</li>\n",
    "</ol>\n",
    "\n",
    "<p>Your model will be trained on the same training set above (loaded by <code>load_spam_data()</code>), but we will test its accuracy on a secret dataset of emails.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-398f8a9b2828278c",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "feature_dimension = 512\n",
    "def extract_features_comp(path, B=feature_dimension):\n",
    "    '''\n",
    "    INPUT:\n",
    "    path : file path of email\n",
    "    B    : dimensionality of feature vector\n",
    "    \n",
    "    OUTPUTS:\n",
    "    x    : B dimensional vector\n",
    "    '''\n",
    "    x = np.zeros(B)\n",
    "    with open(path, 'r') as email_file:\n",
    "        email = email_file.read()\n",
    "        # breaks for non-ascii characters\n",
    "        tokens = email.split()\n",
    "        for token in tokens:\n",
    "            x[hash(token) % B] = 1\n",
    "    ### BEGIN SOLUTION\n",
    "    ### END SOLUTION\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1b36d64b684f934e",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def train_spam_filter_comp(xTr, yTr):\n",
    "    '''\n",
    "    INPUT:\n",
    "    xTr : nxd dimensional matrix (each row is an input vector)\n",
    "    yTr : d   dimensional vector (each entry is a label)\n",
    "    \n",
    "    OUTPUTS:\n",
    "    w : d dimensional vector for linear classification\n",
    "    '''\n",
    "    n, d = xTr.shape\n",
    "    lmbda = 5\n",
    "    \n",
    "    w, losses = grad_descent(lambda weight: ridge(weight, xTr, yTr, lmbda), np.random.rand(d), 0.001, 1000)\n",
    "    ### BEGIN SOLUTION\n",
    "    ### END SOLUTION\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-01756223d2bd3974",
     "locked": true,
     "points": 0,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "### Instructor's solution. Do Not Modify\n",
    "def ridge_grader(w,xTr,yTr,lmbda):\n",
    "    n, d = xTr.shape\n",
    "    preds = xTr @ (w)\n",
    "    diff = preds - yTr\n",
    "    loss = np.mean(diff ** 2) + lmbda * w.dot(w)\n",
    "    grad = 2.0 * (xTr.T) @(diff) /n  + 2*lmbda * w\n",
    "    \n",
    "    return loss, grad\n",
    "\n",
    "def grad_descent_grader(func,w,alpha,maxiter,delta=1e-02):\n",
    "    losses = np.zeros(maxiter)\n",
    "    for i in range(maxiter):\n",
    "        losses[i], grad = func(w)\n",
    "        w -= alpha * grad\n",
    "        if np.linalg.norm(grad) < delta: \n",
    "            losses = losses[:i+1]\n",
    "            break\n",
    "    return w, losses\n",
    "\n",
    "def logistic_grader(w,xTr,yTr, lmbda):\n",
    "    n, d = xTr.shape\n",
    "    \n",
    "    e = np.exp((xTr@w).flatten() * - yTr)\n",
    "    loss = np.sum(np.log(1+e)) / n + lmbda * w.dot(w)\n",
    "    grad = np.dot(e * -yTr/(1+e), xTr) / n + 2*lmbda * w\n",
    "    \n",
    "    return loss,grad\n",
    "\n",
    "def linclassify_grader(w,xTr):\n",
    "    return np.sign(xTr.dot(w))\n",
    "\n",
    "def analyze_grader(kind,truth,preds):\n",
    "    truth = truth.flatten()\n",
    "    preds = preds.flatten()\n",
    "    if kind == 'abs':\n",
    "        # compute the absolute difference between truth and predictions\n",
    "        output = np.sum(np.abs(truth - preds)) / float(len(truth))\n",
    "    elif kind == 'acc':\n",
    "        if len(truth) == 0 and len(preds) == 0:\n",
    "            output = 0\n",
    "        else:\n",
    "            output = np.sum(truth == preds) / float(len(truth))\n",
    "    return output\n",
    "\n",
    "N = 50\n",
    "D = 5\n",
    "X = np.vstack((np.random.randn(50, 5), np.random.randn(50, 5) + 2))\n",
    "Y = np.hstack((np.ones(50), -np.ones(50)))\n",
    "XUnit = np.array([[-1,1],[-1,0],[0,-1],[-1,2],[1,-2],[1,-1],[1,0],[0,1],[1,-2],[-1,2]])\n",
    "YUnit = np.hstack((np.ones(5), -np.ones(5)))\n",
    "### END HIDDEN TESTS\n",
    "#Autograder cell- do not edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b33b9ef35cc881c3",
     "locked": true,
     "points": 4,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "# naive unit testing using simply w, x and y data\n",
    "def ridge_test0():\n",
    "    w = np.ones(2)\n",
    "    [lss1,grd1] = ridge(w, XUnit, YUnit, 0.05)\n",
    "    [lss2,grd2] = ridge_grader(w, XUnit, YUnit, 0.05)\n",
    "    return (np.linalg.norm(lss1 - lss2) < 1e-5), (np.linalg.norm(grd1 - grd2) < 1e-5)\n",
    "\n",
    "# Check whether gradient is consistent with loss\n",
    "def ridge_test1():\n",
    "    w = np.random.rand(D)\n",
    "    ratio = check_grad(lambda weight: ridge(weight,X,Y,0.3),w,1e-05, False)\n",
    "    return (ratio < 1e-8)\n",
    "\n",
    "# Check whether loss is correct\n",
    "def ridge_test2():\n",
    "    w = np.random.rand(D)\n",
    "    [lss1, grd1] = ridge(w, X, Y, 0.3)\n",
    "    [lss2, grd2] = ridge_grader(w, X, Y, 0.3)\n",
    "    return (np.linalg.norm(lss1 - lss2) < 1e-5)\n",
    "\n",
    "# Check whether gradient is correct\n",
    "def ridge_test3():\n",
    "    w = np.random.rand(D)\n",
    "    [lss1, grd1] = ridge(w, X, Y, 0.3)\n",
    "    [lss2, grd2] = ridge_grader(w, X, Y, 0.3)\n",
    "    return (np.linalg.norm(grd1 - grd2) < 1e-5)\n",
    "\n",
    "\n",
    "ridge_unit_pass_lss, ridge_unit_pass_grd = ridge_test0()\n",
    "assert ridge_unit_pass_lss, \"Ridge Unit Test - loss seems incorrect for simple unit test data\"\n",
    "assert ridge_unit_pass_grd, \"Ridge Unit Test - gradient seems incorrect for simple unit test data\"\n",
    "assert ridge_test1(), \"Ridge: Analytical and numerical gradients don't match\"\n",
    "assert ridge_test2(), \"Ridge: Loss seems incorrect\"\n",
    "assert ridge_test3(), \"Ridge: Gradient seems incorrect\"\n",
    "### END HIDDEN TESTS\n",
    "#Autograder cell- do not edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f6e53a1c7fc16916",
     "locked": true,
     "points": 4,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "# naive unit testing using simply w, x and y data\n",
    "def logistic_test0():\n",
    "    w = np.ones(2)\n",
    "    [lss1,grd1] = logistic(w, XUnit, YUnit, 0.3)\n",
    "    [lss2,grd2] = logistic_grader(w, XUnit, YUnit, 0.3)\n",
    "    return (np.linalg.norm(lss1 - lss2) < 1e-5), (np.linalg.norm(grd1 - grd2) < 1e-5)\n",
    "\n",
    "# Check whether gradient is consistent with loss\n",
    "def logistic_test1():\n",
    "    w = np.random.rand(D)\n",
    "    ratio = check_grad(lambda weight: logistic(weight,X,Y, 0.3),w,1e-05, False)\n",
    "    return (ratio < 1e-8)\n",
    "\n",
    "# Check whether loss is correct\n",
    "def logistic_test2():\n",
    "    w = np.random.rand(D)\n",
    "    [lss1, grd1] = logistic(w, X, Y, 0.3)\n",
    "    [lss2, grd2] = logistic_grader(w, X, Y, 0.3)\n",
    "    return (np.linalg.norm(lss1 - lss2) < 1e-5)\n",
    "\n",
    "# Check whether gradient is correct\n",
    "def logistic_test3():\n",
    "    w = np.random.rand(D)\n",
    "    [lss1, grd1] = logistic(w, X, Y, 0.3)\n",
    "    [lss2, grd2] = logistic_grader(w, X, Y, 0.3)\n",
    "    return (np.linalg.norm(grd1 - grd2) < 1e-5)\n",
    "\n",
    "logistic_unit_pass_lss, logistic_unit_pass_grd = logistic_test0()\n",
    "assert logistic_unit_pass_lss, \"Logistic Unit Test - loss seems incorrect for simple unit test data\"\n",
    "assert logistic_unit_pass_grd, \"Logistic Unit Test - gradient seems incorrect for simple unit test data\"\n",
    "assert logistic_test1(), \"Logistic: Analytical and numerical gradients don't match\"\n",
    "assert logistic_test2(), \"Logistic: Loss seems incorrect\"\n",
    "assert logistic_test3(), \"Logistic: Gradient seems incorrect\"\n",
    "### END HIDDEN TESTS\n",
    "#Autograder cell- do not edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-dd6fa770b6c73d8d",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "def linclassify_test():\n",
    "    w = np.random.rand(D)\n",
    "    xtoy = np.random.rand(N,D)\n",
    "    pred1 = linclassify(w, xtoy)\n",
    "    pred2 = linclassify_grader(w, xtoy)\n",
    "    return (np.linalg.norm(pred1 - pred2) < 1e-10)\n",
    "\n",
    "assert linclassify_test(), \"Linclassify Test - linear classification seems incorrect\"\n",
    "### END HIDDEN TESTS\n",
    "#Autograder cell- do not edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-0394e04221e28891",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "# Check whether the first step of adagrad is correct\n",
    "def grad_descent_test1():\n",
    "    w = np.random.rand(D)\n",
    "    w2 = np.copy(w)\n",
    "    eps = 1e-06\n",
    "    weight1, _ = grad_descent(lambda weight: ridge(weight,X,Y,0.3),w,0.001,1,eps)\n",
    "    weight2, _ = grad_descent_grader(lambda weight: ridge_grader(weight,X,Y,0.3),w2,0.001,1,eps)\n",
    "    return (np.linalg.norm(weight1 - weight2) < 1e-5)\n",
    "\n",
    "# Check the convergence of adagrad\n",
    "def grad_descent_test2():\n",
    "    w = np.random.rand(D)\n",
    "    w2 = np.copy(w)\n",
    "    eps = 1e-06\n",
    "    weight1, _ = grad_descent(lambda weight: ridge(weight,X,Y,0.3),w,0.001,100,eps)\n",
    "    weight2, _ = grad_descent_grader(lambda weight: ridge_grader(weight,X,Y,0.3),w2,0.001,100,eps)\n",
    "    return (np.linalg.norm(weight1 - weight2) < 1e-5)\n",
    "\n",
    "# Check various steps of adagrad\n",
    "def grad_descent_test3():\n",
    "    w = np.random.rand(D)\n",
    "    w2 = np.copy(w)\n",
    "    eps = 1e-06\n",
    "    for i in range(25,101,25):\n",
    "        weight1, _ = grad_descent(lambda weight: ridge(weight,X,Y,0.3),w,0.001,i,eps)\n",
    "        weight2, _ = grad_descent(lambda weight: ridge_grader(weight,X,Y,0.3),w2,0.001,i,eps)\n",
    "        if not (np.linalg.norm(weight1 - weight2) < 1e-5): return False\n",
    "    return True\n",
    "\n",
    "assert grad_descent_test1(), \"Gradient Descent Test 1 - single step using ridge seems incorrect\"\n",
    "assert grad_descent_test2(), \"Gradient Descent Test 2 - many steps using ridge seems incorrect\"\n",
    "assert grad_descent_test3(), \"Gradient Descent Test 3 - checking various steps using ridge seems incorrect\"\n",
    "### END HIDDEN TESTS\n",
    "#Autograder cell- do not edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-21db9dedb682172d",
     "locked": true,
     "points": 0,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "def competition_test():\n",
    "    xTr,yTr = load_spam_data(extract_features_comp, feature_dimension, 'data_train')\n",
    "    weight = train_spam_filter_comp(xTr, yTr)\n",
    "    xTe,yTe = load_spam_data(extract_features_comp, feature_dimension, 'data_test')\n",
    "    preds = linclassify_grader(weight, xTe)\n",
    "    pos_ind = (yTe == 1)\n",
    "    neg_ind = (yTe == -1)\n",
    "    pos_acc = np.mean(yTe[pos_ind] == preds[pos_ind])\n",
    "    neg_acc = np.mean(yTe[neg_ind] == preds[neg_ind])\n",
    "    test_accuracy = 0.5*pos_acc + 0.5*neg_acc\n",
    "    return analyze_grader(\"acc\", yTr, linclassify_grader(weight, xTr)), test_accuracy\n",
    "\n",
    "training_acc, test_acc = competition_test()\n",
    "print(\"Your features achieved training accuracy: {:.2f}% and test accuracy: {:.2f}%\".format(training_acc*100, test_acc*100))\n",
    "### END HIDDEN TESTS\n",
    "#Autograder cell- do not edit"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
