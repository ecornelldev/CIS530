{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this activity, we will find the gradient of the logistic loss function and implement gradient descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pylab import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll simulate data points in 2D with binary (0/1) labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "n_samples = 500\n",
    "\n",
    "class_one = np.random.multivariate_normal([5, 10], [[1, .25],[.25, 1]], n_samples)\n",
    "class_one_labels = np.zeros(n_samples)\n",
    "\n",
    "class_two = np.random.multivariate_normal([0, 5], [[1, .25],[.25, 1]], n_samples)\n",
    "class_two_labels = np.ones(n_samples)\n",
    "\n",
    "X = np.vstack((class_one, class_two))\n",
    "y = np.hstack((class_one_labels, class_two_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(X[:, 0], X[:, 1],\n",
    "            c = y, alpha = .6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In logistic regression, we use gradient descent to solve for the weight vector that maximizes the log likelihood.  Alternatively, we are looking for the weight vector that _minimizes_ the negative log likelihood (refer back to Module 2 if you need a review of this derivation!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in the code below to finish implementing the MLE solution for logistic regression (if you need guidance, refer back to the Logistic Regression MLE page in Module 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(X, y, w):\n",
    "    f_x = np.dot(X, w)\n",
    "    # YOUR CODE HERE\n",
    "    return log_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, compute the gradient by taking the derivative of the above expression with respect to the weight vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X, y, w):\n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in the function below to finish implementing the sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    # YOUR CODE HERE\n",
    "    return sgmd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that logistic regression makes predictions using the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_pred(x, w):\n",
    "    return sigmoid(np.dot(x, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in the code below to implement the weight update step of gradient descent.  Hint: use the `y_pred` function above.\n",
    "\n",
    "Recall that in gradient descent, the weights are updated by the product of the learning rate alpha, and the gradient of the loss function with respect to the weight vector.  Pay close attention to the signs - are we adding or subtracting at each weight update step?  Why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X, y, max_iter, alpha):\n",
    "    n, d = X.shape\n",
    "    weights = np.zeros(d)\n",
    "    losses = np.zeros(max_iter)    \n",
    "    \n",
    "#     for step in range(max_iter):\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we'll check your code by plotting the values of the negative log likelihood - should these values increase or decrease as the number of iterations grows?  Do your values move in the right direction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 30000\n",
    "loss_func_values = np.zeros(max_iter)  # UPDATE ME\n",
    "plt.plot(loss_func_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
