{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bb88f0a35b56d0ae",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2>About this Exercise</h2>\n",
    "<p>In this exercise, you will find the gradient of the logistic loss function and implement gradient descent.</p>\n",
    "\n",
    "<h3>Evaluation</h3>\n",
    "\n",
    "<p><strong>You must complete this exercise in order to unlock the final project in this module. Your score on this assignment will not be included in the final grade calculation.</strong><p>\n",
    "\n",
    "<p>You are expected to write code where you see <em># YOUR CODE HERE</em> within the cells of this notebook. Not all cells will be graded; code input cells followed by cells marked with <em>#Autograder test cell</em> will be graded. Upon submitting your work, the code you write at these designated positions will be assessed using an \"autograder\" that will run all test cells to assess your code. You will receive feedback from the autograder that will identify any errors in your code. Use this feedback to improve your code if you need to resubmit. Be sure not to change the names of any provided functions, classes, or variables within the existing code cells, as this will interfere with the autograder. Also, remember to execute all code cells sequentially, not just those you’ve edited, to ensure your code runs properly.</p>\n",
    "    \n",
    "<p>You can resubmit your work as many times as necessary before the submission deadline. If you experience difficulty or have questions about this exercise, use the Q&A discussion board to engage with your peers or seek assistance from the instructor.<p>\n",
    "\n",
    "<p>Before starting your work, please review <a href=\"https://s3.amazonaws.com/ecornell/global/eCornellPlagiarismPolicy.pdf\">eCornell's policy regarding plagiarism</a> (the presentation of someone else's work as your own without source credit).</p>\n",
    "\n",
    "<h3>Submit Code for Autograder Feedback</h3>\n",
    "\n",
    "<p>Once you have completed your work on this notebook, you will submit your code for autograder review. Follow these steps:</p>\n",
    "\n",
    "<ol>\n",
    "  <li><strong>Save your notebook.</strong></li>\n",
    "  <li><strong>Mark as Completed —</strong> In the blue menu bar along the top of this code exercise window, you’ll see a menu item called <strong>Education</strong>. In the <strong>Education</strong> menu, click <strong>Mark as Completed</strong> to submit your code for autograder/instructor review. This process will take a moment and a progress bar will show you the status of your submission.</li>\n",
    "\t<li><strong>Review your results —</strong> Once your work is marked as complete, the results of the autograder will automatically be presented in a new tab within the code exercise window. You can click on the assessment name in this feedback window to see more details regarding specific feedback/errors in your code submission.</li>\n",
    "  <li><strong>Repeat, if necessary —</strong> The Jupyter notebook will always remain accessible in the first tabbed window of the exercise. To reattempt the work, you will first need to click <strong>Mark as Uncompleted</strong> in the <strong>Education</strong> menu and then proceed to make edits to the notebook. Once you are ready to resubmit, follow steps one through three. You can repeat this procedure as many times as necessary.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ae461e465b7f800e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2>Getting Started</h2>\n",
    "<h3>Python Initialization</h3> \n",
    "\n",
    "Please run the following code to initialize your Python kernel. You should be running a version of Python 3.x. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a801a09f2b1fc82b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pylab import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "from helper import *\n",
    "\n",
    "print('You\\'re running python %s' % sys.version.split(' ')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5e40cd81b8f0dd4a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Next, let's simulate some 2D data with binary (0/1) labels.  You'll be generating this data from non-overlapping multivariate normal distributions that should be very easily separable for logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-48afa45c97dc36fb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "n_samples = 500\n",
    "\n",
    "class_one = np.random.multivariate_normal([5, 10], [[1, .25],[.25, 1]], n_samples)\n",
    "class_one_labels = np.zeros(n_samples)\n",
    "\n",
    "class_two = np.random.multivariate_normal([0, 5], [[1, .25],[.25, 1]], n_samples)\n",
    "class_two_labels = np.ones(n_samples)\n",
    "\n",
    "features = np.vstack((class_one, class_two))\n",
    "labels = np.hstack((class_one_labels, class_two_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a262f12a83fdbc86",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's see what what our feature arrays look like. \n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ff1f0296d7835ca5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# We can visualize these data distributions\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.scatter(features[:, 0], features[:, 1],\n",
    "            c = labels, alpha = .6);\n",
    "\n",
    "plt.title(\"Binary labeled data in 2D\", size=15);\n",
    "plt.xlabel(\"Feature 1\", size=13);\n",
    "plt.ylabel(\"Feature 2\", size=13);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ff5ae8402f789a19",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In logistic regression, we use gradient ascent to solve for the weight vector that maximizes the (log) likelihood of observing the data.  (Equivalently, we can use gradient _descent_ to solve for the weight vector that _minimizes_ the _negative_ log likelihood - refer to Module 3 if you need a review of this derivation!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a2e7bdf679a2c694",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2>Exercises</h2>\n",
    "\n",
    "<h3>Exercise 1: Logistic Regression Using MLE</h3>\n",
    "\n",
    "<p>Fill in the code below to finish implementing the MLE solution for logistic regression (if you need guidance, refer back to the Logistic Regression MLE read page in Module 3. Code cells requiring your input will display # YOUR CODE HERE and graded portions will be adequately labeled.<br></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(X, y, w):\n",
    "    h_x = np.dot(X, w)\n",
    "    ### BEGIN SOLUTION\n",
    "    log_likelihood = np.sum(y*h_x - np.log(1 + np.exp(h_x)))\n",
    "    ### END SOLUTION\n",
    "    return log_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-882d2b4f514b7ad6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Recall that logistic regression makes predictions using this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-543c90890999c115",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def y_pred(X, w):\n",
    "    return sigmoid(np.dot(X, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fe493512c6a3ed9b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Write the sigmoid function to be used in ` y_pred()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-sigmoid",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    ### BEGIN SOLUTION\n",
    "    sgmd = 1 / (1 + np.exp(-z))\n",
    "    ### END SOLUTION\n",
    "    return sgmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-sigmoid_selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def test_sigmoid1():\n",
    "    n = 10\n",
    "    h = np.random.rand(n)\n",
    "    sgmd1 = sigmoid(h)\n",
    "    sgmd2 = sigmoid_grader(h)\n",
    "    return (np.linalg.norm(sgmd1 - sgmd2) < 1e-5)\n",
    "\n",
    "def test_sigmoid2():\n",
    "    x = np.random.rand(1)\n",
    "    sgmd1 = sigmoid(x)\n",
    "    sgmd2 = sigmoid_grader(x)\n",
    "    return (np.linalg.norm(sgmd1 - sgmd2) < 1e-5)\n",
    "\n",
    "runtest(test_sigmoid1, 'test_sigmoid1')\n",
    "runtest(test_sigmoid2, 'test_sigmoid2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-test_sigmoid1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_sigmoid1\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "n = 10\n",
    "h = np.random.rand(n)\n",
    "sgmd1 = sigmoid(h)\n",
    "sgmd2 = sigmoid_grader(h)\n",
    "assert (np.linalg.norm(sgmd1 - sgmd2) < 1e-5)\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-test_sigmoid2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_sigmoid2\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "x = np.random.rand(1)\n",
    "sgmd1 = sigmoid(x)\n",
    "sgmd2 = sigmoid_grader(x)\n",
    "assert (np.linalg.norm(sgmd1 - sgmd2) < 1e-5)\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-af8fd1a81a6e6b97",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Exercise 2: Compute Gradient</h3>\n",
    "\n",
    "Now, compute the gradient by taking the derivative of the log likelihood expression with respect to the weight vector.\n",
    "\n",
    "Hint: you'll use the expressions `h_x` and `y_hat` that are already implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X, y, w):\n",
    "    h_x = np.dot(X, w)\n",
    "    y_hat = sigmoid(h_x)\n",
    "    ### BEGIN SOLUTION\n",
    "    grad = np.dot(X.T, (y - y_hat))\n",
    "    ### END SOLUTION\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-692e12ddff9a0aaa",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Exercise 3: Weight Update of Gradient Ascent</h3>\n",
    "    \n",
    "Write code below to implement the weight update of gradient ascent.  Hint: use the `y_pred` function above.\n",
    "\n",
    "Recall that in gradient ascent, the weights are updated by the product of the learning rate alpha, and the gradient of the loss function with respect to the weight vector (which you implemented in the function above).  Pay close attention to the signs - are we adding or subtracting at each weight update step?  Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-logistic_regression",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def logistic_regression(X, y, max_iter, alpha):\n",
    "    _, d = X.shape\n",
    "    weights = np.zeros(d)\n",
    "    losses = np.zeros(max_iter)    \n",
    "    \n",
    "    for step in range(max_iter):\n",
    "        ### BEGIN SOLUTION\n",
    "        grad = gradient(X, y, weights)\n",
    "        weights += alpha * grad\n",
    "        losses[step] = log_likelihood(X, y, weights)\n",
    "        ### END SOLUTION\n",
    "    return weights, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_logistic_regression1 ():\n",
    "\n",
    "    XUnit = np.array([[-1,1],[-1,0],[0,-1],[-1,2],[1,-2],[1,-1],[1,0],[0,1],[1,-2],[-1,2]])\n",
    "    YUnit = np.hstack((np.ones(5), -np.ones(5)))\n",
    "\n",
    "    [lss1,grd1] = logistic_regression(XUnit, YUnit, 30000, 5e-5)\n",
    "    [lss2,grd2] = logistic_regression_grader(XUnit, YUnit, 30000, 5e-5)\n",
    "    return(np.linalg.norm(lss1 - lss2) < 1e-5)\n",
    "\n",
    "def test_logistic_regression2 ():\n",
    "\n",
    "    X = np.vstack((np.random.randn(50, 5), np.random.randn(50, 5) + 2))\n",
    "    Y = np.hstack((np.ones(50), -np.ones(50)))\n",
    "    max_iter = 300\n",
    "    alpha = 1e-5\n",
    "    weights1, _ = logistic_regression(X, Y, max_iter, alpha)\n",
    "    weights2, _ = logistic_regression_grader(X, Y, max_iter, alpha)\n",
    "    return (np.linalg.norm(weights1 - weights2) < 1e-5)\n",
    "\n",
    "runtest(test_logistic_regression1, 'test_logistic_regression1')\n",
    "runtest(test_logistic_regression2, 'test_logistic_regression2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-test_logistic_regression1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_logistic_regression1\n",
    "### BEGIN HIDDEN TESTS\n",
    "XUnit = np.array([[-1,1],[-1,0],[0,-1],[-1,2],[1,-2],[1,-1],[1,0],[0,1],[1,-2],[-1,2]])\n",
    "YUnit = np.hstack((np.ones(5), -np.ones(5)))\n",
    "\n",
    "[lss1,grd1] = logistic_regression(XUnit, YUnit, 30000, 5e-5)\n",
    "[lss2,grd2] = logistic_regression_grader(XUnit, YUnit, 30000, 5e-5)\n",
    "assert(np.linalg.norm(lss1 - lss2) < 1e-5)\n",
    "\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-test_logistic_regression_2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_logistic_regression2\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "X = np.vstack((np.random.randn(50, 5), np.random.randn(50, 5) + 2))\n",
    "Y = np.hstack((np.ones(50), -np.ones(50)))\n",
    "max_iter = 300\n",
    "alpha = 1e-5\n",
    "weights1, _ = logistic_regression(X, Y, max_iter, alpha)\n",
    "weights2, _ = logistic_regression_grader(X, Y, max_iter, alpha)\n",
    "assert (np.linalg.norm(weights1 - weights2) < 1e-5)\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f874275c305249ec",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Now run your implementation on the binary classification data from the top of the notebook.  Check your code by plotting the values of the negative log likelihood - should these values increase or decrease as the number of iterations grows?  Do your values move in the right direction?\n",
    "\n",
    "You can tune `max_iter` and `alpha` to see how they affect convergence!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a8f0e2256f9dc7a7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "max_iter = 30000\n",
    "alpha = 1e-5\n",
    "final_weights, losses = logistic_regression(features, labels, max_iter, alpha)\n",
    "plt.figure(figsize=(9, 6));\n",
    "plt.plot(losses);\n",
    "plt.title(\"Loss vs. iteration\", size=15);\n",
    "plt.xlabel(\"Num iteration\", size=13);\n",
    "plt.ylabel(\"Loss value\", size=13);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-334b2b1ff6a97606",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Below, we'll take the final weights from the logistic solver and predict labels for the entire dataset.  By plotting the results, we can get a sense of where the linear decision boundary lies.  What do you notice?  What could be changed to further improve the accuracy of the classifier? (_Hint: take a look at the second video in Module 1._)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d3c49e5cdaebdd3d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "scores = y_pred(features, final_weights)\n",
    "pred_labels = np.round(scores)\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.scatter(features[:, 0], features[:, 1],\n",
    "            c = pred_labels, alpha = .6);\n",
    "plt.title(\"Predicted labels\", size=15);\n",
    "plt.xlabel(\"Feature 1\", size=13);\n",
    "plt.ylabel(\"Feature 2\", size=13);"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
