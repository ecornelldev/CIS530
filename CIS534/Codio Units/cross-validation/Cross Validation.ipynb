{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ee3457876c038971",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2>About this Project</h2>\n",
    "<p>In this project, you will implement cross validation to pick the best depth (hyperparameter) for a regression tree, again using the ION data set.</p>\n",
    "\n",
    "<h3>Evaluation</h3>\n",
    "\n",
    "<p><strong>This project must be successfully completed and submitted in order to receive credit for this course. Your score on this project will be included in your final grade calculation.</strong><p>\n",
    "    \n",
    "<p>You are expected to write code where you see <em># YOUR CODE HERE</em> within the cells of this notebook. Not all cells will be graded; code input cells followed by cells marked with <em>#Autograder test cell</em> will be graded. Upon submitting your work, the code you write at these designated positions will be assessed using an \"autograder\" that will run all test cells to assess your code. You will receive feedback from the autograder that will identify any errors in your code. Use this feedback to improve your code if you need to resubmit. Be sure not to change the names of any provided functions, classes, or variables within the existing code cells, as this will interfere with the autograder. Also, remember to execute all code cells sequentially, not just those you’ve edited, to ensure your code runs properly.</p>\n",
    "    \n",
    "<p>You can resubmit your work as many times as necessary before the submission deadline. If you experience difficulty or have questions about this exercise, use the Q&A discussion board to engage with your peers or seek assistance from the instructor.<p>\n",
    "\n",
    "<p>Before starting your work, please review <a href=\"https://s3.amazonaws.com/ecornell/global/eCornellPlagiarismPolicy.pdf\">eCornell's policy regarding plagiarism</a> (the presentation of someone else's work as your own without source credit).</p>\n",
    "\n",
    "<h3>Submit Code for Autograder Feedback</h3>\n",
    "\n",
    "<p>Once you have completed your work on this notebook, you will submit your code for autograder review. Follow these steps:</p>\n",
    "\n",
    "<ol>\n",
    "  <li><strong>Save your notebook.</strong></li>\n",
    "  <li><strong>Mark as Completed —</strong> In the blue menu bar along the top of this code exercise window, you’ll see a menu item called <strong>Education</strong>. In the <strong>Education</strong> menu, click <strong>Mark as Completed</strong> to submit your code for autograder/instructor review. This process will take a moment and a progress bar will show you the status of your submission.</li>\n",
    "\t<li><strong>Review your results —</strong> Once your work is marked as complete, the results of the autograder will automatically be presented in a new tab within the code exercise window. You can click on the assessment name in this feedback window to see more details regarding specific feedback/errors in your code submission.</li>\n",
    "  <li><strong>Repeat, if necessary —</strong> The Jupyter notebook will always remain accessible in the first tabbed window of the exercise. To reattempt the work, you will first need to click <strong>Mark as Uncompleted</strong> in the <strong>Education</strong> menu and then proceed to make edits to the notebook. Once you are ready to resubmit, follow steps one through three. You can repeat this procedure as many times as necessary.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9eed63d0f1650fcc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Get Started\n",
    "\n",
    "<p>Let's import a few packages that you will need. You will work with the <a href=\"https://archive.ics.uci.edu/ml/datasets/Ionosphere\">ION</a> dataset for this project.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-39ac94e6e8bdc336",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pylab import *\n",
    "from numpy.matlib import repmat\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "import time\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "sys.path.append('/home/codio/workspace/.guides/hf')\n",
    "from helper import *\n",
    "\n",
    "print('You\\'re running python %s' % sys.version.split(' ')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e09c08076a310a72",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "data = loadmat(\"ion.mat\")\n",
    "xTr  = data['xTr'].T\n",
    "yTr  = data['yTr'].flatten()\n",
    "xTe  = data['xTe'].T\n",
    "yTe  = data['yTe'].flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7d95b26aeed38b8e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We also developed a regression tree classifier in ``helper.py``. The following code cell shows you how to instantiate a  regression tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-778baeb6ff2b94be",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a regression tree with no restriction on its depth\n",
    "# if you want to create a tree of depth k\n",
    "# then call h.RegressionTree(depth=k)\n",
    "tree = RegressionTree(depth=np.inf)\n",
    "\n",
    "# To fit/train the regression tree\n",
    "tree.fit(xTr, yTr)\n",
    "\n",
    "# To use the trained regression tree to make prediction\n",
    "pred = tree.predict(xTr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-37e18ecd068c189d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We have also created a square loss function that takes in the prediction <code>pred</code> and ground truth <code>truth</code> and returns the average square loss between prediction and ground truth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d620b929b0f3b5ed",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def square_loss(pred, truth):\n",
    "    return np.mean((pred - truth)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a7b5b10efd4cca7f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now, look at the performance of your tree on both the training set and test set using the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bee4f89dde382e5f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print('Training Loss: {:.4f}'.format(square_loss(tree.predict(xTr), yTr)))\n",
    "print('Test Loss: {:.4f}'.format(square_loss(tree.predict(xTe), yTe)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5c969869a177745f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "As you can see, your tree achives zero training loss on the training set but not zero test loss. Clearly, the model is overfitting! To reduce overfitting, you need to control the depth of the tree. One way to pick the optimal depth is to do kFold Cross Validation. To do so, you will first implement <code>grid_search</code>, which finds the best depths given a training set and validation set. Then you will implement <code>generate_kFold</code>, which generates the split for kFold cross validation. Finally, you will combine the two functions to implement <code>cross_validation</code>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-88634e1d5f07d0ab",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Implement Cross Validation\n",
    "\n",
    "### Part One [Graded]\n",
    "Implement the function <code>grid_search</code>, which takes in a training set <code>xTr, yTr</code>, a validation set <code>xVal, yVal</code> and a list of tree depth candidates <code>depths</code>. Your job here is to fit a regression tree for each depth candidate on the training set <code>xTr, yTr</code>, evaluate the fitted tree on the validation set <code>xVal, yVal</code> and then pick the candidate that yields the lowest loss for the validation set. Note: in the event of a tie, return the smallest depth candidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-grid_search",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def grid_search(xTr, yTr, xVal, yVal, depths):\n",
    "    '''\n",
    "    Input:\n",
    "        xTr: nxd matrix\n",
    "        yTr: n vector\n",
    "        xVal: mxd matrix\n",
    "        yVal: m vector\n",
    "        depths: a list of len k\n",
    "    Return:\n",
    "        best_depth: the depth that yields that lowest loss on the validation set\n",
    "        training losses: a list of len k. the i-th entry corresponds to the the training loss\n",
    "                the tree of depths[i]\n",
    "        validation_losses: a list of len k. the i-th entry corresponds to the the validation loss\n",
    "                the tree of depths[i]\n",
    "    '''\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "    best_depth = None\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    for i in depths:\n",
    "        tree = RegressionTree(i)\n",
    "        tree.fit(xTr, yTr)\n",
    "        \n",
    "        training_loss = square_loss(tree.predict(xTr), yTr)\n",
    "        validation_loss = square_loss(tree.predict(xVal), yVal)\n",
    "        training_losses.append(training_loss)\n",
    "        validation_losses.append(validation_loss)\n",
    "    \n",
    "    best_depth = depths[np.argmin(validation_losses)]\n",
    "    ### END SOLUTION\n",
    "    return best_depth, training_losses, validation_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-gridsearch_selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# The following tests check that your implementation of grid search returns the correct number of training and validation loss values and the correct best depth\n",
    "\n",
    "depths = [1,2,3,4,5]\n",
    "k = len(depths)\n",
    "best_depth, training_losses, validation_losses = grid_search(xTr, yTr, xTe, yTe, depths)\n",
    "best_depth_grader, training_losses_grader, validation_losses_grader = grid_search_grader(xTr, yTr, xTe, yTe, depths)\n",
    "\n",
    "# Check the length of the training loss\n",
    "def grid_search_test1():\n",
    "    return (len(training_losses) == k) \n",
    "\n",
    "# Check the length of the validation loss\n",
    "def grid_search_test2():\n",
    "    return (len(validation_losses) == k)\n",
    "\n",
    "# Check the argmin\n",
    "def grid_search_test3():\n",
    "    return (best_depth == depths[np.argmin(validation_losses)])\n",
    "\n",
    "def grid_search_test4():\n",
    "    return (best_depth == best_depth_grader)\n",
    "\n",
    "def grid_search_test5():\n",
    "    return np.linalg.norm(np.array(training_losses) - np.array(training_losses_grader)) < 1e-7\n",
    "\n",
    "def grid_search_test6():\n",
    "    return np.linalg.norm(np.array(validation_losses) - np.array(validation_losses_grader)) < 1e-7\n",
    "\n",
    "runtest(grid_search_test1, 'grid_search_test1')\n",
    "runtest(grid_search_test2, 'grid_search_test2')\n",
    "runtest(grid_search_test3, 'grid_search_test3')\n",
    "runtest(grid_search_test4, 'grid_search_test4')\n",
    "runtest(grid_search_test5, 'grid_search_test5')\n",
    "runtest(grid_search_test6, 'grid_search_test6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-gridsearch_test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs grid search test#\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "assert (len(training_losses) == k) \n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-gridsearch_test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs grid search test2\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "assert (len(validation_losses) == k)\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-gridsearch_test3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs grid search test3\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "assert (best_depth == depths[np.argmin(validation_losses)])\n",
    "\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-gridsearch_test4",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs grid search test4\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "assert (best_depth == best_depth_grader)\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-gridsearch_test5",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs grid search test5\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "assert np.linalg.norm(np.array(training_losses) - np.array(training_losses_grader)) < 1e-7\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-gridsearch_test6",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs grid search test6\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "assert np.linalg.norm(np.array(validation_losses) - np.array(validation_losses_grader)) < 1e-7\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-67814c28d3b41d3e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part Two [Graded]\n",
    "\n",
    "Now, implement the <code>generate_kFold</code> function, which takes in the number of training examples <code>n</code> and the number of folds <code>k</code> and returns a list of <code>k</code> folds where each fold takes the form <code>(training indices, validation indices)</code>.\n",
    "\n",
    "For instance, if n = 3 and k = 3, then one possible output of the the function is <code>[([1, 2], [3]), ([2, 3], [1]), ([1, 3], [2])]</code> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-generate_kFold",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def generate_kFold(n, k):\n",
    "    '''\n",
    "    Input:\n",
    "        n: number of training examples\n",
    "        k: number of folds\n",
    "    Returns:\n",
    "        kfold_indices: a list of len k. Each entry takes the form\n",
    "        (training indices, validation indices)\n",
    "    '''\n",
    "    assert k >= 2\n",
    "    kfold_indices = []\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    indices = np.random.permutation(n)\n",
    "    fold_size = n // k\n",
    "    \n",
    "    fold_indices = [indices[i*fold_size: (i+1)*fold_size] for i in range(k - 1)]\n",
    "    fold_indices.append(indices[(k-1)*fold_size:])\n",
    "    \n",
    "    \n",
    "    for i in range(k):\n",
    "        training_indices = [fold_indices[j] for j in range(k) if j != i]\n",
    "        validation_indices = fold_indices[i]\n",
    "        kfold_indices.append((np.concatenate(training_indices), validation_indices))\n",
    "    ### END SOLUTION\n",
    "    return kfold_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-generate_Kfold_selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# The following tests check that your generate_kFold function returns the correct number of total indices, train and test indices, and the correct ratio\n",
    "\n",
    "kfold_indices = generate_kFold(1004, 5)\n",
    "\n",
    "def generate_kFold_test1():\n",
    "    return len(kfold_indices) == 5\n",
    "\n",
    "def generate_kFold_test2():\n",
    "    t = [((len(train_indices) + len(test_indices)) == 1004) for (train_indices, test_indices) in kfold_indices]\n",
    "    return np.all(t)\n",
    "\n",
    "def generate_kFold_test3():\n",
    "    ratio_test = []\n",
    "    for (train_indices, validation_indices) in kfold_indices:\n",
    "        ratio = len(validation_indices) / len(train_indices)\n",
    "        ratio_test.append((ratio > 0.24 and ratio < 0.26))\n",
    "    return np.all(ratio_test)\n",
    "\n",
    "runtest(generate_kFold_test1, 'generate_kFold_test1')\n",
    "runtest(generate_kFold_test2, 'generate_kFold_test2')\n",
    "runtest(generate_kFold_test3, 'generate_kFold_test3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-generate_kFold_test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs generate Kfold test1\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "assert len(kfold_indices) == 5\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-generate_kFold_test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs generate Kfold test2\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "t = [((len(train_indices) + len(test_indices)) == 1004) for (train_indices, test_indices) in kfold_indices]\n",
    "assert np.all(t)\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-generate_kFold_test3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs generate Kfold test3\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "ratio_test = []\n",
    "for (train_indices, validation_indices) in kfold_indices:\n",
    "    ratio = len(validation_indices) / len(train_indices)\n",
    "    ratio_test.append((ratio > 0.24 and ratio < 0.26))\n",
    "assert np.all(ratio_test)\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-150c6bb77ba223d1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part Three [Graded]\n",
    "\n",
    "Use <code>grid_search</code> to implement the <code>cross_validation</code> function that takes in the training set <code>xTr, yTr</code>, a list of depth candidates <code>depths</code> and a list of indices that is generated by <code>generate_kFold</code>. Using <code>indices</code>, the function will do a grid search  on each fold and return the parameter that yields the best average validation loss across the folds. Note that in event of tie, the function should return the smallest depth candidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cross_validation",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def cross_validation(xTr, yTr, depths, indices):\n",
    "    '''\n",
    "    Input:\n",
    "        xTr: nxd matrix (training data)\n",
    "        yTr: n vector (training data)\n",
    "        depths: a list of len k\n",
    "        indices: indices from generate_kFold\n",
    "    Returns:\n",
    "        best_depth: the best parameter \n",
    "        training losses: a list of len k. the i-th entry corresponds to the the average training loss\n",
    "                the tree of depths[i]\n",
    "        validation_losses: a list of len k. the i-th entry corresponds to the the average validation loss\n",
    "                the tree of depths[i] \n",
    "    '''\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "    best_depth = None\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    for train_indices, validation_indices in indices:\n",
    "        xtrain, ytrain = xTr[train_indices], yTr[train_indices]\n",
    "        xval, yval = xTr[validation_indices], yTr[validation_indices]\n",
    "        \n",
    "        _, training_loss, validation_loss = grid_search(xtrain, ytrain, xval, yval, depths)\n",
    "        \n",
    "        training_losses.append(training_loss)\n",
    "        validation_losses.append(validation_loss)\n",
    "    \n",
    "    training_losses = np.mean(training_losses, axis=0)\n",
    "    validation_losses = np.mean(validation_losses, axis=0)\n",
    "    \n",
    "    best_depth = depths[np.argmin(validation_losses)]\n",
    "    best_tree = RegressionTree(depth=best_depth)\n",
    "    best_tree.fit(xTr, yTr)\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    return best_depth, training_losses, validation_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cross_validation_selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# The following tests check that your implementation of cross_validation returns the correct number of training and validation losses, the correct \"best depth\" and the correct values for training and validation loss\n",
    "\n",
    "depths = [1,2,3,4,5]\n",
    "k = len(depths)\n",
    "indices = generate_kFold(len(xTr), 5)\n",
    "best_depth, training_losses, validation_losses = cross_validation(xTr, yTr, depths, indices)\n",
    "best_depth_grader, training_losses_grader, validation_losses_grader = cross_validation_grader(xTr, yTr, depths, indices)\n",
    "\n",
    "# Check the length of the training loss\n",
    "def cross_validation_test1():\n",
    "    return (len(training_losses) == k) \n",
    "\n",
    "# Check the length of the validation loss\n",
    "def cross_validation_test2():\n",
    "    return (len(validation_losses) == k)\n",
    "\n",
    "# Check the argmin\n",
    "def cross_validation_test3():\n",
    "    return (best_depth == depths[np.argmin(validation_losses)])\n",
    "\n",
    "def cross_validation_test4():\n",
    "    return (best_depth == best_depth_grader)\n",
    "\n",
    "def cross_validation_test5():\n",
    "    return np.linalg.norm(np.array(training_losses) - np.array(training_losses_grader)) < 1e-7\n",
    "\n",
    "def cross_validation_test6():\n",
    "    return np.linalg.norm(np.array(validation_losses) - np.array(validation_losses_grader)) < 1e-7\n",
    "\n",
    "runtest(cross_validation_test1, 'cross_validation_test1')\n",
    "runtest(cross_validation_test2, 'cross_validation_test2')\n",
    "runtest(cross_validation_test3, 'cross_validation_test3')\n",
    "runtest(cross_validation_test4, 'cross_validation_test4')\n",
    "runtest(cross_validation_test5, 'cross_validation_test5')\n",
    "runtest(cross_validation_test6, 'cross_validation_test6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-cross_validation_test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs cross validation test1\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "assert (len(training_losses) == k) \n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-cross_validation_test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs cross validation test2\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "assert (len(validation_losses) == k)\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-cross_validation_test3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs cross validation test3\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "assert (best_depth == depths[np.argmin(validation_losses)])\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-cross_validation_test4",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs cross validation test4\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "assert (best_depth == best_depth_grader)\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-cross_validation_test5",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs cross validation test5\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "assert np.linalg.norm(np.array(training_losses) - np.array(training_losses_grader)) < 1e-7\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-cross_validation_test6",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs cross validation test6\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "assert np.linalg.norm(np.array(validation_losses) - np.array(validation_losses_grader)) < 1e-7\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
