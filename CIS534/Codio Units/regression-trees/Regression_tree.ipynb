{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-202739b8f3a67536",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2>About this Project</h2>\n",
    "<p>In this project, you will implement a Classification and Regression Tree (CART) algorithm. You will work with the <a href=\"https://archive.ics.uci.edu/ml/datasets/Ionosphere\">Ionosphere Data Set from the UCI Machine Learning Repository</a>, which consists of radar data from a system designed to target free electrons in the ionosphere. The data will be used to determine if a radar return was \"good\" (i.e. a signal was returned) or \"bad\" (i.e. the signal passed straight through the ionosphere). Your task will be to classify whether a return is good or bad based on the data.</p>\n",
    "\n",
    "<h3>Evaluation</h3>\n",
    "\n",
    "<p><strong>This project must be successfully completed and submitted in order to receive credit for this course. Your score on this project will be included in your final grade calculation.</strong><p>\n",
    "    \n",
    "<p>You are expected to write code where you see <em># YOUR CODE HERE</em> within the cells of this notebook. Not all cells will be graded; code input cells followed by cells marked with <em>#Autograder test cell</em> will be graded. Upon submitting your work, the code you write at these designated positions will be assessed using an \"autograder\" that will run all test cells to assess your code. You will receive feedback from the autograder that will identify any errors in your code. Use this feedback to improve your code if you need to resubmit. Be sure not to change the names of any provided functions, classes, or variables within the existing code cells, as this will interfere with the autograder. Also, remember to execute all code cells sequentially, not just those you’ve edited, to ensure your code runs properly.</p>\n",
    "    \n",
    "<p>You can resubmit your work as many times as necessary before the submission deadline. If you experience difficulty or have questions about this exercise, use the Q&A discussion board to engage with your peers or seek assistance from the instructor.<p>\n",
    "\n",
    "<p>Before starting your work, please review <a href=\"https://s3.amazonaws.com/ecornell/global/eCornellPlagiarismPolicy.pdf\">eCornell's policy regarding plagiarism</a> (the presentation of someone else's work as your own without source credit).</p>\n",
    "\n",
    "<h3>Submit Code for Autograder Feedback</h3>\n",
    "\n",
    "<p>Once you have completed your work on this notebook, you will submit your code for autograder review. Follow these steps:</p>\n",
    "\n",
    "<ol>\n",
    "  <li><strong>Save your notebook.</strong></li>\n",
    "  <li><strong>Mark as Completed —</strong> In the blue menu bar along the top of this code exercise window, you’ll see a menu item called <strong>Education</strong>. In the <strong>Education</strong> menu, click <strong>Mark as Completed</strong> to submit your code for autograder/instructor review. This process will take a moment and a progress bar will show you the status of your submission.</li>\n",
    "\t<li><strong>Review your results —</strong> Once your work is marked as complete, the results of the autograder will automatically be presented in a new tab within the code exercise window. You can click on the assessment name in this feedback window to see more details regarding specific feedback/errors in your code submission.</li>\n",
    "  <li><strong>Repeat, if necessary —</strong> The Jupyter notebook will always remain accessible in the first tabbed window of the exercise. To reattempt the work, you will first need to click <strong>Mark as Uncompleted</strong> in the <strong>Education</strong> menu and then proceed to make edits to the notebook. Once you are ready to resubmit, follow steps one through three. You can repeat this procedure as many times as necessary.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-727395f9bc8b5c9d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2>Implementing CART</h2>\n",
    "\n",
    "### Getting Started\n",
    "\n",
    "Before you get started, let's import a few packages that you will need. You will also load a data set <a href=\"https://archive.ics.uci.edu/ml/datasets/Ionosphere\">ION</a>, which you will use for our binary test classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5445d0afdbc4c054",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're running python 3.7.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pylab import *\n",
    "from numpy.matlib import repmat\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "import time\n",
    "import sys\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "sys.path.append('/home/codio/workspace/.guides/hf')\n",
    "from helper import *\n",
    "\n",
    "print('You\\'re running python %s' % sys.version.split(' ')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-134b99a8d0c69131",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# load in some binary test data (labels are -1, +1)\n",
    "data = loadmat(\"ion.mat\")\n",
    "\n",
    "# Load the training data\n",
    "xTrIon  = data['xTr'].T\n",
    "yTrIon  = data['yTr'].flatten()\n",
    "\n",
    "# Load the test data\n",
    "xTeIon  = data['xTe'].T\n",
    "yTeIon  = data['yTe'].flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-482d8824ee6d2b59",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Tree Structure\n",
    "\n",
    "<p>We've provided a tree structure for you with distinct leaves and nodes. Leaves have two fields, parent (another node) and prediction (a numerical value).</p>\n",
    "\n",
    "### Nodes have six fields:\n",
    "\n",
    "<ol>\n",
    "<li> <b>left</b>: node describing left subtree </li>\n",
    "<li> <b>right</b>: node describing right subtree </li>\n",
    "<li> <b>parent</b>: the parent of the current subtree. The head of the tree always has <code><b>None</b></code> as its parent. Feel free to initialize nodes with this field set to <code><b>None</b></code> as long as you set the correct parent later on. </li>\n",
    "<li> <b>cutoff_id</b>: index of feature to cut </li>\n",
    "<li> <b>cutoff_val</b>: cutoff value c (<=c : left, and >c : right)</li>\n",
    "<li> <b>prediction</b>: prediction at this node </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-919899ceb491184f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "class TreeNode(object):\n",
    "    \"\"\"Tree class.\n",
    "    (You don't need to add any methods or fields here but feel\n",
    "    free to if you like. The tests will only reference the fields\n",
    "    defined in the constructor below, so be sure to set these\n",
    "    correctly.)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, left, right, parent, cutoff_id, cutoff_val, prediction):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.parent = parent\n",
    "        self.cutoff_id = cutoff_id\n",
    "        self.cutoff_val = cutoff_val\n",
    "        self.prediction = prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c219552fd154672d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The code below generates spiral data using the trigonometric functions sine and cosine, then splits the data into train and test segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9a38f0686e9d323f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def spiraldata(N=300):\n",
    "    r = np.linspace(1,2*np.pi,N) # generate a vector of \"radius\" values\n",
    "    xTr1 = np.array([np.sin(2.*r)*r, np.cos(2*r)*r]).T # generate a curve that draws circles with increasing radius\n",
    "    xTr2 = np.array([np.sin(2.*r+np.pi)*r, np.cos(2*r+np.pi)*r]).T\n",
    "    xTr = np.concatenate([xTr1, xTr2], axis=0)\n",
    "    yTr = np.concatenate([np.ones(N), -1 * np.ones(N)])\n",
    "    xTr = xTr + np.random.randn(xTr.shape[0], xTr.shape[1])*0.2\n",
    "    \n",
    "    # Now sample alternating values to generate the test and train sets.\n",
    "    xTe = xTr[::2,:]\n",
    "    yTe = yTr[::2]\n",
    "    xTr = xTr[1::2,:]\n",
    "    yTr = yTr[1::2]\n",
    "    \n",
    "    return xTr, yTr, xTe, yTe\n",
    "\n",
    "xTrSpiral, yTrSpiral, xTeSpiral, yTeSpiral = spiraldata(150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-65f296cbe2f8b1ab",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We can plot xTrSpiral to see the curve generated by the function above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x11c540c18>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(xTrSpiral[:,0], xTrSpiral[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c632b2561da0c25c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2> Implement Regression Trees </h2>\n",
    "\n",
    "<h3> Part One [Graded]</h3>\n",
    "\n",
    "<p>First, implement the function <code>sqsplit</code>, which takes as input a (weighted) data set with labels and computes the best feature and cut-value of an optimal split based on minimum squared error. The third input is a weight vector which assigns a positive weight to each training sample. The loss you should minimize is the averaged weighted squared-loss:\n",
    "$$\n",
    "\t{\\cal L}(S)=\\sum_{i \\in L} {w_{i}(y_{i} - T_{L})}^2+\\sum_{i \\in R} {w_{i}(y_{i} - T_{R})}^2.\\label{q2:loss}\n",
    "$$\n",
    "<br>\n",
    "</p>\n",
    "You are building a regression tree and right now you need to choose a split for the given dataset $S=\\{(\\vec x_1,y_1),\\dots,(\\vec x_n,y_n)\\}$ (where there are continuous labels $y_i\\in{\\cal R}$). Suppose you split on some feature $j$ with value $c$ and partition the dataset in to two sets of indices, $L$--the set of indices on the left (i.e., $i \\in L \\Rightarrow [x_{i}]_{j} < c$)--and $R$--the set of indices on the right (i.e., $i \\in R \\Rightarrow [x_{i}]_{j} > c$). Suppose you assign every data point on the left the prediction $T_{L}$ and every data point on the right the prediction $T_{R}$. Finally, suppose that each data point $x_{i}$ has an associated weight $w_{i}$, and that the weights are normalized (i.e., $\\sum_{i} w_{i} = 1$). \n",
    "\n",
    "\n",
    "<p> Now, you show that setting $T_{L}$ and $T_{R}$ to the weighted average label over their respective sets (i.e., $T_{L} = \\frac{1}{W_{L}}\\sum_{i\\in L}w_{i}y_{i}$ and $T_{R} = \\frac{1}{W_{R}}\\sum_{i\\in R}w_{i}y_{i}$) minimizes the loss $\\cal L$, where $W_{L}=\\sum_{i \\in L}w_{i}$ and $W_{R}=\\sum_{i \\in R} w_{i}$ are the total weight of the left and right side respectively.\n",
    "\n",
    "<p> You take the derivative of the loss with respect to $T_{L}$ to obtain $$\\frac{d}{dT_{L}} {\\cal L}(S) = -2\\sum_{i \\in L}w_{i}(y_i - T_L)=-2\\sum_{i\\in L}w_iy_i + 2T_{L}\\sum_{i}w_{i}$$ Setting this equal to zero and solving, we get $$2T_{L}w_{L}=2\\sum_{i \\in L}w_{i}y_{i}$$ and therefore $$T_{L} = \\frac{1}{W_{L}}\\sum_{i \\in L}w_{i}y_{i}$$ A symmetric argument holds for $T_{R}$.</p>\n",
    "\n",
    "So if you know the split, setting $T_{L}$ and $T_{R}$ to the weighted average label over their respective sets minimizes the loss. To find the best split, you can evaluate all possible splits and then search for the split that yields the minimum loss which you will in <code>sqsplit</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-sqsplit",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def sqsplit(xTr,yTr,weights=None):\n",
    "    \"\"\"Finds the best feature, cut value, and loss value.\n",
    "    \n",
    "    Input:\n",
    "        xTr:     n x d matrix of data points\n",
    "        yTr:     n-dimensional vector of labels\n",
    "        weights: n-dimensional weight vector for data points\n",
    "    \n",
    "    Output:\n",
    "        feature:  index of the best cut's feature\n",
    "        cut:      cut-value of the best cut\n",
    "        bestloss: loss of the best cut\n",
    "    \"\"\"\n",
    "    N,D = xTr.shape\n",
    "    assert D > 0 # must have at least one dimension\n",
    "    assert N > 1 # must have at least two samples\n",
    "    if weights is None: # if no weights are passed on, assign uniform weights\n",
    "        weights = np.ones(N)\n",
    "    weights = weights/sum(weights) # Weights need to sum to one (we just normalize them)\n",
    "    bestloss = np.inf\n",
    "    feature = np.inf\n",
    "    cut = np.inf\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    for d in range(D):\n",
    "        ii = xTr[:, d].argsort() # sort data along that dimensions\n",
    "        xs = xTr[ii, d] # sorted feature values\n",
    "        ws = weights[ii] # sorted weights\n",
    "        ys = yTr[ii] # sorted labels\n",
    "        \n",
    "        # Get indices where we should split\n",
    "        idif = np.where(np.logical_not(np.isclose(np.diff(xs, axis=0), 0)))[0]\n",
    "        \n",
    "        for j in idif:\n",
    "            T_L = (ws[:j + 1] @ ys[:j + 1]) / np.sum(ws[:j + 1])\n",
    "            T_R = (ws[j + 1: ] @ ys[j + 1:]) / np.sum(ws[j + 1:])\n",
    "            \n",
    "            left_loss = ((ys[:j + 1] - T_L)**2) @ ws[:j + 1]\n",
    "            right_loss = ((ys[j + 1:] - T_R)**2) @ ws[j + 1:]\n",
    "            \n",
    "            loss = left_loss + right_loss            \n",
    "            if loss < bestloss:\n",
    "                feature = d\n",
    "                cut = (xs[j]+xs[j+1])/2\n",
    "                bestloss = loss\n",
    "        \n",
    "    assert feature != np.inf and cut != np.inf\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    return feature, cut, bestloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-sqsplit_selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# The tests below check that your sqsplit function returns the correct values for several different input datasets\n",
    "\n",
    "t0 = time.time()\n",
    "fid, cut, loss = sqsplit(xTrIon,yTrIon)\n",
    "t1 = time.time()\n",
    "\n",
    "print('Elapsed time: {:0.2f} seconds'.format(t1-t0))\n",
    "print(\"The best split is on feature 2 on value 0.304\")\n",
    "print(\"Your tree split on feature %i on value: %2.3f \\n\" % (fid,cut))\n",
    "\n",
    "def sqsplit_test1():\n",
    "    a = np.isclose(sqsplit(xor4,yor4)[2], .25)\n",
    "    b = np.isclose(sqsplit(xor3,yor3)[2], .25)\n",
    "    c = np.isclose(sqsplit(xor2,yor2)[2], .25)\n",
    "    return a and b and c\n",
    "\n",
    "def sqsplit_test2():\n",
    "    x = np.array(range(1000)).reshape(-1,1)\n",
    "    y = np.hstack([np.ones(500),-1*np.ones(500)]).T\n",
    "    w = np.ones(1000)\n",
    "    _, cut, _ = sqsplit(x, y, weights=w)\n",
    "    # 499.5 is the correct answer but will accept other two answers\n",
    "    return cut == 499 or cut == 500 or cut == 499.5\n",
    "\n",
    "def sqsplit_test3():\n",
    "    fid,cut,loss = sqsplit(xor5,yor5)\n",
    "    # cut should be 0.5 but 0 is also accepted\n",
    "    return fid == 0 and (cut == 0 or cut == 0.5) and np.isclose(loss, 2/3)\n",
    "\n",
    "runtest(sqsplit_test1,'sqsplit_test1')\n",
    "runtest(sqsplit_test2,'sqsplit_test2')\n",
    "runtest(sqsplit_test3,'sqsplit_test3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-sqsplit_test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs distance_accuracy\n",
    "### BEGIN HIDDEN TESTS\n",
    "a = np.isclose(sqsplit(xor4,yor4)[2], .25)\n",
    "b = np.isclose(sqsplit(xor3,yor3)[2], .25)\n",
    "c = np.isclose(sqsplit(xor2,yor2)[2], .25)\n",
    "assert a and b and c\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-sqsplit_test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs distance_accuracy\n",
    "### BEGIN HIDDEN TESTS\n",
    "x = np.array(range(1000)).reshape(-1,1)\n",
    "y = np.hstack([np.ones(500),-1*np.ones(500)]).T\n",
    "w = np.ones(1000)\n",
    "_, cut, _ = sqsplit(x, y, weights=w)\n",
    "assert cut == 499 or cut == 500 or cut == 499.5\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-sqsplit_test3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs distance_accuracy\n",
    "### BEGIN HIDDEN TESTS\n",
    "fid,cut,loss = sqsplit(xor5,yor5)\n",
    "assert fid == 0 and (cut == 0 or cut == 0.5) and np.isclose(loss, 2/3)\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-985b43cf6a0b1e16",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part Two [Graded]\n",
    "\n",
    "Implement the function <code>cart</code>, which returns a regression tree based on the minimum squared loss splitting rule. The function takes training data, a maximum depth, and the weight of each training example. Maximum depth and weight are optional arguments. If they are not provided you should set the maximum depth to infinity and equally weight each example. You should use the function <code>sqsplit</code> to make your splits.</p>\n",
    "\n",
    "<p>Use the provided <code>TreeNode</code> class to represent your tree. Note that the nature of CART trees implies that every node has exactly 0 or 2 children.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cart",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def cart(xTr,yTr,depth=np.inf,weights=None):\n",
    "    \"\"\"Builds a CART tree.\n",
    "    \n",
    "    The maximum tree depth is defined by \"maxdepth\" (maxdepth=2 means one split).\n",
    "    Each example can be weighted with \"weights\".\n",
    "\n",
    "    Args:\n",
    "        xTr:      n x d matrix of data\n",
    "        yTr:      n-dimensional vector\n",
    "        maxdepth: maximum tree depth\n",
    "        weights:  n-dimensional weight vector for data points\n",
    "\n",
    "    Returns:\n",
    "        tree: root of decision tree\n",
    "    \"\"\"\n",
    "    n,d = xTr.shape\n",
    "    if weights is None:\n",
    "        w = np.ones(n) / float(n)\n",
    "    else:\n",
    "        w = weights\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    index = np.arange(n)\n",
    "    prediction = yTr.dot(w) / float(np.sum(w))\n",
    "    if depth == 0 or np.all(yTr == yTr[0]) or np.max(np.abs(np.diff(xTr, axis=0))) < (np.finfo(float).eps * 100):\n",
    "        # Create leaf Node\n",
    "        return TreeNode(None, None, None, None, None, prediction)\n",
    "    else:\n",
    "        feature,cut,h = sqsplit(xTr,yTr,w)\n",
    "        left_idx  = index[xTr[:,feature] <= cut]\n",
    "        right_idx = index[xTr[:,feature] > cut]\n",
    "        \n",
    "        left_w  = w[left_idx]\n",
    "        right_w = w[right_idx]\n",
    "        left  = cart(xTr[left_idx,:],   yTr[left_idx],  depth=depth-1, weights=left_w)\n",
    "        right = cart(xTr[right_idx,:],  yTr[right_idx], depth=depth-1, weights=right_w)\n",
    "        currNode = TreeNode(left, right, None, feature, cut, prediction)\n",
    "        left.parent  = currNode\n",
    "        right.parent = currNode\n",
    "        \n",
    "        return currNode\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cart_selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# The tests below check that your implementation of cart  returns the correct predicted values for a sample dataset\n",
    "\n",
    "#test case 1\n",
    "def cart_test1():\n",
    "    t=cart(xor4,yor4)\n",
    "    return DFSxor(t)\n",
    "\n",
    "#test case 2\n",
    "def cart_test2():\n",
    "    y = np.random.rand(16);\n",
    "    t = cart(xor4,y);\n",
    "    yTe = DFSpreds(t)[:];\n",
    "    # Check that every label appears exactly once in the tree\n",
    "    y.sort()\n",
    "    yTe.sort()\n",
    "    return np.all(np.isclose(y, yTe))\n",
    "\n",
    "#test case 3\n",
    "def cart_test3():\n",
    "    xRep = np.concatenate([xor2, xor2])\n",
    "    yRep = np.concatenate([yor2, 1-yor2])\n",
    "    t = cart(xRep, yRep)\n",
    "    return DFSxorUnsplittable(t)\n",
    "\n",
    "runtest(cart_test1,'cart_test1')\n",
    "runtest(cart_test2,'cart_test2')\n",
    "runtest(cart_test3,'cart_test3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-cart_test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs cart_test1\n",
    "### BEGIN HIDDEN TESTS\n",
    "t=cart(xor4,yor4)\n",
    "assert DFSxor(t)\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-cart_test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs cart_test2\n",
    "### BEGIN HIDDEN TESTS\n",
    "y = np.random.rand(16);\n",
    "t = cart(xor4,y);\n",
    "yTe = DFSpreds(t)[:];\n",
    "y.sort()\n",
    "yTe.sort()\n",
    "assert np.all(np.isclose(y, yTe))\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-cart_test3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs cart_test3\n",
    "### BEGIN HIDDEN TESTS\n",
    "xRep = np.concatenate([xor2, xor2])\n",
    "yRep = np.concatenate([yor2, 1-yor2])\n",
    "t = cart(xRep, yRep)\n",
    "assert DFSxorUnsplittable(t)\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-91bac7e5c5968bbf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part Three [Graded]\n",
    "\n",
    "<p>Implement the function <code>evaltree</code>, which evaluates a decision tree on a given test data set.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-evaltree",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def evaltree(root,xTe):\n",
    "    \"\"\"Evaluates xTe using decision tree root.\n",
    "    \n",
    "    Input:\n",
    "        root: TreeNode decision tree\n",
    "        xTe:  n x d matrix of data points\n",
    "    \n",
    "    Output:\n",
    "        pred: n-dimensional vector of predictions\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    def evaltreehelper(root,xTe, idx=[]):\n",
    "        \"\"\"Evaluates xTe using decision tree root.\n",
    "\n",
    "        Input:\n",
    "            root: TreeNode decision tree\n",
    "            xTe:  n x d matrix of data points\n",
    "\n",
    "        Output:\n",
    "            pred: n-dimensional vector of predictions\n",
    "        \"\"\"\n",
    "        assert root is not None\n",
    "        n = xTe.shape[0]\n",
    "        pred = np.zeros(n)\n",
    "\n",
    "        # TODO:\n",
    "        if len(idx)==0: idx=np.ones(n)==1 \n",
    "\n",
    "        if root.left is None and root.right is None:\n",
    "             return np.ones(sum(idx))*root.prediction\n",
    "            \n",
    "        assert root.left is not None and root.right is not None\n",
    "        feature, cutoff = root.cutoff_id, root.cutoff_val\n",
    "\n",
    "        idxL=idx & (xTe[:,feature] <= cutoff)\n",
    "        if root.left.left is None and root.left.right is None:\n",
    "             pred[idxL]=root.left.prediction\n",
    "        else:\n",
    "             pred[idxL]=evaltreehelper(root.left, xTe,idxL) \n",
    "\n",
    "        idxR=idx & (xTe[:,feature]  > cutoff)\n",
    "        if root.right.left is None and root.right.right is None:\n",
    "             pred[idxR]=root.right.prediction\n",
    "        else:\n",
    "             pred[idxR]=evaltreehelper(root.right,xTe,idxR)\n",
    "        return(pred[idx])\n",
    "    \n",
    "    return evaltreehelper(root,xTe)\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-evaltree_selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# The following tests check that your implementation of evaltree returns the correct predictions for two sample trees\n",
    "\n",
    "t0 = time.time()\n",
    "root = cart(xTrIon, yTrIon)\n",
    "t1 = time.time()\n",
    "\n",
    "tr_err   = np.mean((evaltree(root,xTrIon) - yTrIon)**2)\n",
    "te_err   = np.mean((evaltree(root,xTeIon) - yTeIon)**2)\n",
    "\n",
    "print(\"Elapsed time: %.2f seconds\" % (t1-t0))\n",
    "print(\"Training RMSE : %.2f\" % tr_err)\n",
    "print(\"Testing  RMSE : %.2f \\n\" % te_err)\n",
    "\n",
    "#test case 1\n",
    "def evaltree_test1():\n",
    "    t = cart(xor4,yor4)\n",
    "    xor4te = xor4 + (np.sign(xor4 - .5) * .1)\n",
    "    inds = np.arange(16)\n",
    "    np.random.shuffle(inds)\n",
    "    # Check that shuffling and expanding the data doesn't affect the predictions\n",
    "    return np.all(np.isclose(evaltree(t, xor4te[inds,:]), yor4[inds]))\n",
    "\n",
    "#test case 2\n",
    "def evaltree_test2():\n",
    "    a = TreeNode(None, None, None, None, None, 1)\n",
    "    b = TreeNode(None, None, None, None, None, -1)\n",
    "    c = TreeNode(None, None, None, None, None, 0)\n",
    "    d = TreeNode(None, None, None, None, None, -1)\n",
    "    e = TreeNode(None, None, None, None, None, -1)\n",
    "    x = TreeNode(a, b, None, 0, 10, 0)\n",
    "    y = TreeNode(x, c, None, 0, 20, 0)\n",
    "    z = TreeNode(d, e, None, 0, 40, 0)\n",
    "    t = TreeNode(y, z, None, 0, 30, 0)\n",
    "    # Check that the custom tree evaluates correctly\n",
    "    return np.all(np.isclose(\n",
    "            evaltree(t, np.array([[45, 35, 25, 15, 5]]).T),\n",
    "            np.array([-1, -1, 0, -1, 1])))\n",
    "\n",
    "runtest(evaltree_test1,'evaltree_test1')\n",
    "runtest(evaltree_test2,'evaltree_test2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-evaltree_test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs evaltree_test1\n",
    "### BEGIN HIDDEN TESTS\n",
    "t = cart(xor4,yor4)\n",
    "xor4te = xor4 + (np.sign(xor4 - .5) * .1)\n",
    "inds = np.arange(16)\n",
    "np.random.shuffle(inds)\n",
    "# Check that shuffling and expanding the data doesn't affect the predictions\n",
    "assert np.all(np.isclose(evaltree(t, xor4te[inds,:]), yor4[inds]))\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-evaltree_test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs evaltree_test2\n",
    "### BEGIN HIDDEN TESTS\n",
    "a = TreeNode(None, None, None, None, None, 1)\n",
    "b = TreeNode(None, None, None, None, None, -1)\n",
    "c = TreeNode(None, None, None, None, None, 0)\n",
    "d = TreeNode(None, None, None, None, None, -1)\n",
    "e = TreeNode(None, None, None, None, None, -1)\n",
    "x = TreeNode(a, b, None, 0, 10, 0)\n",
    "y = TreeNode(x, c, None, 0, 20, 0)\n",
    "z = TreeNode(d, e, None, 0, 40, 0)\n",
    "t = TreeNode(y, z, None, 0, 30, 0)\n",
    "assert np.all(np.isclose(\n",
    "        evaltree(t, np.array([[45, 35, 25, 15, 5]]).T),\n",
    "        np.array([-1, -1, 0, -1, 1])))\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-534ce4bb724b2f26",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<p>The following code defines a function <code>visclassifier()</code>, which plots the decision boundary of a classifier in 2 dimensions. Execute the following code to see what the decision boundary of your tree looks like on the ion data set. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8059df3cc7cdb39d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def visclassifier(fun,xTr,yTr,w=[],b=0):\n",
    "    \"\"\"\n",
    "    visualize decision boundary\n",
    "    Define the symbols and colors we'll use in the plots later\n",
    "    \"\"\"\n",
    "\n",
    "    yTr = np.array(yTr).flatten()\n",
    "    w = np.array(w).flatten()\n",
    "\n",
    "    symbols = [\"ko\",\"kx\"]\n",
    "    marker_symbols = ['o', 'x']\n",
    "    mycolors = [[0.5, 0.5, 1], [1, 0.5, 0.5]]\n",
    "    # get the unique values from labels array\n",
    "    classvals = np.unique(yTr)\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    # return 300 evenly spaced numbers over this interval\n",
    "    res=300\n",
    "    xrange = np.linspace(min(xTr[:, 0]), max(xTr[:, 0]),res)\n",
    "    yrange = np.linspace(min(xTr[:, 1]), max(xTr[:, 1]),res)\n",
    "    \n",
    "    # repeat this matrix 300 times for both axes\n",
    "    pixelX = repmat(xrange, res, 1)\n",
    "    pixelY = repmat(yrange, res, 1).T\n",
    "\n",
    "    \n",
    "    xTe = np.array([pixelX.flatten(), pixelY.flatten()]).T\n",
    "\n",
    "    # test all of these points on the grid\n",
    "    testpreds = fun(xTe)\n",
    "    \n",
    "    # reshape it back together to make our grid\n",
    "    Z = testpreds.reshape(res, res)\n",
    "    # Z[0,0] = 1 # optional: scale the colors correctly\n",
    "    \n",
    "    # fill in the contours for these predictions\n",
    "    plt.contourf(pixelX, pixelY, np.sign(Z), colors=mycolors)\n",
    "\n",
    "    # creates x's and o's for training set\n",
    "    for idx, c in enumerate(classvals):\n",
    "        plt.scatter(xTr[yTr == c,0],\n",
    "            xTr[yTr == c,1],\n",
    "            marker=marker_symbols[idx],\n",
    "            color='k'\n",
    "            )\n",
    "    \n",
    "    if w != []:\n",
    "        alpha = -1 * b / (w ** 2).sum()\n",
    "        plt.quiver(w[0] * alpha, w[1] * alpha,\n",
    "            w[0], w[1], linewidth=2, color=[0,1,0])\n",
    "\n",
    "    plt.axis('tight')\n",
    "    # shows figure and blocks\n",
    "    plt.show()\n",
    "\n",
    "tree=cart(xTrSpiral,yTrSpiral) # compute tree on training data \n",
    "visclassifier(lambda X:evaltree(tree,X),xTrSpiral,yTrSpiral)\n",
    "print(\"Training error: %.4f\" % np.mean(np.sign(evaltree(tree,xTrSpiral)) != yTrSpiral))\n",
    "print(\"Testing error:  %.4f\" % np.mean(np.sign(evaltree(tree,xTeSpiral)) != yTeSpiral))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8ad51604f6c3e28c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def onclick_cart(event):\n",
    "    \"\"\"\n",
    "    Visualize cart, including new point\n",
    "    \"\"\"\n",
    "    global xTraining,labels\n",
    "    # create position vector for new point\n",
    "    pos=np.array([[event.xdata,event.ydata]]) \n",
    "    if event.key == 'shift': # add positive point\n",
    "        color='or'\n",
    "        label=1\n",
    "    else: # add negative point\n",
    "        color='ob'\n",
    "        label=-1    \n",
    "    xTraining = np.concatenate((xTraining,pos), axis = 0)\n",
    "    labels.append(label);\n",
    "    marker_symbols = ['o', 'x']\n",
    "    classvals = np.unique(labels)\n",
    "        \n",
    "    mycolors = [[0.5, 0.5, 1], [1, 0.5, 0.5]]\n",
    "    \n",
    "    # return 300 evenly spaced numbers over this interval\n",
    "    res=300\n",
    "    xrange = np.linspace(0, 1,res)\n",
    "    yrange = np.linspace(0, 1,res)\n",
    "\n",
    "    \n",
    "    # repeat this matrix 300 times for both axes\n",
    "    pixelX = repmat(xrange, res, 1)\n",
    "    pixelY = repmat(yrange, res, 1).T\n",
    "\n",
    "    xTe = np.array([pixelX.flatten(), pixelY.flatten()]).T\n",
    "\n",
    "    # get decision tree\n",
    "    tree=cart(xTraining,np.array(labels).flatten())\n",
    "    fun = lambda X:evaltree(tree,X)\n",
    "    # test all of these points on the grid\n",
    "    testpreds = fun(xTe)\n",
    "    \n",
    "    # reshape it back together to make our grid\n",
    "    Z = testpreds.reshape(res, res)\n",
    "    # Z[0,0] = 1 # optional: scale the colors correctly\n",
    "    \n",
    "    plt.cla()    \n",
    "    plt.xlim((0,1))\n",
    "    plt.ylim((0,1))\n",
    "    # fill in the contours for these predictions\n",
    "    plt.contourf(pixelX, pixelY, np.sign(Z), colors=mycolors)\n",
    "    \n",
    "    for idx, c in enumerate(classvals):\n",
    "        plt.scatter(xTraining[labels == c,0],\n",
    "            xTraining[labels == c,1],\n",
    "            marker=marker_symbols[idx],\n",
    "            color='k'\n",
    "            )\n",
    "    plt.show()\n",
    "    \n",
    "        \n",
    "xTraining= np.array([[5,6]])\n",
    "labels = [1]\n",
    "%matplotlib notebook\n",
    "fig = plt.figure()\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "cid = fig.canvas.mpl_connect('button_press_event', onclick_cart)\n",
    "plt.title('Use shift-click to add negative points.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional linear time update rule \n",
    "# generate two gaussians as data that has a lot of features"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
