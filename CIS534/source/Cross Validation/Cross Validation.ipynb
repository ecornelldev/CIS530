{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ee3457876c038971",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Introduction\n",
    "\n",
    "In this assignment, we will implement cross validation to pick the best depth (hyperparameter) for a regression tree. Before we get started, let's import a few packages that you might need. We will use the <a href=\"https://archive.ics.uci.edu/ml/datasets/Ionosphere\">ION</a> dataset for regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-39ac94e6e8bdc336",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pylab import *\n",
    "from numpy.matlib import repmat\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "import time\n",
    "import helper as h\n",
    "%matplotlib notebook\n",
    "\n",
    "data = loadmat(\"ion.mat\")\n",
    "xTr  = data['xTr'].T\n",
    "yTr  = data['yTr'].flatten()\n",
    "xTe  = data['xTe'].T\n",
    "yTe  = data['yTe'].flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7d95b26aeed38b8e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We also developed a regression tree classifier in helper.py. The following code cell shows you how to instantiate a  regression tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-778baeb6ff2b94be",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a regression tree with no restriction on its depth\n",
    "# if you want to create a tree of depth k\n",
    "# then call h.RegressionTree(depth=k)\n",
    "tree = h.RegressionTree(depth=np.inf)\n",
    "\n",
    "# To fit/train the regression tree\n",
    "tree.fit(xTr, yTr)\n",
    "\n",
    "# To use the trained regression tree to make prediction\n",
    "pred = tree.predict(xTr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-37e18ecd068c189d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We have also created square loss function that takes in the prediction <code>pred</code> and ground truth <code>truth</code> and returns the average square loss between prediction and ground truth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d620b929b0f3b5ed",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def square_loss(pred, truth):\n",
    "    return np.mean((pred - truth)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a7b5b10efd4cca7f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now we will look at the performance of our tree on both the training set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bee4f89dde382e5f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0000\n",
      "Test Loss: 0.6857\n"
     ]
    }
   ],
   "source": [
    "print('Training Loss: {:.4f}'.format(square_loss(tree.predict(xTr), yTr)))\n",
    "print('Test Loss: {:.4f}'.format(square_loss(tree.predict(xTe), yTe)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5c969869a177745f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "As you can see, our tree achives zero training loss on the training set but not zero test loss. Clearly, our model is overfitting! To reduce overfitting, we need to control the depth of the tree; one way to pick the optimal depth is to do kFold Cross Validation. To do so, you will first implement <code>grid_search</code>, which finds the best depths given a training set and validation set. Then you will implement <code>generate_kFold</code> that generates the split for kFold cross validation. Finally, you will combine the two functions to implement <code>cross_validation</code>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-88634e1d5f07d0ab",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Implement the function <code>grid_search</code>, which takes in a training set <code>xTr, yTr</code>, a validation set <code>xVal, yVal</code> and a list of tree depth candidates <code>depths</code>. Your job here is to fit a regression tree for each depth candidate on the training set <code>xTr, yTr</code>, evaluate the fitted tree on the validation set <code>xVal, yVal</code> and then pick the candidate that yields the lowest loss for the validation set. Note: in the event of tie, return the smallest depth candidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1ba37290c7fce98e",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def grid_search(xTr, yTr, xVal, yVal, depths):\n",
    "    '''\n",
    "    Input:\n",
    "        xTr: nxd matrix\n",
    "        yTr: n vector\n",
    "        xVal: mxd matrix\n",
    "        yVal: m vector\n",
    "        depths: a list of len k\n",
    "    Return:\n",
    "        best_depth: the depth that yields that lowest loss on the validation set\n",
    "        training losses: a list of len k. the i-th entry corresponds to the the training loss\n",
    "                the tree of depths[i]\n",
    "        validation_losses: a list of len k. the i-th entry corresponds to the the validation loss\n",
    "                the tree of depths[i]\n",
    "    '''\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "    best_depth = None\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    for i in depths:\n",
    "        tree = h.RegressionTree(i)\n",
    "        tree.fit(xTr, yTr)\n",
    "        \n",
    "        training_loss = square_loss(tree.predict(xTr), yTr)\n",
    "        validation_loss = square_loss(tree.predict(xVal), yVal)\n",
    "        training_losses.append(training_loss)\n",
    "        validation_losses.append(validation_loss)\n",
    "    \n",
    "    best_depth = depths[np.argmin(validation_losses)]\n",
    "    ### END SOLUTION\n",
    "    return best_depth, training_losses, validation_losses\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-67814c28d3b41d3e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now, implement the <code>generate_kFold</code> function, which takes in the number of training examples <code>n</code> and the number of folds <code>k</code> and returns a list of <code>k</code> folds where each fold takes the form <code>(training indices, validation indices)</code> .\n",
    "\n",
    "For instance, if n = 3 and k = 3, then one possible output of the the function is <code>[([1, 2], [3]), ([2, 3], [1]), ([1, 3], [2])]</code> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5b769b8f6720f1cf",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def generate_kFold(n, k):\n",
    "    '''\n",
    "    Input:\n",
    "        n: number of training examples\n",
    "        k: number of folds\n",
    "    Returns:\n",
    "        kfold_indices: a list of len k. Each entry takes the form\n",
    "        (training indices, validation indices)\n",
    "    '''\n",
    "    assert k >= 2\n",
    "    kfold_indices = []\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    indices = np.random.permutation(n)\n",
    "    fold_size = n // k\n",
    "    \n",
    "    fold_indices = [indices[i*fold_size: (i+1)*fold_size] for i in range(k - 1)]\n",
    "    fold_indices.append(indices[(k-1)*fold_size:])\n",
    "    \n",
    "    \n",
    "    for i in range(k):\n",
    "        training_indices = [fold_indices[j] for j in range(k) if j != i]\n",
    "        validation_indices = fold_indices[i]\n",
    "        kfold_indices.append((np.concatenate(training_indices), validation_indices))\n",
    "    ### END SOLUTION\n",
    "    return kfold_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-150c6bb77ba223d1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Use <code>grid_search</code> to implement the <code>cross_validation</code> function that takes in the training set <code>xTr, yTr</code>, a list of depth candidates <code>depths</code> and a list of indices that is generated by <code>generate_kFold</code>. Using <code>indices</code>, the function will do a grid search  on each fold and return the parameter that yields the best average validation loss across the folds. Note that in event of tie, the function should return the smallest depth candidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-736efbb9c901411c",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def cross_validation(xTr, yTr, depths, indices):\n",
    "    '''\n",
    "    Input:\n",
    "        xTr: nxd matrix (training data)\n",
    "        yTr: n vector (training data)\n",
    "        depths: a list of len k\n",
    "        indices: indices from generate_kFold\n",
    "    Returns:\n",
    "        best_depth: the best parameter \n",
    "        training losses: a list of len k. the i-th entry corresponds to the the average training loss\n",
    "                the tree of depths[i]\n",
    "        validation_losses: a list of len k. the i-th entry corresponds to the the average validation loss\n",
    "                the tree of depths[i] \n",
    "    '''\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "    best_depth = None\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    for train_indices, validation_indices in indices:\n",
    "        xtrain, ytrain = xTr[train_indices], yTr[train_indices]\n",
    "        xval, yval = xTr[validation_indices], yTr[validation_indices]\n",
    "        \n",
    "        _, training_loss, validation_loss = grid_search(xtrain, ytrain, xval, yval, depths)\n",
    "        \n",
    "        training_losses.append(training_loss)\n",
    "        validation_losses.append(validation_loss)\n",
    "    \n",
    "    training_losses = np.mean(training_losses, axis=0)\n",
    "    validation_losses = np.mean(validation_losses, axis=0)\n",
    "    \n",
    "    best_depth = depths[np.argmin(validation_losses)]\n",
    "    best_tree = h.RegressionTree(depth=best_depth)\n",
    "    best_tree.fit(xTr, yTr)\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    return best_depth, training_losses, validation_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f792406e372f9c42",
     "locked": true,
     "points": 6,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "def grid_search_grader(xTr, yTr, xVal, yVal, depths):\n",
    "    '''\n",
    "    Input:\n",
    "        xTr: nxd matrix\n",
    "        yTr: n vector\n",
    "        xVal: mxd matrix\n",
    "        yVal: m vector\n",
    "        depths: a list of len k\n",
    "    Return:\n",
    "        best_depth: the depth that yields that lowest loss on the validation set\n",
    "        training losses: a list of len k. the i-th entry corresponds to the the training loss\n",
    "                the tree of depths[i]\n",
    "        validation_losses: a list of len k. the i-th entry corresponds to the the validation loss\n",
    "                the tree of depths[i]\n",
    "    '''\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "    best_depth = None\n",
    "    \n",
    "\n",
    "    for i in depths:\n",
    "        tree = h.RegressionTree(i)\n",
    "        tree.fit(xTr, yTr)\n",
    "        \n",
    "        training_loss = square_loss(tree.predict(xTr), yTr)\n",
    "        validation_loss = square_loss(tree.predict(xVal), yVal)\n",
    "        training_losses.append(training_loss)\n",
    "        validation_losses.append(validation_loss)\n",
    "    \n",
    "    best_depth = depths[np.argmin(validation_losses)]\n",
    "\n",
    "    return best_depth, training_losses, validation_losses\n",
    "\n",
    "depths = [1,2,3,4,5]\n",
    "k = len(depths)\n",
    "best_depth, training_losses, validation_losses = grid_search(xTr, yTr, xTe, yTe, depths)\n",
    "best_depth_grader, training_losses_grader, validation_losses_grader = grid_search_grader(xTr, yTr, xTe, yTe, depths)\n",
    "\n",
    "\n",
    "# Check the length of the training loss\n",
    "def grid_search_test1(training_losses, k):\n",
    "    return (len(training_losses) == k) \n",
    "\n",
    "# Check the length of the validation loss\n",
    "def grid_search_test2(validation_losses, k):\n",
    "    return (len(validation_losses) == k)\n",
    "\n",
    "# Check the argmin\n",
    "def grid_search_test3(best_depth, validation_losses):\n",
    "    return (best_depth == depths[np.argmin(validation_losses)])\n",
    "\n",
    "def grid_search_test4(best_depth, best_depth_grader):\n",
    "    return (best_depth == best_depth_grader)\n",
    "\n",
    "def grid_search_test5(training_losses, training_losses_grader):\n",
    "    return np.linalg.norm(np.array(training_losses) - np.array(training_losses_grader)) < 1e-7\n",
    "\n",
    "def grid_search_test6(validation_losses, validation_losses_grader):\n",
    "    return np.linalg.norm(np.array(validation_losses) - np.array(validation_losses_grader)) < 1e-7\n",
    "\n",
    "assert grid_search_test1(training_losses, k), \"[Failed] grid_search: the len(training_losses) != len(depths)\"\n",
    "assert grid_search_test2(validation_losses, k), \"[Failed] grid_search: the len(validation_losses) != len(depths)\"\n",
    "assert grid_search_test3(best_depth, validation_losses), \"[Failed] grid_search: Your best depth is not the minimizer of your validation loss\" \n",
    "assert grid_search_test4(best_depth, best_depth_grader), \"[Failed] grid_search: Your best depth does not match the optimal max depth!\"\n",
    "assert grid_search_test5(training_losses, training_losses_grader), \"[Failed] grid_search: Some of your training losses are not right!\"\n",
    "assert grid_search_test6(validation_losses, validation_losses_grader), \"[Failed] grid search: Some of your validation losses are not right\"\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-302726897bf1f3cf",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "kfold_indices = generate_kFold(1004, 5)\n",
    "\n",
    "def generate_kFold_test1(kfold_indices):\n",
    "    return len(kfold_indices) == 5\n",
    "\n",
    "def generate_kFold_test2(kfold_indices):\n",
    "    t = [((len(train_indices) + len(test_indices)) == 1004) for (train_indices, test_indices) in kfold_indices]\n",
    "    return np.all(t)\n",
    "\n",
    "def generate_kFold_test3(kfold_indices):\n",
    "    ratio_test = []\n",
    "    for (train_indices, validation_indices) in kfold_indices:\n",
    "        ratio = len(validation_indices) / len(train_indices)\n",
    "        ratio_test.append((ratio > 0.24 and ratio < 0.26))\n",
    "    return np.all(ratio_test)\n",
    "\n",
    "assert generate_kFold_test1(kfold_indices), \"[Failed] generate_kFold: You did not generate k folds!\"\n",
    "assert generate_kFold_test2(kfold_indices), \"[Failed] generate_kFold: The length of your train indices and validation indices does not add up!\"\n",
    "assert generate_kFold_test3(kfold_indices), \"[Failed] generate_kFold: The ratio len(validation_indices) / len(train_indices) is incorrect\"\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-16e8fa4519bb747d",
     "locked": true,
     "points": 6,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "def cross_validation_grader(xTr, yTr, depths, indices):\n",
    "    '''\n",
    "    Input:\n",
    "        xTr: nxd matrix (training data)\n",
    "        yTr: n vector (training data)\n",
    "        depths: a list of len k\n",
    "        indices: indices from generate_kFold\n",
    "    Returns:\n",
    "        best_depth: the best parameter \n",
    "        training losses: a list of len k. the i-th entry corresponds to the the average training loss\n",
    "                the tree of depths[i]\n",
    "        validation_losses: a list of len k. the i-th entry corresponds to the the average validation loss\n",
    "                the tree of depths[i] \n",
    "    '''\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "    best_depth = None\n",
    "    \n",
    "    for train_indices, validation_indices in indices:\n",
    "        xtrain, ytrain = xTr[train_indices], yTr[train_indices]\n",
    "        xval, yval = xTr[validation_indices], yTr[validation_indices]\n",
    "        \n",
    "        _, training_loss, validation_loss = grid_search(xtrain, ytrain, xval, yval, depths)\n",
    "        \n",
    "        training_losses.append(training_loss)\n",
    "        validation_losses.append(validation_loss)\n",
    "    \n",
    "    training_losses = np.mean(training_losses, axis=0)\n",
    "    validation_losses = np.mean(validation_losses, axis=0)\n",
    "    \n",
    "    best_depth = depths[np.argmin(validation_losses)]\n",
    "    best_tree = h.RegressionTree(depth=best_depth)\n",
    "    best_tree.fit(xTr, yTr)\n",
    "    \n",
    "    return best_depth, training_losses, validation_losses\n",
    "\n",
    "depths = [1,2,3,4,5]\n",
    "k = len(depths)\n",
    "indices = generate_kFold(len(xTr), 5)\n",
    "best_depth, training_losses, validation_losses = cross_validation(xTr, yTr, depths, indices)\n",
    "best_depth_grader, training_losses_grader, validation_losses_grader = cross_validation_grader(xTr, yTr, depths, indices)\n",
    "\n",
    "\n",
    "# Check the length of the training loss\n",
    "def cross_validation_test1(training_losses, k):\n",
    "    return (len(training_losses) == k) \n",
    "\n",
    "# Check the length of the validation loss\n",
    "def cross_validation_test2(validation_losses, k):\n",
    "    return (len(validation_losses) == k)\n",
    "\n",
    "# Check the argmin\n",
    "def cross_validation_test3(best_depth, validation_losses):\n",
    "    return (best_depth == depths[np.argmin(validation_losses)])\n",
    "\n",
    "def cross_validation_test4(best_depth, best_depth_grader):\n",
    "    return (best_depth == best_depth_grader)\n",
    "\n",
    "def cross_validation_test5(training_losses, training_losses_grader):\n",
    "    return np.linalg.norm(np.array(training_losses) - np.array(training_losses_grader)) < 1e-7\n",
    "\n",
    "def cross_validation_test6(validation_losses, validation_losses_grader):\n",
    "    return np.linalg.norm(np.array(validation_losses) - np.array(validation_losses_grader)) < 1e-7\n",
    "\n",
    "assert cross_validation_test1(training_losses, k), \"[Failed] cross_validation: the len(training_losses) != len(depths)\"\n",
    "assert cross_validation_test2(validation_losses, k), \"[Failed] cross_validation: the len(validation_losses) != len(depths)\"\n",
    "assert cross_validation_test3(best_depth, validation_losses), \"[Failed] cross_validation: Your best depth is not the minimizer of your validation loss\" \n",
    "assert cross_validation_test4(best_depth, best_depth_grader), \"[Failed] cross_validation: Your best depth does not match the optimal max depth!\"\n",
    "assert cross_validation_test5(training_losses, training_losses_grader), \"[Failed] cross_validation: Some of your training losses are not right!\"\n",
    "assert cross_validation_test6(validation_losses, validation_losses_grader), \"[Failed] cross_validation: Some of your validation losses are not right\"\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
