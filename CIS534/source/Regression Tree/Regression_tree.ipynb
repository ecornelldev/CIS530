{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-202739b8f3a67536",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2>About this Project</h2>\n",
    "<p>In this project, you will implement a decision tree algorithm.</p>\n",
    "\n",
    "<h3>Evaluation</h3>\n",
    "\n",
    "<p>You are expected to write code where you see <em># YOUR CODE HERE</em> within the cells of this notebook. Upon submitting your work, the code you write at these designated positions will be assessed using an \"autograder\" that will run a series of tests on your code. You will receive instant feedback from the autograder that will identify issues with and errors in your code. Use this feedback to improve your code if you need to resubmit. Be sure not to change the names of any provided functions, classes, or variables within the existing code cells, as this will interfere with the autograder. Also, remember to execute all code cells, not just those you’ve edited, to ensure the code runs properly.</p>\n",
    "    \n",
    "<p>You can resubmit your work as many times as necessary before the submission deadline. If you experience difficulty or have questions about this exercise, use the Q&A discussion board to engage with your peers or seek assistance from the instructor.<p>\n",
    "\n",
    "<p><strong>This exercise must be successfully completed in order to receive credit for this course.</strong><p>\n",
    "\n",
    "<p>Before starting your work, please review <a href=\"https://s3.amazonaws.com/ecornell/global/eCornellPlagiarismPolicy.pdf\">eCornell's policy regarding plagiarism</a> (the presentation of someone else's work as your own without source credit).</p>\n",
    "\n",
    "<h3>Submit Code for Autograder/Instructor Feedback</h3>\n",
    "\n",
    "<p>Once you have completed your work on this notebook, you will submit your code for autograder/instructor review. Follow these steps:</p>\n",
    "\n",
    "<ol>\n",
    "<li>Save your notebook. Though the system should automatically save your progress, you should ensure the latest version of your work is saved before submitting. </li>\n",
    "  <li>In the blue menu bar along the top of the code exercise window, you’ll see a menu item called <strong>Education</strong>. In the <strong>Education</strong> menu, click <strong>Mark as Completed</strong> to submit your code for autograder/instructor review. This process will take a moment and a progress bar will show you the status of your submission.</li>\n",
    "\t<li>Once your work is marked as complete, the results of the autograder will automatically be presented in a new tab within the code exercise window. You can click on the assessment name in this feedback window to see more details regarding specific feedback/errors in your code submission.</li>\n",
    "  <li>The Jupyter notebook will always remain accessible in the first tabbed window of the exercise. To reattempt the work, you will first need to click <strong>Mark as Uncompleted</strong> in the <strong>Education</strong> menu and then proceed to make edits to the notebook. Once you are ready to resubmit, follow steps one through three. You can repeat this procedure as many times as necessary.</li>\n",
    "</ol>\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Structure\n",
    "\n",
    "<p>We've provided a tree structure for you with distinct leaves and nodes. Leaves have two fields, parent (another node) and prediction (a numerical value). Nodes have six fields:</p> \n",
    "\n",
    "<ol>\n",
    "<li> <b>left</b>: node describing left subtree </li>\n",
    "<li> <b>right</b>: node describing right subtree </li>\n",
    "<li> <b>parent</b>: the parent of the current subtree. The head of the tree always has <code><b>None</b></code> as its parent. Feel free to initialize nodes with this field set to <code><b>None</b></code> as long as you set the correct parent later on. </li>\n",
    "<li> <b>cutoff_id</b>: index of feature to cut </li>\n",
    "<li> <b>cutoff_val</b>: cutoff value c (<=c : left, and >c : right)</li>\n",
    "<li> <b>prediction</b>: prediction at this node </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-919899ceb491184f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "class TreeNode(object):\n",
    "    \"\"\"Tree class.\n",
    "    \n",
    "    (You don't need to add any methods or fields here but feel\n",
    "    free to if you like. The tests will only reference the fields\n",
    "    defined in the constructor below, so be sure to set these\n",
    "    correctly.)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, left, right, parent, cutoff_id, cutoff_val, prediction):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.parent = parent\n",
    "        self.cutoff_id = cutoff_id\n",
    "        self.cutoff_val = cutoff_val\n",
    "        self.prediction = prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-727395f9bc8b5c9d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Implementing CART</h3>\n",
    "\n",
    "Before you get started, let's import a few packages that you will need. You will also load a data set <a href=\"https://archive.ics.uci.edu/ml/datasets/Ionosphere\">ION</a>, which you will use for our binary test classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5445d0afdbc4c054",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pylab import *\n",
    "from numpy.matlib import repmat\n",
    "import sys\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "import time\n",
    "%matplotlib notebook\n",
    "\n",
    "# load in some binary test data (labels are -1, +1)\n",
    "data = loadmat(\"ion.mat\")\n",
    "\n",
    "# Load the training data\n",
    "xTrIon  = data['xTr'].T\n",
    "yTrIon  = data['yTr'].flatten()\n",
    "\n",
    "# Load the test data\n",
    "xTeIon  = data['xTe'].T\n",
    "yTeIon  = data['yTe'].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-793ddbc6a9a57142",
     "locked": true,
     "points": 0,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "Hidden Instructor Code\n",
    "### BEGIN HIDDEN TESTS\n",
    "# Instructor Code\n",
    "def DFSxor(t):\n",
    "    if t.left is not None and t.right is not None:\n",
    "        if not np.isclose(t.prediction, 0.5):\n",
    "            return False\n",
    "        else:\n",
    "            return DFSxor(t.left) and DFSxor(t.right)\n",
    "    else:\n",
    "        return np.isclose(t.prediction, 0.) or np.isclose(t.prediction, 1.)\n",
    "\n",
    "def DFSpreds(t):\n",
    "    if t.left is not None and t.right is not None:\n",
    "        return np.concatenate((DFSpreds(t.left), DFSpreds(t.right)))\n",
    "    else:\n",
    "        return np.array([t.prediction])\n",
    "\n",
    "def DFSxorUnsplittable(t, depth=0):\n",
    "    if t.left is not None and t.right is not None:\n",
    "        if not np.isclose(t.prediction, 0.5):\n",
    "            return False\n",
    "        else:\n",
    "            return DFSxorUnsplittable(t.left, depth=depth + 1) and DFSxorUnsplittable(t.right, depth=depth + 1)\n",
    "    else:\n",
    "        return np.isclose(t.prediction, 0.5) and depth < 3\n",
    "\n",
    "\n",
    "xor2 = np.array([[1, 1, 0, 0],\n",
    "                 [1, 0, 1, 0]]).T\n",
    "yor2 = np.array( [1, 0, 0, 1])\n",
    "\n",
    "xor3 = np.array([[1, 1, 1, 1, 0, 0, 0, 0],\n",
    "                 [1, 1, 0, 0, 1, 1, 0, 0],\n",
    "                 [1, 0, 1, 0, 1, 0, 1, 0]]).T\n",
    "yor3 = np.array( [1, 0, 0, 1, 0, 1, 1, 0])\n",
    "\n",
    "xor4 = np.array([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                 [1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0],\n",
    "                 [1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0],\n",
    "                 [1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0]]).T\n",
    "yor4 = np.array( [1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1])\n",
    "\n",
    "xor5 = np.array([[0],[1],[1]])\n",
    "yor5 = np.array([1,1,-1])\n",
    "\n",
    "def sqsplit_grader(xTr,yTr,weights=[]):\n",
    "    \"\"\"Finds the best feature, cut value, and loss value.\n",
    "    \n",
    "    Input:\n",
    "        xTr:     n x d matrix of data points\n",
    "        yTr:     n-dimensional vector of labels\n",
    "        weights: n-dimensional weight vector for data points\n",
    "    \n",
    "    Output:\n",
    "        feature:  index of the best cut's feature\n",
    "        cut:      cut-value of the best cut\n",
    "        bestloss: loss of the best cut\n",
    "    \"\"\"\n",
    "    N,D = xTr.shape\n",
    "    assert D > 0 # must have at least one dimension\n",
    "    assert N > 1 # must have at least two samples\n",
    "    if weights == []: # if no weights are passed on, assign uniform weights\n",
    "        weights = np.ones(N)\n",
    "    weights = weights/sum(weights) # Weights need to sum to one (we just normalize them)\n",
    "    bestloss = np.inf\n",
    "    feature = np.inf\n",
    "    cut = np.inf\n",
    "    \n",
    "    for d in range(D):\n",
    "        ii = xTr[:,d].argsort() # sort data along that dimensions\n",
    "        xs = xTr[ii,d] # sorted feature values\n",
    "        ws = weights[ii] # sorted weights\n",
    "        ys = yTr[ii] # sorted labels\n",
    "        \n",
    "        # Initialize constants\n",
    "        sL=0.0 # mean squared label on left side\n",
    "        muL=0.0 # mean label on left side\n",
    "        wL=0.0 # total weight on left side\n",
    "        sR=ws.dot(ys**2) #mean squared label on right \n",
    "        muR=ws.dot(ys) # mean label on right\n",
    "        wR=sum(ws) # weight on right\n",
    "        \n",
    "        idif = np.where(np.abs(np.diff(xs, axis=0)) > np.finfo(float).eps * 100)[0]\n",
    "        pj = 0\n",
    "        \n",
    "        for j in idif:\n",
    "            deltas = np.dot(ys[pj:j+1]**2, ws[pj:j+1])\n",
    "            deltamu = np.dot(ws[pj:j+1], ys[pj:j+1])\n",
    "            deltaw = np.sum(ws[pj:j+1])\n",
    "            \n",
    "            sL += deltas\n",
    "            muL += deltamu\n",
    "            wL += deltaw\n",
    "\n",
    "            sR -= deltas\n",
    "            muR -= deltamu\n",
    "            wR -= deltaw\n",
    "            \n",
    "            L = sL - muL**2 / wL\n",
    "            R = sR - muR**2 / wR\n",
    "            loss = L + R\n",
    "            \n",
    "            if loss < bestloss:\n",
    "                feature = d\n",
    "                cut = (xs[j]+xs[j+1])/2\n",
    "                bestloss = loss\n",
    "            \n",
    "            pj = j + 1\n",
    "        \n",
    "    assert feature != np.inf and cut != np.inf\n",
    "    \n",
    "    return feature, cut, bestloss\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9a38f0686e9d323f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def spiraldata(N=300):\n",
    "    r = np.linspace(1,2*np.pi,N)\n",
    "    xTr1 = np.array([np.sin(2.*r)*r, np.cos(2*r)*r]).T\n",
    "    xTr2 = np.array([np.sin(2.*r+np.pi)*r, np.cos(2*r+np.pi)*r]).T\n",
    "    xTr = np.concatenate([xTr1, xTr2], axis=0)\n",
    "    yTr = np.concatenate([np.ones(N), -1 * np.ones(N)])\n",
    "    xTr = xTr + np.random.randn(xTr.shape[0], xTr.shape[1])*0.2\n",
    "    \n",
    "    xTe = xTr[::2,:]\n",
    "    yTe = yTr[::2]\n",
    "    xTr = xTr[1::2,:]\n",
    "    yTr = yTr[1::2]\n",
    "    \n",
    "    return xTr,yTr,xTe,yTe\n",
    "\n",
    "xTrSpiral,yTrSpiral,xTeSpiral,yTeSpiral=spiraldata(150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Implement Regression Trees </h2>\n",
    "\n",
    "### Part One [Graded]\n",
    "\n",
    "<p>First, implement the function <code>sqsplit</code>, which takes as input a (weighted) data set with labels and computes the best feature and cut-value of an optimal split based on minimum squared error. The third input is a weight vector which assigns a positive weight to each training sample. The loss you should minimize is the averaged weighted squared-loss:\n",
    "$$\n",
    "\t{\\cal L}(S)=\\sum_{i \\in L} {w_{i}(y_{i} - T_{L})}^2+\\sum_{i \\in R} {w_{i}(y_{i} - T_{R})}^2.\\label{q2:loss}\n",
    "$$\n",
    "<br>\n",
    "</p>\n",
    "You are building a regression tree and right now you need to choose a split for the given dataset $S=\\{(\\vec x_1,y_1),\\dots,(\\vec x_n,y_n)\\}$ (where there are continuous labels $y_i\\in{\\cal R}$). Suppose you split on some feature $j$ with value $c$ and partition the dataset in to two sets of indices, $L$--the set of indices on the left (i.e., $i \\in L \\Rightarrow [x_{i}]_{j} < c$)--and $R$--the set of indices on the right (i.e., $i \\in R \\Rightarrow [x_{i}]_{j} > c$). Suppose you assign every data point on the left the prediction $T_{L}$ and every data point on the right the prediction $T_{R}$. Finally, suppose that each data point $x_{i}$ has an associated weight $w_{i}$, and that the weights are normalized (i.e., $\\sum_{i} w_{i} = 1$). \n",
    "\n",
    "\n",
    "<p> Now, you show that setting $T_{L}$ and $T_{R}$ to the weighted average label over their respective sets (i.e., $T_{L} = \\frac{1}{W_{L}}\\sum_{i\\in L}w_{i}y_{i}$ and $T_{R} = \\frac{1}{W_{R}}\\sum_{i\\in R}w_{i}y_{i}$) minimizes the loss $\\cal L$, where $W_{L}=\\sum_{i \\in L}w_{i}$ and $W_{R}=\\sum_{i \\in R} w_{i}$ are the total weight of the left and right side respectively.\n",
    "\n",
    "<p> You take the derivative of the loss with respect to $T_{L}$ to obtain $$\\frac{d}{dT_{L}} {\\cal L}(S) = -2\\sum_{i \\in L}w_{i}(y_i - T_L)=-2\\sum_{i\\in L}w_iy_i + 2T_{L}\\sum_{i}w_{i}$$ Setting this equal to zero and solving, we get $$2T_{L}w_{L}=2\\sum_{i \\in L}w_{i}y_{i}$$ and therefore $$T_{L} = \\frac{1}{W_{L}}\\sum_{i \\in L}w_{i}y_{i}$$ A symmetric argument holds for $T_{R}$.</p>\n",
    "\n",
    "So if you know the split, setting $T_{L}$ and $T_{R}$ to the weighted average label over their respective sets minimizes the loss. To find the best split, you can evaluate all possible splits and then search for the split that yields the minimum loss which you will in <code>sqsplit</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9963791851b241ca",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def sqsplit(xTr,yTr,weights=None):\n",
    "    \"\"\"Finds the best feature, cut value, and loss value.\n",
    "    \n",
    "    Input:\n",
    "        xTr:     n x d matrix of data points\n",
    "        yTr:     n-dimensional vector of labels\n",
    "        weights: n-dimensional weight vector for data points\n",
    "    \n",
    "    Output:\n",
    "        feature:  index of the best cut's feature\n",
    "        cut:      cut-value of the best cut\n",
    "        bestloss: loss of the best cut\n",
    "    \"\"\"\n",
    "    N,D = xTr.shape\n",
    "    assert D > 0 # must have at least one dimension\n",
    "    assert N > 1 # must have at least two samples\n",
    "    if weights is None: # if no weights are passed on, assign uniform weights\n",
    "        weights = np.ones(N)\n",
    "    weights = weights/sum(weights) # Weights need to sum to one (we just normalize them)\n",
    "    bestloss = np.inf\n",
    "    feature = np.inf\n",
    "    cut = np.inf\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    for d in range(D):\n",
    "        ii = xTr[:, d].argsort() # sort data along that dimensions\n",
    "        xs = xTr[ii, d] # sorted feature values\n",
    "        ws = weights[ii] # sorted weights\n",
    "        ys = yTr[ii] # sorted labels\n",
    "        \n",
    "        # Get indices where we should split\n",
    "        idif = np.where(np.logical_not(np.isclose(np.diff(xs, axis=0), 0)))[0]\n",
    "        \n",
    "        for j in idif:\n",
    "            T_L = (ws[:j + 1] @ ys[:j + 1]) / np.sum(ws[:j + 1])\n",
    "            T_R = (ws[j + 1: ] @ ys[j + 1:]) / np.sum(ws[j + 1:])\n",
    "            \n",
    "            left_loss = ((ys[:j + 1] - T_L)**2) @ ws[:j + 1]\n",
    "            right_loss = ((ys[j + 1:] - T_R)**2) @ ws[j + 1:]\n",
    "            \n",
    "            loss = left_loss + right_loss            \n",
    "            if loss < bestloss:\n",
    "                feature = d\n",
    "                cut = (xs[j]+xs[j+1])/2\n",
    "                bestloss = loss\n",
    "        \n",
    "    assert feature != np.inf and cut != np.inf\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    return feature, cut, bestloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-37c1ab4899a4882f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "fid,cut,loss = sqsplit(xTrIon,yTrIon)\n",
    "t1 = time.time()\n",
    "\n",
    "# change \n",
    "print('Elapsed time: {:0.2f} seconds'.format(t1-t0))\n",
    "print(\"The best split is on feature 2 on value 0.304\")\n",
    "print(\"Your tree split on feature %i on value: %2.3f\" % (fid,cut))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1dde8387ff8bcee5",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#test case 1\n",
    "def sqsplit_test1():\n",
    "    a = np.isclose(sqsplit(xor4,yor4)[2], .25)\n",
    "    b = np.isclose(sqsplit(xor3,yor3)[2], .25)\n",
    "    c = np.isclose(sqsplit(xor2,yor2)[2], .25)\n",
    "    return a and b and c\n",
    "\n",
    "#test case 2\n",
    "def sqsplit_test2():\n",
    "    x = np.array(range(1000)).reshape(-1,1)\n",
    "    y = np.hstack([np.ones(500),-1*np.ones(500)]).T\n",
    "    w = np.ones(1000)\n",
    "    _, cut, _ = sqsplit(x, y, weights=w)\n",
    "    # 499.5 is the correct answer but will accept other two answers\n",
    "    return cut == 499 or cut == 500 or cut == 499.5\n",
    "\n",
    "#test case 3\n",
    "def sqsplit_test3():\n",
    "    fid,cut,loss = sqsplit(xor5,yor5)\n",
    "    # cut should be 0.5 but 0 is also accepted\n",
    "    return fid == 0 and (cut == 0 or cut == 0.5) and np.isclose(loss, 2/3)\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert sqsplit_test1(), \"[FAIL] sqsplit Test 1 - calculated bestloss is inconsistent across similar datasets.\"\n",
    "assert sqsplit_test2(), \"[FAIL] sqsplit Test 2 - calculated best cut location is not showing expected behavior for large datasets.\"\n",
    "assert sqsplit_test3(), \"[FAIL] sqsplit Test 3 - did you split in the middle of a feature? e.g. [1,1,1,2,2] should not be splitted to [1,1], [1,2,2].\"\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-985b43cf6a0b1e16",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part Two [Graded]\n",
    "\n",
    "Implement the function <code>cart</code>, which returns a regression tree based on the minimum squared loss splitting rule. The function takes training data, a maximum depth, and the weight of each training example. Maximum depth and weight are optional arguments. If they are not provided you should set the maximum depth to infinity and equally weight each example. You should use the function <code>sqsplit</code> to make your splits.</p>\n",
    "\n",
    "<p>Use the provided <code>TreeNode</code> class to represent your tree. Note that the nature of CART trees implies that every node has exactly 0 or 2 children.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fa49d74a4ed998aa",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def cart(xTr,yTr,depth=np.inf,weights=None):\n",
    "    \"\"\"Builds a CART tree.\n",
    "    \n",
    "    The maximum tree depth is defined by \"maxdepth\" (maxdepth=2 means one split).\n",
    "    Each example can be weighted with \"weights\".\n",
    "\n",
    "    Args:\n",
    "        xTr:      n x d matrix of data\n",
    "        yTr:      n-dimensional vector\n",
    "        maxdepth: maximum tree depth\n",
    "        weights:  n-dimensional weight vector for data points\n",
    "\n",
    "    Returns:\n",
    "        tree: root of decision tree\n",
    "    \"\"\"\n",
    "    n,d = xTr.shape\n",
    "    if weights is None:\n",
    "        w = np.ones(n) / float(n)\n",
    "    else:\n",
    "        w = weights\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    index = np.arange(n)\n",
    "    prediction = yTr.dot(w) / float(np.sum(w))\n",
    "    if depth == 0 or np.all(yTr == yTr[0]) or np.max(np.abs(np.diff(xTr, axis=0))) < (np.finfo(float).eps * 100):\n",
    "        # Create leaf Node\n",
    "        return TreeNode(None, None, None, None, None, prediction)\n",
    "    else:\n",
    "        feature,cut,h = sqsplit(xTr,yTr,w)\n",
    "        left_idx  = index[xTr[:,feature] <= cut]\n",
    "        right_idx = index[xTr[:,feature] > cut]\n",
    "        \n",
    "        left_w  = w[left_idx]\n",
    "        right_w = w[right_idx]\n",
    "        left  = cart(xTr[left_idx,:],   yTr[left_idx],  depth=depth-1, weights=left_w)\n",
    "        right = cart(xTr[right_idx,:],  yTr[right_idx], depth=depth-1, weights=right_w)\n",
    "        currNode = TreeNode(left, right, None, feature, cut, prediction)\n",
    "        left.parent  = currNode\n",
    "        right.parent = currNode\n",
    "        \n",
    "        return currNode\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-43ae33c0334f4bba",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#test case 1\n",
    "def cart_test1():\n",
    "    t=cart(xor4,yor4)\n",
    "    return DFSxor(t)\n",
    "\n",
    "#test case 2\n",
    "def cart_test2():\n",
    "    y = np.random.rand(16);\n",
    "    t = cart(xor4,y);\n",
    "    yTe = DFSpreds(t)[:];\n",
    "    # Check that every label appears exactly once in the tree\n",
    "    y.sort()\n",
    "    yTe.sort()\n",
    "    return np.all(np.isclose(y, yTe))\n",
    "\n",
    "#test case 3\n",
    "def cart_test3():\n",
    "    xRep = np.concatenate([xor2, xor2])\n",
    "    yRep = np.concatenate([yor2, 1-yor2])\n",
    "    t = cart(xRep, yRep)\n",
    "    return DFSxorUnsplittable(t)\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert cart_test1(), \"[FAIL] cart Test 1 - at least one prediction is incorrect.\"\n",
    "assert cart_test2(), \"[FAIL] cart Test 2 - tree is failing to split the way it should\"\n",
    "assert cart_test3(), \"[FAIL] cart Test 3 - tree is failing to split the way it should.\"\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-91bac7e5c5968bbf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part Three [Graded]\n",
    "\n",
    "<p>Implement the function <code>evaltree</code>, which evaluates a decision tree on a given test data set.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-70a1836e2c912bda",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def evaltree(root,xTe):\n",
    "    \"\"\"Evaluates xTe using decision tree root.\n",
    "    \n",
    "    Input:\n",
    "        root: TreeNode decision tree\n",
    "        xTe:  n x d matrix of data points\n",
    "    \n",
    "    Output:\n",
    "        pred: n-dimensional vector of predictions\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    def evaltreehelper(root,xTe, idx=[]):\n",
    "        \"\"\"Evaluates xTe using decision tree root.\n",
    "\n",
    "        Input:\n",
    "            root: TreeNode decision tree\n",
    "            xTe:  n x d matrix of data points\n",
    "\n",
    "        Output:\n",
    "            pred: n-dimensional vector of predictions\n",
    "        \"\"\"\n",
    "        assert root is not None\n",
    "        n = xTe.shape[0]\n",
    "        pred = np.zeros(n)\n",
    "\n",
    "        # TODO:\n",
    "        if len(idx)==0: idx=np.ones(n)==1 \n",
    "\n",
    "        if root.left is None and root.right is None:\n",
    "             return np.ones(sum(idx))*root.prediction\n",
    "            \n",
    "        assert root.left is not None and root.right is not None\n",
    "        feature, cutoff = root.cutoff_id, root.cutoff_val\n",
    "\n",
    "        idxL=idx & (xTe[:,feature] <= cutoff)\n",
    "        if root.left.left is None and root.left.right is None:\n",
    "             pred[idxL]=root.left.prediction\n",
    "        else:\n",
    "             pred[idxL]=evaltreehelper(root.left, xTe,idxL) \n",
    "\n",
    "        idxR=idx & (xTe[:,feature]  > cutoff)\n",
    "        if root.right.left is None and root.right.right is None:\n",
    "             pred[idxR]=root.right.prediction\n",
    "        else:\n",
    "             pred[idxR]=evaltreehelper(root.right,xTe,idxR)\n",
    "        return(pred[idx])\n",
    "    \n",
    "    return evaltreehelper(root,xTe)\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5d403bb97098920e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "root = cart(xTrIon, yTrIon)\n",
    "t1 = time.time()\n",
    "\n",
    "tr_err   = np.mean((evaltree(root,xTrIon) - yTrIon)**2)\n",
    "te_err   = np.mean((evaltree(root,xTeIon) - yTeIon)**2)\n",
    "\n",
    "print(\"Elapsed time: %.2f seconds\" % (t1-t0))\n",
    "print(\"Training RMSE : %.2f\" % tr_err)\n",
    "print(\"Testing  RMSE : %.2f\" % te_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b049b9945af6b250",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#test case 1\n",
    "def evaltree_test1():\n",
    "    t = cart(xor4,yor4)\n",
    "    xor4te = xor4 + (np.sign(xor4 - .5) * .1)\n",
    "    inds = np.arange(16)\n",
    "    np.random.shuffle(inds)\n",
    "    # Check that shuffling and expanding the data doesn't affect the predictions\n",
    "    return np.all(np.isclose(evaltree(t, xor4te[inds,:]), yor4[inds]))\n",
    "\n",
    "#test case 2\n",
    "def evaltree_test2():\n",
    "    a = TreeNode(None, None, None, None, None, 1)\n",
    "    b = TreeNode(None, None, None, None, None, -1)\n",
    "    c = TreeNode(None, None, None, None, None, 0)\n",
    "    d = TreeNode(None, None, None, None, None, -1)\n",
    "    e = TreeNode(None, None, None, None, None, -1)\n",
    "    x = TreeNode(a, b, None, 0, 10, 0)\n",
    "    y = TreeNode(x, c, None, 0, 20, 0)\n",
    "    z = TreeNode(d, e, None, 0, 40, 0)\n",
    "    t = TreeNode(y, z, None, 0, 30, 0)\n",
    "    # Check that the custom tree evaluates correctly\n",
    "    return np.all(np.isclose(\n",
    "            evaltree(t, np.array([[45, 35, 25, 15, 5]]).T),\n",
    "            np.array([-1, -1, 0, -1, 1])))\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert evaltree_test1(), \"[FAIL] evaltree Test 1 - tree is sensitive to irrelevant perturbations in the data.\"\n",
    "assert evaltree_test2(), \"[FAIL] evaltree Test 2 - tree is predicting incorrectly.\"\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-534ce4bb724b2f26",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<p>The following code defines a function <code>visclassifier()</code>, which plots the decision boundary of a classifier in 2 dimensions. Execute the following code to see what the decision boundary of your tree looks like on the ion data set. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8059df3cc7cdb39d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def visclassifier(fun,xTr,yTr,w=[],b=0):\n",
    "    \"\"\"\n",
    "    visualize decision boundary\n",
    "    Define the symbols and colors we'll use in the plots later\n",
    "    \"\"\"\n",
    "\n",
    "    yTr = np.array(yTr).flatten()\n",
    "    w = np.array(w).flatten()\n",
    "\n",
    "    symbols = [\"ko\",\"kx\"]\n",
    "    marker_symbols = ['o', 'x']\n",
    "    mycolors = [[0.5, 0.5, 1], [1, 0.5, 0.5]]\n",
    "    # get the unique values from labels array\n",
    "    classvals = np.unique(yTr)\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    # return 300 evenly spaced numbers over this interval\n",
    "    res=300\n",
    "    xrange = np.linspace(min(xTr[:, 0]), max(xTr[:, 0]),res)\n",
    "    yrange = np.linspace(min(xTr[:, 1]), max(xTr[:, 1]),res)\n",
    "    \n",
    "    # repeat this matrix 300 times for both axes\n",
    "    pixelX = repmat(xrange, res, 1)\n",
    "    pixelY = repmat(yrange, res, 1).T\n",
    "\n",
    "    \n",
    "    xTe = np.array([pixelX.flatten(), pixelY.flatten()]).T\n",
    "\n",
    "    # test all of these points on the grid\n",
    "    testpreds = fun(xTe)\n",
    "    \n",
    "    # reshape it back together to make our grid\n",
    "    Z = testpreds.reshape(res, res)\n",
    "    # Z[0,0] = 1 # optional: scale the colors correctly\n",
    "    \n",
    "    # fill in the contours for these predictions\n",
    "    plt.contourf(pixelX, pixelY, np.sign(Z), colors=mycolors)\n",
    "\n",
    "    # creates x's and o's for training set\n",
    "    for idx, c in enumerate(classvals):\n",
    "        plt.scatter(xTr[yTr == c,0],\n",
    "            xTr[yTr == c,1],\n",
    "            marker=marker_symbols[idx],\n",
    "            color='k'\n",
    "            )\n",
    "    \n",
    "    if w != []:\n",
    "        alpha = -1 * b / (w ** 2).sum()\n",
    "        plt.quiver(w[0] * alpha, w[1] * alpha,\n",
    "            w[0], w[1], linewidth=2, color=[0,1,0])\n",
    "\n",
    "    plt.axis('tight')\n",
    "    # shows figure and blocks\n",
    "    plt.show()\n",
    "\n",
    "tree=cart(xTrSpiral,yTrSpiral) # compute tree on training data \n",
    "visclassifier(lambda X:evaltree(tree,X),xTrSpiral,yTrSpiral)\n",
    "print(\"Training error: %.4f\" % np.mean(np.sign(evaltree(tree,xTrSpiral)) != yTrSpiral))\n",
    "print(\"Testing error:  %.4f\" % np.mean(np.sign(evaltree(tree,xTeSpiral)) != yTeSpiral))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8ad51604f6c3e28c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def onclick_cart(event):\n",
    "    \"\"\"\n",
    "    Visualize cart, including new point\n",
    "    \"\"\"\n",
    "    global xTraining,labels\n",
    "    # create position vector for new point\n",
    "    pos=np.array([[event.xdata,event.ydata]]) \n",
    "    if event.key == 'shift': # add positive point\n",
    "        color='or'\n",
    "        label=1\n",
    "    else: # add negative point\n",
    "        color='ob'\n",
    "        label=-1    \n",
    "    xTraining = np.concatenate((xTraining,pos), axis = 0)\n",
    "    labels.append(label);\n",
    "    marker_symbols = ['o', 'x']\n",
    "    classvals = np.unique(labels)\n",
    "        \n",
    "    mycolors = [[0.5, 0.5, 1], [1, 0.5, 0.5]]\n",
    "    \n",
    "    # return 300 evenly spaced numbers over this interval\n",
    "    res=300\n",
    "    xrange = np.linspace(0, 1,res)\n",
    "    yrange = np.linspace(0, 1,res)\n",
    "\n",
    "    \n",
    "    # repeat this matrix 300 times for both axes\n",
    "    pixelX = repmat(xrange, res, 1)\n",
    "    pixelY = repmat(yrange, res, 1).T\n",
    "\n",
    "    xTe = np.array([pixelX.flatten(), pixelY.flatten()]).T\n",
    "\n",
    "    # get decision tree\n",
    "    tree=cart(xTraining,np.array(labels).flatten())\n",
    "    fun = lambda X:evaltree(tree,X)\n",
    "    # test all of these points on the grid\n",
    "    testpreds = fun(xTe)\n",
    "    \n",
    "    # reshape it back together to make our grid\n",
    "    Z = testpreds.reshape(res, res)\n",
    "    # Z[0,0] = 1 # optional: scale the colors correctly\n",
    "    \n",
    "    plt.cla()    \n",
    "    plt.xlim((0,1))\n",
    "    plt.ylim((0,1))\n",
    "    # fill in the contours for these predictions\n",
    "    plt.contourf(pixelX, pixelY, np.sign(Z), colors=mycolors)\n",
    "    \n",
    "    for idx, c in enumerate(classvals):\n",
    "        plt.scatter(xTraining[labels == c,0],\n",
    "            xTraining[labels == c,1],\n",
    "            marker=marker_symbols[idx],\n",
    "            color='k'\n",
    "            )\n",
    "    plt.show()\n",
    "    \n",
    "        \n",
    "xTraining= np.array([[5,6]])\n",
    "labels = [1]\n",
    "%matplotlib notebook\n",
    "fig = plt.figure()\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "cid = fig.canvas.mpl_connect('button_press_event', onclick_cart)\n",
    "plt.title('Use shift-click to add negative points.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional linear time update rule \n",
    "# generate two gaussians as data that has a lot of features"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
