{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-138bf686eef190d3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2>About this Project</h2>\n",
    "<p> In this project, you are going to implement AdaBoost for Regression Trees.</p>\n",
    "\n",
    "<h3>Evaluation</h3>\n",
    "\n",
    "<p><strong>This project must be successfully completed and submitted in order to receive credit for this course. Your score on this project will be included in your final grade calculation.</strong><p>\n",
    "    \n",
    "<p>You are expected to write code where you see <em># YOUR CODE HERE</em> within the cells of this notebook. Not all cells will be graded; code input cells followed by cells marked with <em>#Autograder test cell</em> will be graded. Upon submitting your work, the code you write at these designated positions will be assessed using an \"autograder\" that will run all test cells to assess your code. You will receive feedback from the autograder that will identify any errors in your code. Use this feedback to improve your code if you need to resubmit. Be sure not to change the names of any provided functions, classes, or variables within the existing code cells, as this will interfere with the autograder. Also, remember to execute all code cells sequentially, not just those you’ve edited, to ensure your code runs properly.</p>\n",
    "    \n",
    "<p>You can resubmit your work as many times as necessary before the submission deadline. If you experience difficulty or have questions about this exercise, use the Q&A discussion board to engage with your peers or seek assistance from the instructor.<p>\n",
    "\n",
    "<p>Before starting your work, please review <a href=\"https://s3.amazonaws.com/ecornell/global/eCornellPlagiarismPolicy.pdf\">eCornell's policy regarding plagiarism</a> (the presentation of someone else's work as your own without source credit).</p>\n",
    "\n",
    "<h3>Submit Code for Autograder Feedback</h3>\n",
    "\n",
    "<p>Once you have completed your work on this notebook, you will submit your code for autograder review. Follow these steps:</p>\n",
    "\n",
    "<ol>\n",
    "  <li><strong>Save your notebook.</strong></li>\n",
    "  <li><strong>Mark as Completed —</strong> In the blue menu bar along the top of this code exercise window, you’ll see a menu item called <strong>Education</strong>. In the <strong>Education</strong> menu, click <strong>Mark as Completed</strong> to submit your code for autograder/instructor review. This process will take a moment and a progress bar will show you the status of your submission.</li>\n",
    "\t<li><strong>Review your results —</strong> Once your work is marked as complete, the results of the autograder will automatically be presented in a new tab within the code exercise window. You can click on the assessment name in this feedback window to see more details regarding specific feedback/errors in your code submission.</li>\n",
    "  <li><strong>Repeat, if necessary —</strong> The Jupyter notebook will always remain accessible in the first tabbed window of the exercise. To reattempt the work, you will first need to click <strong>Mark as Uncompleted</strong> in the <strong>Education</strong> menu and then proceed to make edits to the notebook. Once you are ready to resubmit, follow steps one through three. You can repeat this procedure as many times as necessary.</li>\n",
    "</ol>\n",
    "<p>You can also download a copy of this notebook in multiple formats using the <strong>Download as</strong> option in the <strong>File</strong> menu above.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started\n",
    "\n",
    "Before you get started, let's import a few packages that you will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cfd4728219aa2e6f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pylab import *\n",
    "from numpy.matlib import repmat\n",
    "import sys\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "import time\n",
    "from helper import *\n",
    "%matplotlib notebook\n",
    "\n",
    "print('You\\'re running python %s' % sys.version.split(' ')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6613495f7b984ddf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In addition, we'll create a 2D spiral dataset of size 150 for visualization and a high dimensional dataset <a href=\"https://archive.ics.uci.edu/ml/datasets/Ionosphere\">ION</a>, which we will use as our binary test classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ea9d14b9dc5c69e4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "xTrSpiral,yTrSpiral,xTeSpiral,yTeSpiral= spiraldata(150)\n",
    "xTrIon,yTrIon,xTeIon,yTeIon= iondata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a219c78df15f9ea0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<p>The following code defines a function <code>visclassifier()</code>, which plots the decision boundary of a classifier in 2 dimensions. Execute the following code to see what the decision boundary of your tree looks like on the spiral data set. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ea356e95d6f8a95d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def visclassifier(fun,xTr,yTr):\n",
    "    \"\"\"\n",
    "    visualize decision boundary\n",
    "    Define the symbols and colors we'll use in the plots later\n",
    "    \"\"\"\n",
    "\n",
    "    yTr = np.array(yTr).flatten()\n",
    "    \n",
    "    symbols = [\"ko\",\"kx\"]\n",
    "    marker_symbols = ['o', 'x']\n",
    "    mycolors = [[0.5, 0.5, 1], [1, 0.5, 0.5]]\n",
    "    # get the unique values from labels array\n",
    "    classvals = np.unique(yTr)\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    # return 300 evenly spaced numbers over this interval\n",
    "    res=300\n",
    "    xrange = np.linspace(min(xTr[:, 0]), max(xTr[:, 0]),res)\n",
    "    yrange = np.linspace(min(xTr[:, 1]), max(xTr[:, 1]),res)\n",
    "    \n",
    "    # repeat this matrix 300 times for both axes\n",
    "    pixelX = repmat(xrange, res, 1)\n",
    "    pixelY = repmat(yrange, res, 1).T\n",
    "\n",
    "    \n",
    "    xTe = np.array([pixelX.flatten(), pixelY.flatten()]).T\n",
    "\n",
    "    # test all of these points on the grid\n",
    "    testpreds = fun(xTe)\n",
    "    \n",
    "    # reshape it back together to make our grid\n",
    "    Z = testpreds.reshape(res, res)\n",
    "    # Z[0,0] = 1 # optional: scale the colors correctly\n",
    "    \n",
    "    # fill in the contours for these predictions\n",
    "    plt.contourf(pixelX, pixelY, np.sign(Z), colors=mycolors)\n",
    "\n",
    "    # creates x's and o's for training set\n",
    "    for idx, c in enumerate(classvals):\n",
    "        plt.scatter(xTr[yTr == c,0],\n",
    "            xTr[yTr == c,1],\n",
    "            marker=marker_symbols[idx],\n",
    "            color='k'\n",
    "                   )\n",
    "\n",
    "    plt.axis('tight')\n",
    "    # shows figure and blocks\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "tree=RegressionTree(depth=np.inf)\n",
    "tree.fit(xTrSpiral,yTrSpiral) # compute tree on training data \n",
    "visclassifier(lambda X: tree.predict(X),xTrSpiral,yTrSpiral)\n",
    "print(\"Training error: %.4f\" % np.mean(np.sign(tree.predict(xTrSpiral)) != yTrSpiral))\n",
    "print(\"Testing error:  %.4f\" % np.mean(np.sign(tree.predict(xTeSpiral)) != yTeSpiral))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-84ec95c505a4dbca",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2>Boosting in Action</h2>\n",
    "<h3>Part One: Implement <code>evalboostforest</code> [Graded]</h3>\n",
    "\n",
    "<p>Another option to improve your decision trees is to build trees of small depth (e.g. only depth=3 or depth=4). These do not have high variance, but instead suffer from <b>high bias</b>. You can reduce the bias of a classifier with boosting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a16f7968543b4118",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h4>Sample weights</h4>\n",
    "<p>Before we get started with boosting, we have to ensure that our classifier can handle data sets with <b>sample weights</b>. Luckily we have already implemented a regression tree that can incorporate weights of training examples. Consider you have training examples $(\\mathbf{x}_i, y_i)$ and you want to weigh each example with a positive weight $w_i$ during training. Then essentially what you want to do is to create a regression tree that minimizes:\n",
    "$$ \\frac{1}{Z}\\sum_{i=1}^n  w_i(tree(\\mathbf{x}_i) - y_i)^2 \\textrm{ where: } Z=\\sum_{j=1}^n w_j$$\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Note that the constant $\\frac{1}{Z}$ makes no practical difference and we include it only for mathematical convenience (this way weights are normalized and can be viewed as probabilities). \n",
    "Note that if we set the weights to be one for all training examples (i.e. $w_i = 1$ for every training examples), we recover the decision tree we implemented in the previous project. This extra weighting feature is useful you implement AdaBoost. If $w_1=5$ and $w_{i>1}=1$, the resulting tree should be identical to the tree we would have obtained if we had replicated the first example five times in and used the standard tree building algorithm. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The following cell is an example where we create a tree with randomly weighted training samples. Note that the training error should always be 0, but the weighing does have random effects on the test error. You can see this if you execute the code multiple times. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random weights\n",
    "# weights[i] is the weight for i-th training example\n",
    "weights = np.random.rand(len(xTrSpiral))\n",
    "\n",
    "# Train a regression tree with the prespecified weights\n",
    "# the default for weights is None which weigh all training examples equally\n",
    "tree = RegressionTree(depth=np.inf, weights=weights)\n",
    "\n",
    "# To fit/train the regression tree\n",
    "tree.fit(xTrSpiral, yTrSpiral)\n",
    "\n",
    "# To use the trained regression tree to make prediction\n",
    "pred = tree.predict(xTrSpiral)\n",
    "\n",
    "tr_err   = np.mean((np.sign(tree.predict(xTrSpiral)) - yTrSpiral)**2)\n",
    "te_err   = np.mean((np.sign(tree.predict(xTeSpiral)) - yTeSpiral)**2)\n",
    "\n",
    "print(\"Training error: %.4f\" % np.mean(np.sign(tree.predict(xTrSpiral)) != yTrSpiral))\n",
    "print(\"Testing error:  %.4f\" % np.mean(np.sign(tree.predict(xTeSpiral)) != yTeSpiral))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Weighted trees</h4>\n",
    "<p>The second change we need for Boosting is to evaluate <b>weighted trees</b>. Similar to bagging, in boosting we obtain ensembles of classifiers $h_1,\\dots,h_m$. However, this time we also  associated with weights $\\alpha_1,\\dots,\\alpha_m$ <i>and</i> we sum up the weighted <i>signs</i> of the predictions.  </p>\n",
    "<p>To be more precise, you should implement the function <code>evalboostforest</code>, which takes as input a set of $m$ trees, $h_1,\\dots,h_m$, a set of $m$ corresponding weights $\\alpha_1,\\dots,\\alpha_m$ and a set of $n$ test samples. For each test sample $\\mathbf{x}$ evalforest should output the following prediction\n",
    "$$H(\\mathbf{x})= \\sum_{i=1}^m \\alpha_i \\textrm{sign}(h_i(\\mathbf{x})).$$\n",
    "(We make the tree weights $\\alpha$ optional, and set them to $\\alpha_i=\\frac{1}{m}$ if they are not passed on.)\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalboostforest(trees, X, alphas=None):\n",
    "    \"\"\"Evaluates X using trees.\n",
    "    \n",
    "    Input:\n",
    "        trees:  list of TreeNode decision trees of length m\n",
    "        X:      n x d matrix of data points\n",
    "        alphas: m-dimensional weight vector\n",
    "        \n",
    "    Output:\n",
    "        pred: n-dimensional vector of predictions\n",
    "    \"\"\"\n",
    "    m = len(trees)\n",
    "    n,d = X.shape\n",
    "    \n",
    "    if alphas is None:\n",
    "        alphas = np.ones(m) / len(trees)\n",
    "            \n",
    "    pred = np.zeros(n)\n",
    "    \n",
    "    # BEGIN SOLUTION\n",
    "    for t in range(m):\n",
    "        pred += alphas[t] * np.sign(trees[t].predict(X))      \n",
    "\n",
    "    # END SOLUTION\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalboostforest_test0():\n",
    "    m = 200\n",
    "    x = np.arange(100).reshape((100, 1))\n",
    "    y = np.arange(100)\n",
    "    trees = forest(x, y, m)    \n",
    "    preds = evalboostforest(trees, x)\n",
    "    return preds.shape == y.shape\n",
    "\n",
    "def evalboostforest_test1():\n",
    "    m = 200\n",
    "    x = np.random.rand(10,3)\n",
    "    y = np.ones(10)\n",
    "    x2 = np.random.rand(10,3)\n",
    "\n",
    "    max_depth = 0\n",
    "    \n",
    "    # Create a forest with trees depth 0\n",
    "    # Since the data are all ones, each tree will return 1 as prediction\n",
    "    trees = forest(x, y, m, max_depth)     \n",
    "    pred = evalboostforest(trees, x2)[0]\n",
    "    return np.isclose(pred,1)  # the prediction should be equal to the sum of weights\n",
    "\n",
    "def evalboostforest_test2(): \n",
    "    # results should match evalforest if alphas are 1/m and labels +1, -1\n",
    "    m = 20\n",
    "    x = np.arange(100).reshape((100, 1))\n",
    "    y = np.sign(np.arange(100))\n",
    "    trees = forest(x, y, m)\n",
    "\n",
    "    alphas=np.ones(m)/m\n",
    "    preds1 = evalforest(trees, x)\n",
    "    preds2 = evalboostforest(trees, x, alphas)\n",
    "    return np.all(np.isclose(preds1,preds2))\n",
    "\n",
    "def evalboostforest_test3(): \n",
    "    # results should NOT match evalforest if alphas are 1/m and labels NOT +1, -1\n",
    "    m = 20\n",
    "    x = np.arange(100).reshape((100, 1))\n",
    "    y = np.arange(100)\n",
    "    trees = forest(x, y, m)\n",
    "\n",
    "    alphas=np.ones(m)/m\n",
    "    preds1 = evalforest(trees, x)\n",
    "    preds2 = evalboostforest(trees, x, alphas)\n",
    "    return not(np.all(np.isclose(preds1,preds2)))\n",
    "\n",
    "def evalboostforest_test4(): \n",
    "    # if only alpha[i]=1 and all others are 0, the result should match exactly \n",
    "    # the predictions of the ith tree\n",
    "    m = 20\n",
    "    x = np.random.rand(100,5)\n",
    "    y = np.arange(100)\n",
    "    x2 = np.random.rand(20,5)\n",
    "\n",
    "    trees = forest(x, y, m)\n",
    "    allmatch=True\n",
    "    for i in range(m): # go through each tree i\n",
    "        alphas=np.zeros(m)\n",
    "        alphas[i]=1.0; # set only alpha[i]=1 all other alpha=0\n",
    "        preds1 = np.sign(trees[i].predict(x2)) # get prediction of ith tree\n",
    "        preds2 = evalboostforest(trees, x2, alphas) # geet prediction of weighted ensemble\n",
    "        allmatch=allmatch and all(np.isclose(preds1,preds2))\n",
    "    return allmatch\n",
    "\n",
    "\n",
    "# and some new tests to check if the weights (and the np.sign function) were incorporated correctly \n",
    "runtest(evalboostforest_test0, 'evalboostforest_test0')\n",
    "runtest(evalboostforest_test1, 'evalboostforest_test1')\n",
    "runtest(evalboostforest_test2, 'evalboostforest_test2')\n",
    "runtest(evalboostforest_test3, 'evalboostforest_test3')\n",
    "runtest(evalboostforest_test4, 'evalboostforest_test4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Part Two: Implement <code>boosttree</code> [Graded]</h3>\n",
    "\n",
    "Implement the function <code>boosttree</code>, which applies Adaboost to the decision tree model. You can use the pseudo-code in the Adaboost Cheat Sheet to help construct the function. You can also use your <code>evalboostforest</code> function to evaluate your boosted ensemble (provdided you pass on the weights correctly.)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-45a1c7c13850303b",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def boosttree(x,y,maxiter=100,maxdepth=2):\n",
    "    \"\"\"Learns a boosted decision tree.\n",
    "    \n",
    "    Input:\n",
    "        x:        n x d matrix of data points\n",
    "        y:        n-dimensional vector of labels\n",
    "        maxiter:  maximum number of trees\n",
    "        maxdepth: maximum depth of a tree\n",
    "        \n",
    "    Output:\n",
    "        forest: list of TreeNode decision trees of length m\n",
    "        alphas: m-dimensional weight vector\n",
    "        \n",
    "    (note, m is at most maxiter, but may be smaller,\n",
    "    as dictated by the Adaboost algorithm)\n",
    "    \"\"\"\n",
    "    assert np.allclose(np.unique(y), np.array([-1,1])); # the labels must be -1 and 1 \n",
    "    n,d = x.shape\n",
    "    weights = np.ones(n) / n\n",
    "    preds   = None\n",
    "    forest  = []\n",
    "    alphas  = []\n",
    "\n",
    "    ### BEGIN SOLUTION \n",
    "    for i in range(maxiter):\n",
    "        tree = RegressionTree(depth=maxdepth, weights=weights)\n",
    "        tree.fit(x, y)\n",
    "        preds = np.sign(tree.predict(x))\n",
    "        results = (preds != y)\n",
    "        err = np.sum(results * weights)\n",
    "        if err > 0.5 - np.finfo(float).eps:\n",
    "            print(\"error is too large?\", err, \"max error\", 0.5 - np.finfo(float).eps or abs(err) < np.finfo(float).eps)\n",
    "            break\n",
    "        elif abs(err) < np.finfo(float).eps:\n",
    "            # classification is so simple that one tree is enough\n",
    "            return [tree], [1]\n",
    "        \n",
    "        alpha = 0.5 * np.log((1-err)/err)\n",
    "        forest.append(tree)\n",
    "        alphas.append(alpha)\n",
    "        weights = weights*np.exp(alpha * (results*2 - 1))\n",
    "        weights = weights/sum(weights)\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    return forest, alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-boosttree-selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def boosting_test1():\n",
    "    maxiter = 50\n",
    "    maxdepth = 4\n",
    "    xTr = np.random.rand(500,3) - 0.5\n",
    "    yTr = np.sign(xTr[:,0] * xTr[:,1] * xTr[:,2]) # XOR Classification\n",
    "    xTe = np.random.rand(50,3) - 0.5\n",
    "    yTe = np.sign(xTe[:,0] * xTe[:,1] * xTe[:,2])\n",
    "    \n",
    "    out = boosttree(xTr, yTr, maxiter, maxdepth)\n",
    "    return len(out) == 2 #  boosttrees function should return a tuple\n",
    "\n",
    "def boosting_test2():\n",
    "    maxiter = 50\n",
    "    maxdepth = 4\n",
    "    xTr = np.random.rand(500,3) - 0.5\n",
    "    yTr = np.sign(xTr[:,0] * xTr[:,1] * xTr[:,2]) # XOR Classification\n",
    "    xTe = np.random.rand(50,3) - 0.5\n",
    "    yTe = np.sign(xTe[:,0] * xTe[:,1] * xTe[:,2])\n",
    "    \n",
    "    trees, alphas = boosttree(xTr, yTr, maxiter, maxdepth)\n",
    "    return len(trees) == len(alphas) #  the len of the two returned output should be the same\n",
    "\n",
    "def boosting_test3():\n",
    "    maxiter = 50\n",
    "    maxdepth = 3\n",
    "    xTr = np.random.rand(500,3) - 0.5\n",
    "    yTr = np.sign(xTr[:,0] * xTr[:,1] * xTr[:,2]) # XOR Classification\n",
    "    xTe = np.random.rand(50,3) - 0.5\n",
    "    yTe = np.sign(xTe[:,0] * xTe[:,1] * xTe[:,2])\n",
    "\n",
    "    tree = RegressionTree(maxdepth)\n",
    "    tree.fit(xTr, yTr)\n",
    "    oneacc = np.sum(np.sign(tree.predict(xTe)) == yTe)\n",
    "\n",
    "    trees, alphas = boosttree(xTr, yTr, maxiter, maxdepth)\n",
    "    multiacc = np.sum(np.sign(evalboostforest(trees, xTe, alphas)) == yTe)\n",
    "    return multiacc >= oneacc\n",
    "\n",
    "runtest(boosting_test1, 'boosting_test1')\n",
    "runtest(boosting_test2, 'boosting_test2')\n",
    "runtest(boosting_test3, 'boosting_test3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-boosting-test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs boosting_test1\n",
    "### BEGIN HIDDEN TESTS\n",
    "maxiter = 50\n",
    "maxdepth = 4\n",
    "xTr = np.random.rand(500,3) - 0.5\n",
    "yTr = np.sign(xTr[:,0] * xTr[:,1] * xTr[:,2]) # XOR Classification\n",
    "xTe = np.random.rand(50,3) - 0.5\n",
    "yTe = np.sign(xTe[:,0] * xTe[:,1] * xTe[:,2])\n",
    "\n",
    "out = boosttree(xTr, yTr, maxiter, maxdepth)\n",
    "assert len(out) == 2 #  boosttrees function should return a tuple\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-boosting-test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs boosting_test2\n",
    "### BEGIN HIDDEN TESTS\n",
    "maxiter = 50\n",
    "maxdepth = 4\n",
    "xTr = np.random.rand(500,3) - 0.5\n",
    "yTr = np.sign(xTr[:,0] * xTr[:,1] * xTr[:,2]) # XOR Classification\n",
    "xTe = np.random.rand(50,3) - 0.5\n",
    "yTe = np.sign(xTe[:,0] * xTe[:,1] * xTe[:,2])\n",
    "\n",
    "trees, alphas = boosttree(xTr, yTr, maxiter, maxdepth)\n",
    "assert len(trees) == len(alphas) #  the len of the two returned output should be the same\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-boosting-test3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs boosting_test3\n",
    "### BEGIN HIDDEN TESTS\n",
    "maxiter = 50\n",
    "maxdepth = 3\n",
    "xTr = np.random.rand(500,3) - 0.5\n",
    "yTr = np.sign(xTr[:,0] * xTr[:,1] * xTr[:,2]) # XOR Classification\n",
    "xTe = np.random.rand(50,3) - 0.5\n",
    "yTe = np.sign(xTe[:,0] * xTe[:,1] * xTe[:,2])\n",
    "\n",
    "tree = RegressionTree(maxdepth)\n",
    "tree.fit(xTr, yTr)\n",
    "oneacc = np.sum(np.sign(tree.predict(xTe)) == yTe)\n",
    "\n",
    "trees, alphas = boosttree(xTr, yTr, maxiter, maxdepth)\n",
    "multiacc = np.sum(np.sign(evalboostforest(trees, xTe, alphas)) == yTe)\n",
    "assert multiacc >= oneacc\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2793b65603029f78",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Evaluate Test and Training Error</h3>\n",
    "\n",
    "<p>The following script evaluates the test and training error of a boosted forest as we increase the number of trees. The plot shows nicely that the boosting training error is bounded above by the exponential loss, which is rapidly approaching zero. This is one of the nice aspects of boosting, it turns an ensemble of weak learners into a strong learner, that can always achieve 0 training error in a small number of iterations. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4679f49ad1dcdb53",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "M=20 # max number of trees\n",
    "alltrees,allalphas=boosttree(xTrIon,yTrIon,maxdepth=3,maxiter=M)\n",
    "\n",
    "err_trB=[]\n",
    "loss_trB=[]\n",
    "err_teB=[]\n",
    "for i in range(M):\n",
    "    trees=alltrees[:i+1]\n",
    "    alphas=allalphas[:i+1]\n",
    "    trErr = np.mean(np.sign(evalboostforest(trees,xTrIon,alphas)) != yTrIon)\n",
    "    trLoss =np.mean(np.exp(-evalboostforest(trees,xTrIon,alphas)*yTrIon))\n",
    "    teErr = np.mean(np.sign(evalboostforest(trees,xTeIon,alphas)) != yTeIon)\n",
    "    err_trB.append(trErr)\n",
    "    err_teB.append(teErr)\n",
    "    loss_trB.append(trLoss)\n",
    "    print(\"[%d] exp loss = %.4f training err = %.4f\\ttesting err = %.4f\" % (i,trLoss,trErr, teErr))\n",
    "\n",
    "plt.figure()\n",
    "line_tr, = plt.plot(range(M),err_trB,label=\"Training Error\")\n",
    "line_te, = plt.plot(range(M),err_teB,label=\"Testing error\")\n",
    "line_trloss,=plt.plot(range(M),loss_trB,label='Exp. Loss')\n",
    "plt.title(\"Adaboost\")\n",
    "plt.legend(handles=[line_tr, line_te,line_trloss])\n",
    "plt.xlabel(\"# of trees\")\n",
    "plt.ylabel(\"error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7840cb9a40e37454",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trees,alphas=boosttree(xTrSpiral,yTrSpiral,maxdepth=3,maxiter=50)\n",
    "visclassifier(lambda X:evalforest(trees,X,alphas),xTrSpiral,yTrSpiral)\n",
    "\n",
    "print(\"Training error: %.4f\" % np.mean(np.sign(evalforest(trees,xTrSpiral)) != yTrSpiral))\n",
    "print(\"Testing error:  %.4f\" % np.mean(np.sign(evalforest(trees,xTeSpiral)) != yTeSpiral))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Interactive Demo</h3>\n",
    "\n",
    "The following demo visualizes the boosted classifier. Add your own points directly on the graph with click and shift+click to see the prediction boundaries. There will be a delay between clicks as the new classifier is trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-53ad4beabd63cc86",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "xTrain= np.array([[5,6],[2,5]])\n",
    "yTrain = np.array([-1,1])\n",
    "\n",
    "\n",
    "def onclick_boost(event):\n",
    "    \"\"\"\n",
    "    Visualize boosting, including new point\n",
    "    \"\"\"\n",
    "    global xTrain,yTrain\n",
    "    # create position vector for new point\n",
    "    pos=np.array([[event.xdata,event.ydata]]) \n",
    "    if event.key == 'shift': # add positive point\n",
    "        color='or'\n",
    "        label=1\n",
    "    else: # add negative point\n",
    "        color='ob'\n",
    "        label=-1    \n",
    "    xTrain = np.concatenate((xTrain,pos), axis = 0)\n",
    "    yTrain = np.append(yTrain, label)\n",
    "    marker_symbols = ['o', 'x']\n",
    "    classvals = np.unique(yTrain)\n",
    "            \n",
    "    mycolors = [[0.5, 0.5, 1], [1, 0.5, 0.5]]\n",
    "    \n",
    "    # return 300 evenly spaced numbers over this interval\n",
    "    res=300\n",
    "    xrange = np.linspace(0,1,res)\n",
    "    yrange = np.linspace(0,1,res)\n",
    "    \n",
    "    # repeat this matrix 300 times for both axes\n",
    "    pixelX = repmat(xrange, res, 1)\n",
    "    pixelY = repmat(yrange, res, 1).T\n",
    "    xTe = np.array([pixelX.flatten(), pixelY.flatten()]).T\n",
    "    \n",
    "    # get forest\n",
    "    forest,alphas=boosttree(xTrain,yTrain,maxdepth=3,maxiter=5)\n",
    "    if len(forest) > 0:\n",
    "        fun = lambda X: evalboostforest(forest,X,alphas)\n",
    "        # test all of these points on the grid\n",
    "        testpreds = fun(xTe)\n",
    "\n",
    "        # reshape it back together to make our grid\n",
    "        Z = testpreds.reshape(res, res)\n",
    "        Z[0,0] = 1 # optional: scale the colors correctly\n",
    "\n",
    "        plt.cla()    \n",
    "        plt.xlim((0,1))\n",
    "        plt.ylim((0,1))\n",
    "        # fill in the contours for these predictions\n",
    "        plt.contourf(pixelX, pixelY, np.sign(Z), colors=mycolors)\n",
    "    \n",
    "    for idx, c in enumerate(classvals):\n",
    "        plt.scatter(xTrain[yTrain == c,0],\n",
    "            xTrain[yTrain == c,1],\n",
    "            marker=marker_symbols[idx],\n",
    "            color='k'\n",
    "            )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "%matplotlib notebook\n",
    "fig = plt.figure()\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "cid = fig.canvas.mpl_connect('button_press_event', onclick_boost)\n",
    "plt.title('Use shift-click to add negative points.')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
