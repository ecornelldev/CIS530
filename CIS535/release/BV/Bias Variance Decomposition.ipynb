{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1400f7085ddaeac3907019c6ae2375b0",
     "grade": false,
     "grade_id": "cell-1e2b137b9f48bf8c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<body>\n",
    "<h2>Bias Variance Trade-Off</h2>\n",
    "\n",
    "<!--announcements-->\n",
    "<blockquote>\n",
    "    <center>\n",
    "    <a href=\"http://blogs.worldbank.org/publicsphere/files/publicsphere/biased_processing.jpg\"><img src=\"bias.jpg\" width=\"600px\" /></a>\n",
    "    </center>\n",
    "      <p><cite><center>\"All of us show bias when it comes to what information we take in.<br>We typically focus on anything that agrees with the outcome we want.\"<br>\n",
    "<b>--Noreena Hertz</b>\n",
    "      </center></cite></p>\n",
    "</blockquote>\n",
    "<h3>Introduction</h3>\n",
    "\n",
    "<p>\n",
    "Recall that the squared error can be decomposed into <em>bias</em>, <em>variance</em> and <em>noise</em>: \n",
    "$$\n",
    "    \\underbrace{\\mathbb{E}[(h_D(\\mathbf{x}) - y)^2]}_\\mathrm{Error} = \\underbrace{\\mathbb{E}[(h_D(\\mathbf{x})-\\bar{h}(\\mathbf{x}))^2]}_\\mathrm{Variance} + \\underbrace{\\mathbb{E}[(\\bar{h}(\\mathbf{x})-\\bar{y}(\\mathbf{x}))^2]}_\\mathrm{Bias} + \\underbrace{\\mathbb{E}[(\\bar{y}(\\mathbf{x})-y(\\mathbf{x}))^2]}_\\mathrm{Noise}\\nonumber\n",
    "$$\n",
    "We will now create a data set for which we can approximately compute this decomposition. \n",
    "The function <em><strong>`toydata`</strong></em> generates a binary data set with class $1$ and $2$. Both are sampled from Gaussian distributions:\n",
    "$$\n",
    "p(\\mathbf{x}|y=1)\\sim {\\mathcal{N}}(0,{I}) \\textrm { and } p(\\mathbf{x}|y=2)\\sim {\\mathcal{N}}(\\mu_2,{I}),\n",
    "$$\n",
    "\n",
    "where $\\mathbf{\\mu_2}=[1.75, 1.75]^\\top$ (the global variable <em>OFFSET</em> $\\!=\\!2$ regulates these values: $\\mathbf{\\mu_2}=[$<em>OFFSET</em> $, $ <em>OFFSET</em>$]^\\top$).\n",
    "</p>\n",
    "\n",
    "<h3>Computing noise, bias and variance</h3>\n",
    "<p>\n",
    "You will need to edit three functions:  <em><strong>`computeybar`</strong></em>,  <em><strong>`computehbar`</strong></em>, and <em><strong>`computevariance`</strong></em>. First take a look at <strong>`biasvariancedemo`</strong> and make sure you understand where each function should be called and how they contribute to the Bias/Variance/Noise decomposition. <br/><br/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "056b3a01fd45723e712904009f0c52b5",
     "grade": false,
     "grade_id": "cell-53fdcf615afb6769",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Libraries**: Before we get started we need to install a few libraries. You can do this by executing the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3ee24502b6431ceadb7b731fb69d5469",
     "grade": false,
     "grade_id": "cell-e5ced222ffb03a72",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.matlib import repmat\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import helper as h\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ade2da5764abb38df7b8bf686623444e",
     "grade": false,
     "grade_id": "cell-10f3cee1068653f1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**`h.toydata` Helper Function**: `h.toydata` is a helper function used to generate the the binary data with n/2 values in class 1 and n/2 values in class 2. With class 1 being the label for data drawn from a normal distribution having mean 0 and sigma 1. And class 2 being the label for data drawn from a normal distribution with mean OFFSET and sigma 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "21637c7b30af25310d20bce9b610aa61",
     "grade": false,
     "grade_id": "cell-68beb9e0fa121c43",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "OFFSET = 1.75\n",
    "X, y = h.toydata(OFFSET, 1000)\n",
    "\n",
    "# Visualize the generated data\n",
    "ind1 = y == 1\n",
    "ind2 = y == 2\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(X[ind1, 0], X[ind1, 1], c='r', marker='o', label='Class 1')\n",
    "plt.scatter(X[ind2, 0], X[ind2, 1], c='b', marker='o', label='Class 2')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8761213c7619466d5f4c3eff34f0a474",
     "grade": false,
     "grade_id": "cell-d4332b3b2f191280",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<p>\n",
    "(a) <strong>Noise:</strong> First we focus on the noise. For this, you need to compute $\\bar y(\\mathbf{x})$ in  <em><strong>`computeybar`</strong></em>. You can compute the probability $p(\\mathbf{x}|y)$ with the equations $p(\\mathbf{x}|y=1)\\sim {\\mathcal{N}}(0,{I}) \\textrm { and } p(\\mathbf{x}|y=2)\\sim {\\mathcal{N}}(\\mathbf{\\mu_2},{I})$. Then use Bayes rule to compute $p(y|\\mathbf{x})$. <br/><br/>\n",
    "<strong>Note:</strong> You may want to use the function <em>`normpdf`</em>, which is defined for  you in <em><strong>`computeybar`</strong></em>.\n",
    "<br/><br/></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a618bf69a011e67a01ba4b15ab1b4dc9",
     "grade": false,
     "grade_id": "cell-117facdd1bb21d5f",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def computeybar(xTe, OFFSET):\n",
    "    \"\"\"\n",
    "    function [ybar]=computeybar(xTe, OFFSET);\n",
    "\n",
    "    computes the expected label 'ybar' for a set of inputs x\n",
    "    generated from two standard Normal distributions (one offset by OFFSET in\n",
    "    both dimensions.)\n",
    "\n",
    "    INPUT:\n",
    "    xTe : nx2 array of n vectors with 2 dimensions\n",
    "    OFFSET    : The OFFSET passed into the toyData function. The difference in the\n",
    "                mu of labels class1 and class2 for toyData.\n",
    "\n",
    "    OUTPUT:\n",
    "    ybar : a nx1 vector of the expected labels for vectors xTe\n",
    "    \"\"\"\n",
    "    n, d = xTe.shape\n",
    "    ybar = np.zeros(n)\n",
    "    \n",
    "    # Feel free to use the following function to compute p(x|y), or not\n",
    "    # normal distribution is default mu = 0, sigma = 1.\n",
    "    normpdf = lambda x, mu, sigma: np.exp(-0.5 * np.power((x - mu) / sigma, 2)) / (np.sqrt(2 * np.pi) * sigma)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return ybar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3ec52351e575e723af7a0e2fe81267d1",
     "grade": false,
     "grade_id": "cell-2a0090c31f38a547",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Visualizing the Data**:\n",
    "You can now see the error of the bayes classifier. Below is a plotting of the two classes of points and the misclassified points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "44ed5f0ee3126a3e77e4b116faa321d0",
     "grade": false,
     "grade_id": "cell-ad17a864ee4ad0e9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "OFFSET = 1.75\n",
    "np.random.seed(1)\n",
    "xTe, yTe = h.toydata(OFFSET, 1000)\n",
    "\n",
    "# compute Bayes Error\n",
    "ybar = computeybar(xTe, OFFSET)\n",
    "predictions = np.round(ybar)\n",
    "errors = predictions != yTe\n",
    "err = errors.sum() / len(yTe) * 100\n",
    "print('Error of Bayes classifier: %.1f%%.' % err)\n",
    "\n",
    "# plot data\n",
    "ind1 = yTe == 1\n",
    "ind2 = yTe == 2\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(xTe[ind1, 0], xTe[ind1, 1], c='r', marker='o')\n",
    "plt.scatter(xTe[ind2, 0], xTe[ind2, 1], c='b', marker='o')\n",
    "plt.scatter(xTe[errors, 0], xTe[errors, 1], c='k', s=100, alpha=0.2)\n",
    "plt.title(\"Plot of data (misclassified points highlighted)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4f83a93fa62fdae597a1e2d189420783",
     "grade": false,
     "grade_id": "cell-ybar-selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def test_ybar1():\n",
    "    OFFSET = 2\n",
    "    n = 1000\n",
    "    xTe, yTe = h.toydata(OFFSET, n) # Generate n datapoints\n",
    "    ybar = computeybar(xTe, OFFSET)\n",
    "    \n",
    "    return ybar.shape == (n, ) # the output of your ybar should be a n dimensional array\n",
    "\n",
    "def test_ybar2():\n",
    "    OFFSET = 50\n",
    "    # Create an easy dataset\n",
    "    # We set sigma=1 and since the mean is far apart,\n",
    "    # the noise is negligible\n",
    "    xTe = np.array([\n",
    "        [49.308783, 49.620651], \n",
    "        [1.705462, 1.885418], \n",
    "        [ 51.192402, 50.256330],\n",
    "        [0.205998, -0.089885],\n",
    "        [50.853083, 51.833237]])  \n",
    "    yTe = np.array([2, 1, 2, 1, 2])\n",
    "    \n",
    "    ybar = computeybar(xTe, OFFSET)\n",
    "    noise = np.mean(np.power(yTe - ybar, 2)) # calculate the noise\n",
    "    return np.isclose(noise, 0)\n",
    "\n",
    "def test_ybar3():\n",
    "    OFFSET = 3;\n",
    "\n",
    "    xTe = np.array([\n",
    "        [0.45864, 0.71552],\n",
    "        [2.44662, 1.68167],\n",
    "        [1.00345, 0.15182],\n",
    "        [-0.10560, -0.48155],\n",
    "        [3.07264, 3.81535],\n",
    "        [3.13035, 2.72151],\n",
    "        [2.25265, 3.78697]])\n",
    "    yTe = np.array([1, 2, 1, 1, 2, 2, 2])\n",
    "    \n",
    "    ybar = computeybar(xTe, OFFSET)\n",
    "    noise = np.mean(np.power(yTe - ybar, 2)) # calculate the noise\n",
    "    \n",
    "    return noise < 0.0002 # make sure the noise is small\n",
    "\n",
    "h.runtest(test_ybar1, 'test_ybar1')\n",
    "h.runtest(test_ybar2, 'test_ybar2')\n",
    "h.runtest(test_ybar3, 'test_ybar3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "131db4a29206f408e8a0ae70fd339978",
     "grade": true,
     "grade_id": "cell-ybar1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_ybar1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "353c28393e8c908ab31cb74cf0e20ede",
     "grade": true,
     "grade_id": "cell-ybar2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_ybar2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "19f1fd05e5b1daaf31f4190d914556c1",
     "grade": true,
     "grade_id": "cell-ybar3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_ybar3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fac0f894d884a7b62cd686a3b139f5e4",
     "grade": false,
     "grade_id": "cell-f05e0af9fb83da3b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<p>\n",
    "(b) <strong>Bias:</strong> For the bias, you will need $\\bar{h}$. Although we cannot compute the expected value  $\\bar h\\!=\\!\\mathbb{E}[h]$, we can approximate it by training many $h_D$ and averaging their predictions. Edit the function <em><strong>`computehbar`</strong></em>. Average over <em>NMODELS</em> different $h_D$, each trained on a different data set of <em>Nsmall</em> inputs drawn from the same distribution. Feel free to call <em><strong>`toydata`</strong></em> to obtain more data sets. <br/><br/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4f33127a53a11ae2d70e0625bddfa5b4",
     "grade": false,
     "grade_id": "cell-ff7ca5bceeb099d7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We are going to use the regression tree that we used in our previous project as our $h_D$ . To remind you, the following code shows you how to instantiate a decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6ad1bfa6fa7f5266bbfc2303be939401",
     "grade": false,
     "grade_id": "cell-660c9a79c812ba96",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "xTr, yTr = h.toydata(OFFSET, 100)\n",
    "\n",
    "# Create a regression tree with no restriction on its depth\n",
    "# if you want to create a tree of depth k\n",
    "# then call h.RegressionTree(depth=k)\n",
    "tree = h.RegressionTree(depth=np.inf)\n",
    "\n",
    "# To fit/train the regression tree\n",
    "tree.fit(xTr, yTr)\n",
    "\n",
    "# To use the trained regression tree to make prediction\n",
    "pred = tree.predict(xTr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ee7275baa0ae03214c52d1cdfdb51a29",
     "grade": false,
     "grade_id": "cell-eda02b58ff80bd9f",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def computehbar(xTe, depth, Nsmall, NMODELS, OFFSET):\n",
    "    \"\"\"\n",
    "    function [hbar]=computehbar(xTe, sigma, lmbda, NSmall, NMODELS, OFFSET);\n",
    "\n",
    "    computes the expected prediction of the average regression tree (hbar)\n",
    "    for data set xTe. \n",
    "\n",
    "    The regression tree should be trained using data of size Nsmall and is drawn from toydata with OFFSET \n",
    "    \n",
    "\n",
    "    The \"infinite\" number of models is estimated as an average over NMODELS. \n",
    "\n",
    "    INPUT:\n",
    "    xTe       | nx2 matrix, of n column-wise input vectors (each 2-dimensional)\n",
    "    depth     | Depth of the tree \n",
    "    NSmall    | Number of points to subsample\n",
    "    NMODELS   | Number of Models to average over\n",
    "    OFFSET    | The OFFSET passed into the toyData function. The difference in the\n",
    "                mu of labels class1 and class2 for toyData.\n",
    "    OUTPUT:\n",
    "    hbar | nx1 vector with the predictions of hbar for each test input\n",
    "    \"\"\"\n",
    "    n = xTe.shape[0]\n",
    "    hbar = np.zeros(n)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return hbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a9bab83331750c0bb75681cedfc1585d",
     "grade": false,
     "grade_id": "cell-hbar-selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def test_hbar1():\n",
    "    OFFSET = 2\n",
    "    depth = 2\n",
    "    Nsmall = 10\n",
    "    NMODELS = 10 \n",
    "    n = 1000\n",
    "    xTe, yTe = h.toydata(OFFSET, n)\n",
    "    hbar = computehbar(xTe, depth, Nsmall, NMODELS, OFFSET)\n",
    "    return hbar.shape == (n, ) # the dimension of hbar should be (n, )\n",
    "\n",
    "def test_hbar2():\n",
    "    OFFSET = 50\n",
    "    # Create an easy dataset\n",
    "    # We set sigma=1 and since the mean is far apart,\n",
    "    # the noise is negligible\n",
    "    xTe = np.array([\n",
    "        [49.308783, 49.620651], \n",
    "        [1.705462, 1.885418], \n",
    "        [51.192402, 50.256330],\n",
    "        [0.205998, -0.089885],\n",
    "        [50.853083, 51.833237]])  \n",
    "    yTe = np.array([2, 1, 2, 1, 2])\n",
    "    \n",
    "    depth = 2\n",
    "    Nsmall = 10\n",
    "    NMODELS = 1\n",
    "    \n",
    "    # since the mean is far apart, the tree should be able to learn perfectly\n",
    "    hbar = computehbar(xTe, depth, Nsmall, NMODELS, OFFSET) \n",
    "    ybar = h.computeybar_grader(xTe, OFFSET)\n",
    "    bias = np.mean(np.power(hbar-ybar,2))\n",
    "    return np.isclose(bias, 0) # the bias should be close to zero\n",
    "\n",
    "def test_hbar3():\n",
    "    OFFSET = 3;\n",
    "\n",
    "    xTe = np.array([\n",
    "        [0.45864, 0.71552],\n",
    "        [2.44662, 1.68167],\n",
    "        [1.00345, 0.15182],\n",
    "        [-0.10560, -0.48155],\n",
    "        [3.07264, 3.81535],\n",
    "        [3.13035, 2.72151],\n",
    "        [2.25265, 3.78697]])\n",
    "    yTe = np.array([1, 2, 1, 1, 2, 2, 2])\n",
    "    \n",
    "    depth = 3\n",
    "    Nsmall = 10\n",
    "    NMODELS = 100\n",
    "    \n",
    "    # set the random seed to ensure consistent behavior\n",
    "    np.random.seed(1)\n",
    "    hbar = computehbar(xTe, depth, Nsmall, NMODELS, OFFSET) \n",
    "    ybar = h.computeybar_grader(xTe, OFFSET)\n",
    "    bias = np.mean(np.power(hbar-ybar, 2))\n",
    "    return np.abs(bias - 0.0017) < 0.001 # the bias should be close to 0.007\n",
    "\n",
    "h.runtest(test_hbar1, 'test_hbar1')\n",
    "h.runtest(test_hbar2, 'test_hbar2')\n",
    "h.runtest(test_hbar3, 'test_hbar3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1e6d8b06aa693af3dc5c6151d9cad655",
     "grade": true,
     "grade_id": "cell-hbar1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_hbar1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d1bd384a79608052db9dc59f73ca67b6",
     "grade": true,
     "grade_id": "cell-hbar2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_hbar2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3a3f6f645ee79ca122b6123a5740bc6d",
     "grade": true,
     "grade_id": "cell-hbar3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_hbar3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "378c4700ecc966e4ec50f8b8a93e6d0e",
     "grade": false,
     "grade_id": "cell-484ccd205f7b43fc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "\n",
    "<p>(c) <strong>Variance:</strong> Finally, to compute the variance, we need to compute the term $\\mathbb{E}[(h_D-\\bar{h})^2]$. Once again, we can approximate this term by averaging over  <em>NMODELS</em> models. Edit the function <em><strong>`computevariance`</strong></em>. \n",
    "<br/></br></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "24443323bffe2e0dd13b4ef3602f7070",
     "grade": false,
     "grade_id": "cell-718bfef48e6b892d",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def computevariance(xTe, depth, hbar, Nsmall, NMODELS, OFFSET):\n",
    "    \"\"\"\n",
    "    function variance=computevariance(xTe,sigma,lmbda,hbar,Nsmall,NMODELS,OFFSET)\n",
    "\n",
    "    computes the variance of classifiers trained on data sets from\n",
    "    toydata.m with pre-specified \"OFFSET\" and \n",
    "    with kernel regression with sigma and lmbda\n",
    "    evaluated on xTe. \n",
    "    the prediction of the average classifier is assumed to be stored in \"hbar\".\n",
    "\n",
    "    The \"infinite\" number of models is estimated as an average over NMODELS. \n",
    "\n",
    "    INPUT:\n",
    "    xTe       : nx2 matrix, of n column-wise input vectors (each 2-dimensional)\n",
    "    depth     : Depth of the tree \n",
    "    hbar      : nx1 vector of the predictions of hbar on the inputs xTe\n",
    "    Nsmall    : Number of samples drawn from toyData for one model\n",
    "    NModel    : Number of Models to average over\n",
    "    OFFSET    : The OFFSET passed into the toyData function. The difference in the\n",
    "                mu of labels class1 and class2 for toyData.\n",
    "    \"\"\"\n",
    "    n = xTe.shape[0]\n",
    "    variance = np.zeros(n)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "067b8754b7b8c808044c0d9a8b279bef",
     "grade": false,
     "grade_id": "cell-variance-selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def test_variance1():\n",
    "    OFFSET = 2\n",
    "    depth = 2\n",
    "    Nsmall = 10\n",
    "    NMODELS = 10 \n",
    "    n = 1000\n",
    "    xTe, yTe = h.toydata(OFFSET, n)\n",
    "    hbar = h.computehbar_grader(xTe, depth, Nsmall, NMODELS, OFFSET)\n",
    "    var = computevariance(xTe, depth, hbar, Nsmall, NMODELS, OFFSET)\n",
    "    return np.isscalar(var) # variance should be a scalar\n",
    "\n",
    "def test_variance2():\n",
    "    OFFSET = 50\n",
    "    # Create an easy dataset\n",
    "    # We set sigma=1 and since the mean is far apart,\n",
    "    # the noise is negligible\n",
    "    xTe = np.array([\n",
    "        [49.308783, 49.620651], \n",
    "        [1.705462, 1.885418], \n",
    "        [51.192402, 50.256330],\n",
    "        [0.205998, -0.089885],\n",
    "        [50.853083, 51.833237]])  \n",
    "    yTe = np.array([2, 1, 2, 1, 2])\n",
    "    \n",
    "    depth = 2\n",
    "    Nsmall = 10\n",
    "    NMODELS = 10\n",
    "    \n",
    "    # since the noise is negligible, the tree should be able to learn perfectly\n",
    "    hbar = h.computehbar_grader(xTe, depth, Nsmall, NMODELS, OFFSET) \n",
    "    var = computevariance(xTe, depth, hbar, Nsmall, NMODELS, OFFSET)\n",
    "    return np.isclose(var, 0) # the bias should be close to zero\n",
    "\n",
    "def test_variance3():\n",
    "    OFFSET = 3;\n",
    "\n",
    "    xTe = np.array([\n",
    "        [0.45864, 0.71552],\n",
    "        [2.44662, 1.68167],\n",
    "        [1.00345, 0.15182],\n",
    "        [-0.10560, -0.48155],\n",
    "        [3.07264, 3.81535],\n",
    "        [3.13035, 2.72151],\n",
    "        [2.25265, 3.78697]])\n",
    "    yTe = np.array([1, 2, 1, 1, 2, 2, 2])\n",
    "    \n",
    "    depth = 3\n",
    "    Nsmall = 10\n",
    "    NMODELS = 100\n",
    "    \n",
    "    # set the random seed to ensure consistent behavior\n",
    "    np.random.seed(1)\n",
    "    # since the noise is negligible, the tree should be able to learn perfectly\n",
    "    hbar = h.computehbar_grader(xTe, depth, Nsmall, NMODELS, OFFSET) \n",
    "    var = computevariance(xTe, depth, hbar, Nsmall, NMODELS, OFFSET)\n",
    "    return np.abs(var - 0.0404) < 0.001 # the variance should be close to 0.0404\n",
    "\n",
    "h.runtest(test_variance1, 'test_variance1')\n",
    "h.runtest(test_variance2, 'test_variance2')\n",
    "h.runtest(test_variance3, 'test_variance3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ff4b521ac026a0b4f71348f116ff372e",
     "grade": true,
     "grade_id": "cell-var1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_variance1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "acef3144d0efd73c9ecbf10b2f46d325",
     "grade": true,
     "grade_id": "cell-var2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_variance2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "16fc290b85507965a6eef1c476c080e4",
     "grade": true,
     "grade_id": "cell-var3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_variance3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c1b632ab8e62f67257ad028682991df8",
     "grade": false,
     "grade_id": "cell-3284bddfa0d87f94",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<p>If you did everything correctly and call execute the following demo. You should see how the error decomposes (roughly) into bias, variance and noise for various depths.</p>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# biasvariancedemo\n",
    "\n",
    "OFFSET = 1.75\n",
    "# how big is the training set size N\n",
    "Nsmall = 75\n",
    "# how big is a really big data set (approx. infinity)\n",
    "Nbig = 7500\n",
    "# how many models do you want to average over\n",
    "NMODELS = 100\n",
    "# What regularization constants to evaluate\n",
    "depths = [0, 1, 2, 3, 4, 5, 6, np.inf]\n",
    "\n",
    "# we store\n",
    "Ndepths = len(depths)\n",
    "lbias = np.zeros(Ndepths)\n",
    "lvariance = np.zeros(Ndepths)\n",
    "ltotal = np.zeros(Ndepths)\n",
    "lnoise = np.zeros(Ndepths)\n",
    "lsum = np.zeros(Ndepths)\n",
    "\n",
    "# Different regularization constant classifiers\n",
    "for i in range(Ndepths):\n",
    "    depth = depths[i]\n",
    "    # use this data set as an approximation of the true test set\n",
    "    xTe,yTe = h.toydata(OFFSET, Nbig)\n",
    "    \n",
    "    # Estimate AVERAGE ERROR (TOTAL)\n",
    "    total = 0\n",
    "    for j in range(NMODELS):\n",
    "        # Set the seed for consistent behavior\n",
    "        xTr2,yTr2 = h.toydata(OFFSET, Nsmall)\n",
    "        model = h.RegressionTree(depth=depth)\n",
    "        model.fit(xTr, yTr)\n",
    "        total += np.mean((model.predict(xTe) - yTe) ** 2)\n",
    "    total /= NMODELS\n",
    "    \n",
    "    # Estimate Noise\n",
    "    ybar = computeybar(xTe, OFFSET)\n",
    "    noise = np.mean((yTe-ybar) ** 2)\n",
    "    \n",
    "    # Estimate Bias\n",
    "    hbar = computehbar(xTe, depth, Nsmall, NMODELS, OFFSET)\n",
    "    bias = np.mean((hbar-ybar) ** 2)\n",
    "    \n",
    "    # Estimating VARIANCE\n",
    "    variance = computevariance(xTe,depth,hbar, Nsmall, NMODELS, OFFSET)\n",
    "    \n",
    "    # print and store results\n",
    "    lbias[i] = bias\n",
    "    lvariance[i] = variance\n",
    "    ltotal[i] = total\n",
    "    lnoise[i] = noise\n",
    "    lsum[i] = lbias[i]+lvariance[i]+lnoise[i]\n",
    "    \n",
    "    if np.isinf(depths[i]):\n",
    "        print('Depth infinite: Bias: %2.4f Variance: %2.4f Noise: %2.4f Bias+Variance+Noise: %2.4f Test error: %2.4f'\n",
    "          % (lbias[i],lvariance[i],lnoise[i],lsum[i],ltotal[i]))\n",
    "    else:\n",
    "        print('Depth: %d: Bias: %2.4f Variance: %2.4f Noise: %2.4f Bias+Variance+Noise: %2.4f Test error: %2.4f'\n",
    "          % (depths[i],lbias[i],lvariance[i],lnoise[i],lsum[i],ltotal[i]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "%matplotlib notebook\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(lbias[:Ndepths], '*', c='r',linestyle='-',linewidth=2)\n",
    "plt.plot(lvariance[:Ndepths], '*', c='k', linestyle='-',linewidth=2)\n",
    "plt.plot(lnoise[:Ndepths], '*', c='g',linestyle='-',linewidth=2)\n",
    "plt.plot(ltotal[:Ndepths], '*', c='b', linestyle='-',linewidth=2)\n",
    "plt.plot(lsum[:Ndepths], '*', c='k', linestyle='--',linewidth=2)\n",
    "\n",
    "plt.legend([\"Bias\",\"Variance\",\"Noise\",\"Test error\",\"Bias+Var+Noise\"]);\n",
    "plt.xlabel(\"Depth\",fontsize=18);\n",
    "plt.ylabel(\"Squared Error\",fontsize=18);\n",
    "plt.xticks([i for i in range(Ndepths)], depths);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
