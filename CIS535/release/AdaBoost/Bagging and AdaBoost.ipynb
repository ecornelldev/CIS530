{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "55f2f806cb27e192d7b8c25f72d40e0b",
     "grade": false,
     "grade_id": "cell-138bf686eef190d3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2>Bagging and AdaBoost</h2>\n",
    "\n",
    "\n",
    "<p> In this project, you are going to implement bagging and AdaBoost for Regression Tree. To start, we will import some packages we need </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4f38b0eb3055ddddebf26c6cbdf18ecf",
     "grade": false,
     "grade_id": "cell-cfd4728219aa2e6f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pylab import *\n",
    "from numpy.matlib import repmat\n",
    "import sys\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "import time\n",
    "import helper as h\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a5389e831a8d02ec9d428da11c595686",
     "grade": false,
     "grade_id": "cell-6613495f7b984ddf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In addition, we create a 2D spiral dataset of size 150 for visualization and a high dimensional dataset <a href=\"https://archive.ics.uci.edu/ml/datasets/Ionosphere\">ION</a>, which we will use as our binary test classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "48b7386f4696030ad7807eec6a0c806a",
     "grade": false,
     "grade_id": "cell-ea9d14b9dc5c69e4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "xTrSpiral,yTrSpiral,xTeSpiral,yTeSpiral= h.spiraldata(150)\n",
    "xTrIon,yTrIon,xTeIon,yTeIon= h.iondata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7380b3bf440169da741e5301e10ca5e3",
     "grade": false,
     "grade_id": "cell-b74b5db289644016",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We are going to use the regression tree that we used in our previous project. To remind you, the following code shows you how to instantiate a decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "73fbf006793d2000193f3a94292d7d65",
     "grade": false,
     "grade_id": "cell-425caeb78952a935",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a regression tree with no restriction on its depth\n",
    "# and weights for each training example to be 1\n",
    "# if you want to create a tree of max_depth k\n",
    "# then call h.RegressionTree(depth=k)\n",
    "# Also setting weights=None sets the weights \n",
    "# of each training examples to be the same\n",
    "tree = h.RegressionTree(depth=np.inf, weights=np.ones_like(yTrSpiral))\n",
    "\n",
    "# To fit/train the regression tree\n",
    "tree.fit(xTrSpiral, yTrSpiral)\n",
    "\n",
    "# To use the trained regression tree to make prediction\n",
    "pred = tree.predict(xTrSpiral)\n",
    "\n",
    "tr_err   = np.mean((np.sign(tree.predict(xTrSpiral)) - yTrSpiral)**2)\n",
    "te_err   = np.mean((np.sign(tree.predict(xTeSpiral)) - yTeSpiral)**2)\n",
    "\n",
    "print(\"Training error: %.4f\" % np.mean(np.sign(tree.predict(xTrSpiral)) != yTrSpiral))\n",
    "print(\"Testing error:  %.4f\" % np.mean(np.sign(tree.predict(xTeSpiral)) != yTeSpiral))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fc876beefe4ac7feb46d788d3353e7b8",
     "grade": false,
     "grade_id": "cell-a219c78df15f9ea0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<p>The following code defines a function <code>visclassifier()</code>, which plots the decision boundary of a classifier in 2 dimensions. Execute the following code to see what the decision boundary of your tree looks like on the spiral data set. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b7f72b5ef470dc4a256e47db3ed2886e",
     "grade": false,
     "grade_id": "cell-ea356e95d6f8a95d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def visclassifier(fun,xTr,yTr):\n",
    "    \"\"\"\n",
    "    visualize decision boundary\n",
    "    Define the symbols and colors we'll use in the plots later\n",
    "    \"\"\"\n",
    "\n",
    "    yTr = np.array(yTr).flatten()\n",
    "    \n",
    "    symbols = [\"ko\",\"kx\"]\n",
    "    marker_symbols = ['o', 'x']\n",
    "    mycolors = [[0.5, 0.5, 1], [1, 0.5, 0.5]]\n",
    "    # get the unique values from labels array\n",
    "    classvals = np.unique(yTr)\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    # return 300 evenly spaced numbers over this interval\n",
    "    res=300\n",
    "    xrange = np.linspace(min(xTr[:, 0]), max(xTr[:, 0]),res)\n",
    "    yrange = np.linspace(min(xTr[:, 1]), max(xTr[:, 1]),res)\n",
    "    \n",
    "    # repeat this matrix 300 times for both axes\n",
    "    pixelX = repmat(xrange, res, 1)\n",
    "    pixelY = repmat(yrange, res, 1).T\n",
    "\n",
    "    \n",
    "    xTe = np.array([pixelX.flatten(), pixelY.flatten()]).T\n",
    "\n",
    "    # test all of these points on the grid\n",
    "    testpreds = fun(xTe)\n",
    "    \n",
    "    # reshape it back together to make our grid\n",
    "    Z = testpreds.reshape(res, res)\n",
    "    # Z[0,0] = 1 # optional: scale the colors correctly\n",
    "    \n",
    "    # fill in the contours for these predictions\n",
    "    plt.contourf(pixelX, pixelY, np.sign(Z), colors=mycolors)\n",
    "\n",
    "    # creates x's and o's for training set\n",
    "    for idx, c in enumerate(classvals):\n",
    "        plt.scatter(xTr[yTr == c,0],\n",
    "            xTr[yTr == c,1],\n",
    "            marker=marker_symbols[idx],\n",
    "            color='k'\n",
    "                   )\n",
    "\n",
    "    plt.axis('tight')\n",
    "    # shows figure and blocks\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "tree=h.RegressionTree(depth=np.inf)\n",
    "tree.fit(xTrSpiral,yTrSpiral) # compute tree on training data \n",
    "visclassifier(lambda X: tree.predict(X),xTrSpiral,yTrSpiral)\n",
    "print(\"Training error: %.4f\" % np.mean(np.sign(tree.predict(xTrSpiral)) != yTrSpiral))\n",
    "print(\"Testing error:  %.4f\" % np.mean(np.sign(tree.predict(xTeSpiral)) != yTeSpiral))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "de3e874a5c7c7bd1335573d5a34d56d1",
     "grade": false,
     "grade_id": "cell-d1376bcc54a16465",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2>Bagging</h2>\n",
    "<p>CART trees are known to be high variance classifiers\n",
    "(if trained to full depth).\n",
    "An effective way to prevent overfitting is to use <b>Bagging</b>.\n",
    "Implement the function <code>forest</code>,\n",
    "which builds a forest of regression trees.\n",
    "Each tree should be built using training data\n",
    "drawn by randomly sampling $n$ examples\n",
    "from the training data with replacement.\n",
    "Do not randomly sample features.\n",
    "The function should output a list of trees.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e1a6362931436c96c811c323efc57304",
     "grade": false,
     "grade_id": "cell-18b7488c71f83d20",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def forest(xTr, yTr, m, maxdepth=np.inf):\n",
    "    \"\"\"Creates a random forest.\n",
    "    \n",
    "    Input:\n",
    "        xTr:      n x d matrix of data points\n",
    "        yTr:      n-dimensional vector of labels\n",
    "        m:        number of trees in the forest\n",
    "        maxdepth: maximum depth of tree\n",
    "        \n",
    "    Output:\n",
    "        trees: list of decision trees of length m\n",
    "    \"\"\"\n",
    "    \n",
    "    n, d = xTr.shape\n",
    "    trees = []\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4c19a9e84f38c49ceea1686a4b146622",
     "grade": false,
     "grade_id": "cell-223a74d51a10ce99",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def forest_test1():\n",
    "    m = 20\n",
    "    x = np.arange(100).reshape((100, 1))\n",
    "    y = np.arange(100)\n",
    "    trees = forest(x, y, m)\n",
    "    return len(trees) == m # make sure there are m trees in the forest\n",
    "\n",
    "def forest_test2():\n",
    "    m = 20\n",
    "    x = np.arange(100).reshape((100, 1))\n",
    "    y = np.arange(100)\n",
    "    max_depth = 4\n",
    "    trees = forest(x, y, m, max_depth)\n",
    "    depths_forest = np.array([tree.depth for tree in trees]) # Get the depth of all trees in the forest\n",
    "    return np.all(depths_forest == max_depth) # make sure that the max depth of all the trees is correct\n",
    "\n",
    "\n",
    "def forest_test3():\n",
    "    s = set()\n",
    "\n",
    "    def DFScollect(tree):\n",
    "        # Do Depth first search to all prediction in the tree\n",
    "        if tree.left is None and tree.right is None:\n",
    "            s.add(tree.prediction)\n",
    "        else:\n",
    "            DFScollect(tree.right)\n",
    "            DFScollect(tree.left)\n",
    "\n",
    "    m = 200\n",
    "    x = np.arange(100).reshape((100, 1))\n",
    "    y = np.arange(100)\n",
    "    trees = forest(x, y, m);\n",
    "\n",
    "    lens = np.zeros(m)\n",
    "\n",
    "    for i in range(m):\n",
    "        s.clear()\n",
    "        DFScollect(trees[i].root)\n",
    "        lens[i] = len(s)\n",
    "\n",
    "    # Check that about 63% of data is represented in each random sample\n",
    "    return abs(np.mean(lens) - 100 * (1 - 1 / np.exp(1))) < 2\n",
    "\n",
    "h.runtest(forest_test1, 'forest_test1')\n",
    "h.runtest(forest_test2, 'forest_test2')\n",
    "h.runtest(forest_test3, 'forest_test3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "54fa70ea09af0a38ae6db2820a018b9c",
     "grade": true,
     "grade_id": "cell-forest-test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs forest_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "59f4cca5ae2d595dd05629d0269700a3",
     "grade": true,
     "grade_id": "cell-forest-test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs forest_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "18c95683f1b767648bc61e8ba41a6282",
     "grade": true,
     "grade_id": "cell-forest-test3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs forest_test3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0ff530e2b4bbc20b8974c5520c52795a",
     "grade": false,
     "grade_id": "cell-0d951c782f2e7d46",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<p>Now implement the function <code>evalforest</code>, which should take as input a set of $m$ trees, a set of $n$ test inputs, and an $m$ dimensional weight vector. Each tree should be weighted by the corresponding weight. (For bagged decision trees you can define the weights to be $\\frac{1}{m}$ for all trees).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4450b256539af55f95624e2ee3511ae8",
     "grade": false,
     "grade_id": "cell-58cdd97f2bb636c0",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def evalforest(trees, X, alphas=None):\n",
    "    \"\"\"Evaluates X using trees.\n",
    "    \n",
    "    Input:\n",
    "        trees:  list of TreeNode decision trees of length m\n",
    "        X:      n x d matrix of data points\n",
    "        alphas: m-dimensional weight vector\n",
    "        \n",
    "    Output:\n",
    "        pred: n-dimensional vector of predictions\n",
    "    \"\"\"\n",
    "    m = len(trees)\n",
    "    n,d = X.shape\n",
    "    \n",
    "    if alphas is None:\n",
    "        alphas = np.ones(m) / len(trees)\n",
    "            \n",
    "    pred = np.zeros(n)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6679b56728a7989e556948f4fc6d9a94",
     "grade": false,
     "grade_id": "cell-05edd0f25e42b29a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def evalforest_test1():\n",
    "    m = 200\n",
    "    x = np.arange(100).reshape((100, 1))\n",
    "    y = np.arange(100)\n",
    "    trees = forest(x, y, m)\n",
    "    \n",
    "    preds = evalforest(trees, x)\n",
    "    return preds.shape == y.shape\n",
    "\n",
    "def evalforest_test2():\n",
    "    m = 200\n",
    "    x = np.ones(10).reshape((10, 1))\n",
    "    y = np.ones(10)\n",
    "    max_depth = 0\n",
    "    \n",
    "    # Create a forest with trees depth 0\n",
    "    # Since the data are all ones, each tree will return 1 as prediction\n",
    "    trees = forest(x, y, m, 0) \n",
    "    \n",
    "    # assign the k-th tree in the forest with weight k\n",
    "    alphas = np.arange(1, m + 1)\n",
    "    pred = evalforest(trees, np.ones((1, 1)), alphas)[0]\n",
    "    return pred == np.sum(alphas) # the prediction should be equal to the sum of weights\n",
    "    \n",
    "def bagging_test1():\n",
    "    m = 50\n",
    "    xTr = np.random.rand(500,3) - 0.5\n",
    "    yTr = np.sign(xTr[:,0] * xTr[:,1] * xTr[:,2]) # XOR Classification\n",
    "    xTe = np.random.rand(50,3) - 0.5\n",
    "    yTe = np.sign(xTe[:,0] * xTe[:,1] * xTe[:,2])\n",
    "\n",
    "    tree = h.RegressionTree(depth=5)\n",
    "    tree.fit(xTr, yTr)\n",
    "    oneacc = np.sum(np.sign(tree.predict(xTe)) == yTe)\n",
    "\n",
    "    trees = forest(xTr, yTr, m, maxdepth=5)\n",
    "    multiacc = np.sum(np.sign(evalforest(trees, xTe)) == yTe)\n",
    "\n",
    "    # Check that bagging yields improvement - or doesn't get too much worse\n",
    "    return multiacc * 1.1 >= oneacc\n",
    "\n",
    "def bagging_test2():\n",
    "    m = 50\n",
    "    xTr = (np.random.rand(500,3) - 0.5) * 4\n",
    "    yTr = xTr[:,0] * xTr[:,1] * xTr[:,2] # XOR Regression\n",
    "    xTe = (np.random.rand(50,3) - 0.5) * 4\n",
    "    yTe = xTe[:,0] * xTe[:,1] * xTe[:,2]\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    tree = h.RegressionTree(depth=3)\n",
    "    tree.fit(xTr, yTr)\n",
    "    oneerr = np.sum(np.sqrt((tree.predict(xTe) - yTe) ** 2))\n",
    "\n",
    "    trees = forest(xTr, yTr, m, maxdepth=3)\n",
    "    multierr = np.sum(np.sqrt((evalforest(trees, xTe) - yTe) ** 2))\n",
    "\n",
    "    # Check that bagging yields improvement - or doesn't get too much worse\n",
    "    return multierr <= oneerr * 1.5\n",
    "\n",
    "h.runtest(evalforest_test1, 'evalforest_test1')\n",
    "h.runtest(evalforest_test2, 'evalforest_test2')\n",
    "h.runtest(bagging_test1, 'bagging_test1')\n",
    "h.runtest(bagging_test2, 'bagging_test2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "019884dcf97cdccb16ae2cfe5b83ddce",
     "grade": true,
     "grade_id": "cell-evalforest-test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs evalforest-test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "94aa410d89836b20ec61a23c89b833f7",
     "grade": true,
     "grade_id": "cell-evalforest-test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs evalforest-test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f3dad43cc00473e419d493db7aff6e31",
     "grade": true,
     "grade_id": "cell-bagging-test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs bagging-test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a1d0f1908ac4204b5ab6452a5bc695bc",
     "grade": true,
     "grade_id": "cell-bagging-test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs bagging-test2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c0e3bb298f18ab0068815d96f61b138c",
     "grade": false,
     "grade_id": "cell-a84a7ba4a0e5d391",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<p>The following script visualizes the decision boundary of an ensemble of decision tree.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "46170c5267f1c5475b50d88ec7f2de4e",
     "grade": false,
     "grade_id": "cell-0b793c46d539ba5d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "trees=forest(xTrSpiral,yTrSpiral, 50) # compute tree on training data \n",
    "visclassifier(lambda X:evalforest(trees,X),xTrSpiral,yTrSpiral)\n",
    "print(\"Training error: %.4f\" % np.mean(np.sign(evalforest(trees,xTrSpiral)) != yTrSpiral))\n",
    "print(\"Testing error:  %.4f\" % np.mean(np.sign(evalforest(trees,xTeSpiral)) != yTeSpiral))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "300ec9d266d7e2edbcdff88f40f980c0",
     "grade": false,
     "grade_id": "cell-03a78b7b9ea09b9a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<p>The following script evaluates the test and training error of an ensemble of decision trees as we vary the number of trees.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3a405f6f1c338a739722b5ff93ab395c",
     "grade": false,
     "grade_id": "cell-4c5550a5997aaf04",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "M=20 # max number of trees\n",
    "err_trB=[]\n",
    "err_teB=[]\n",
    "alltrees=forest(xTrIon,yTrIon,M)\n",
    "for i in range(M):\n",
    "    trees=alltrees[:i+1]\n",
    "    trErr = np.mean(np.sign(evalforest(trees,xTrIon)) != yTrIon)\n",
    "    teErr = np.mean(np.sign(evalforest(trees,xTeIon)) != yTeIon)\n",
    "    err_trB.append(trErr)\n",
    "    err_teB.append(teErr)\n",
    "    print(\"[%d]training err = %.4f\\ttesting err = %.4f\" % (i,trErr, teErr))\n",
    "\n",
    "plt.figure()\n",
    "line_tr, = plt.plot(range(M),err_trB,label=\"Training Error\")\n",
    "line_te, = plt.plot(range(M),err_teB,label=\"Testing error\")\n",
    "plt.title(\"Random Forest\")\n",
    "plt.legend(handles=[line_tr, line_te])\n",
    "plt.xlabel(\"# of trees\")\n",
    "plt.ylabel(\"error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b832a001dc954babcfe5b91a37543ca9",
     "grade": false,
     "grade_id": "cell-614060398e2f62ea",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def onclick_forest(event):\n",
    "    \"\"\"\n",
    "    Visualize forest, including new point\n",
    "    \"\"\"\n",
    "    global xTrain,yTrain,w,b,M\n",
    "    # create position vector for new point\n",
    "    pos=np.array([[event.xdata,event.ydata]]) \n",
    "    if event.key == 'shift': # add positive point\n",
    "        color='or'\n",
    "        label=1\n",
    "    else: # add negative point\n",
    "        color='ob'\n",
    "        label=-1    \n",
    "    xTrain = np.concatenate((xTrain,pos), axis = 0)\n",
    "    yTrain = np.append(yTrain, label)\n",
    "    marker_symbols = ['o', 'x']\n",
    "    classvals = np.unique(yTrain)\n",
    "        \n",
    "    w = np.array(w).flatten()\n",
    "    \n",
    "    mycolors = [[0.5, 0.5, 1], [1, 0.5, 0.5]]\n",
    "    \n",
    "    # return 300 evenly spaced numbers over this interval\n",
    "    res=300\n",
    "    xrange = np.linspace(0, 1,res)\n",
    "    yrange = np.linspace(0, 1,res)\n",
    "    \n",
    "    # repeat this matrix 300 times for both axes\n",
    "    pixelX = repmat(xrange, res, 1)\n",
    "    pixelY = repmat(yrange, res, 1).T\n",
    "\n",
    "    xTe = np.array([pixelX.flatten(), pixelY.flatten()]).T\n",
    "\n",
    "    # get forest\n",
    "    trees=forest(xTrain,yTrain,M)\n",
    "    fun = lambda X:evalforest(trees,X)\n",
    "    # test all of these points on the grid\n",
    "    testpreds = fun(xTe)\n",
    "    \n",
    "    # reshape it back together to make our grid\n",
    "    Z = testpreds.reshape(res, res)\n",
    "    # Z[0,0] = 1 # optional: scale the colors correctly\n",
    "    \n",
    "    plt.cla()    \n",
    "    plt.xlim((0,1))\n",
    "    plt.ylim((0,1))\n",
    "    # fill in the contours for these predictions\n",
    "    plt.contourf(pixelX, pixelY, np.sign(Z), colors=mycolors)\n",
    "    \n",
    "    for idx, c in enumerate(classvals):\n",
    "        plt.scatter(xTrain[yTrain == c,0],\n",
    "            xTrain[yTrain == c,1],\n",
    "            marker=marker_symbols[idx],\n",
    "            color='k'\n",
    "            )\n",
    "    plt.show()\n",
    "    \n",
    "        \n",
    "xTrain= np.array([[5,6]])\n",
    "b=yTrIon\n",
    "yTrain = np.array([1])\n",
    "w=xTrIon\n",
    "M=20\n",
    "\n",
    "%matplotlib notebook\n",
    "fig = plt.figure()\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "cid = fig.canvas.mpl_connect('button_press_event', onclick_forest)\n",
    "print('Note: there is strong delay between points')\n",
    "plt.title('Use shift-click to add negative points.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0a98aa6b3381f05dd84d7bef10f83092",
     "grade": false,
     "grade_id": "cell-84ec95c505a4dbca",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2>Boosting</h2>\n",
    "\n",
    "<p>Another option to improve your decision trees is to build trees of small depth (e.g. only depth=3 or depth=4). These do not have high variance, but instead suffer from <b>high bias</b>. You can reduce the bias of a classifier with boosting. Implement the function <code>boosttree</code>, which applies Adaboost to the decision tree model. You should be able to use the function <code>evalforest</code> to evaluate your boosted ensemble (provdided you pass on the weights correctly.)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f8cc2ab4200d88ce0de1308a4b5d06ce",
     "grade": false,
     "grade_id": "cell-45a1c7c13850303b",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def boosttree(x,y,maxiter=100,maxdepth=2):\n",
    "    \"\"\"Learns a boosted decision tree.\n",
    "    \n",
    "    Input:\n",
    "        x:        n x d matrix of data points\n",
    "        y:        n-dimensional vector of labels\n",
    "        maxiter:  maximum number of trees\n",
    "        maxdepth: maximum depth of a tree\n",
    "        \n",
    "    Output:\n",
    "        forest: list of TreeNode decision trees of length m\n",
    "        alphas: m-dimensional weight vector\n",
    "        \n",
    "    (note, m is at most maxiter, but may be smaller,\n",
    "    as dictated by the Adaboost algorithm)\n",
    "    \"\"\"\n",
    "    assert np.allclose(np.unique(y), np.array([-1,1])); # the labels must be -1 and 1 \n",
    "    n,d = x.shape\n",
    "    weights = np.ones(n) / n\n",
    "    preds   = None\n",
    "    forest  = []\n",
    "    alphas  = []\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return forest, alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e3c7b98805014ac33406913bd41e13eb",
     "grade": false,
     "grade_id": "cell-boosttree-selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def boosting_test1():\n",
    "    maxiter = 50\n",
    "    maxdepth = 4\n",
    "    xTr = np.random.rand(500,3) - 0.5\n",
    "    yTr = np.sign(xTr[:,0] * xTr[:,1] * xTr[:,2]) # XOR Classification\n",
    "    xTe = np.random.rand(50,3) - 0.5\n",
    "    yTe = np.sign(xTe[:,0] * xTe[:,1] * xTe[:,2])\n",
    "    \n",
    "    out = boosttree(xTr, yTr, maxiter, maxdepth)\n",
    "    return len(out) == 2 #  boosttrees function should return a tuple\n",
    "\n",
    "def boosting_test2():\n",
    "    maxiter = 50\n",
    "    maxdepth = 4\n",
    "    xTr = np.random.rand(500,3) - 0.5\n",
    "    yTr = np.sign(xTr[:,0] * xTr[:,1] * xTr[:,2]) # XOR Classification\n",
    "    xTe = np.random.rand(50,3) - 0.5\n",
    "    yTe = np.sign(xTe[:,0] * xTe[:,1] * xTe[:,2])\n",
    "    \n",
    "    trees, alphas = boosttree(xTr, yTr, maxiter, maxdepth)\n",
    "    return len(trees) == len(alphas) #  the len of the two returned output should be the same\n",
    "\n",
    "def boosting_test3():\n",
    "    maxiter = 50\n",
    "    maxdepth = 3\n",
    "    xTr = np.random.rand(500,3) - 0.5\n",
    "    yTr = np.sign(xTr[:,0] * xTr[:,1] * xTr[:,2]) # XOR Classification\n",
    "    xTe = np.random.rand(50,3) - 0.5\n",
    "    yTe = np.sign(xTe[:,0] * xTe[:,1] * xTe[:,2])\n",
    "\n",
    "    tree = h.RegressionTree(maxdepth)\n",
    "    tree.fit(xTr, yTr)\n",
    "    oneacc = np.sum(np.sign(tree.predict(xTe)) == yTe)\n",
    "\n",
    "    trees, alphas = boosttree(xTr, yTr, maxiter, maxdepth)\n",
    "    multiacc = np.sum(np.sign(evalforest(trees, xTe, alphas)) == yTe)\n",
    "    return multiacc >= oneacc\n",
    "\n",
    "h.runtest(boosting_test1, 'boosting_test1')\n",
    "h.runtest(boosting_test2, 'boosting_test2')\n",
    "h.runtest(boosting_test3, 'boosting_test3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d839ce0e7f8a144f5262220e0dc8d68e",
     "grade": true,
     "grade_id": "cell-boosting-test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs boosting_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1b3f2c944681bc4a361b7f2d26737131",
     "grade": true,
     "grade_id": "cell-boosting-test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs boosting_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c0bfe5c4ebbcb7fb65bd0b0f7d31a3c1",
     "grade": true,
     "grade_id": "cell-boosting-test3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs boosting_test3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8dbe2eb939685f1ab95799d1602b29f3",
     "grade": false,
     "grade_id": "cell-2793b65603029f78",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<p>The following script evaluates the test and training error of a boosted forest as we increase the number of trees.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d1d254c6300c2f2f060ca0f14b596faf",
     "grade": false,
     "grade_id": "cell-4679f49ad1dcdb53",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "M=20 # max number of trees\n",
    "alltrees,allalphas=boosttree(xTrIon,yTrIon,maxdepth=3,maxiter=M)\n",
    "\n",
    "err_trB=[]\n",
    "loss_trB=[]\n",
    "err_teB=[]\n",
    "for i in range(M):\n",
    "    trees=alltrees[:i+1]\n",
    "    alphas=allalphas[:i+1]\n",
    "    trErr = np.mean(np.sign(evalforest(trees,xTrIon,alphas)) != yTrIon)\n",
    "    trLoss =np.mean(np.exp(-evalforest(trees,xTrIon,alphas)*yTrIon))\n",
    "    teErr = np.mean(np.sign(evalforest(trees,xTeIon,alphas)) != yTeIon)\n",
    "    err_trB.append(trErr)\n",
    "    err_teB.append(teErr)\n",
    "    loss_trB.append(trLoss)\n",
    "    print(\"[%d] exp loss = %.4f training err = %.4f\\ttesting err = %.4f\" % (i,trLoss,trErr, teErr))\n",
    "\n",
    "plt.figure()\n",
    "line_tr, = plt.plot(range(M),err_trB,label=\"Training Error\")\n",
    "line_te, = plt.plot(range(M),err_teB,label=\"Testing error\")\n",
    "line_trloss,=plt.plot(range(M),loss_trB,label='Exp. Loss')\n",
    "plt.title(\"Adaboost\")\n",
    "plt.legend(handles=[line_tr, line_te,line_trloss])\n",
    "plt.xlabel(\"# of trees\")\n",
    "plt.ylabel(\"error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b341db65bef9d4281bae83fe64f9fdb2",
     "grade": false,
     "grade_id": "cell-7840cb9a40e37454",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "trees,alphas=boosttree(xTrSpiral,yTrSpiral,maxdepth=3,maxiter=50)\n",
    "visclassifier(lambda X:evalforest(trees,X,alphas),xTrSpiral,yTrSpiral)\n",
    "\n",
    "print(\"Training error: %.4f\" % np.mean(np.sign(evalforest(trees,xTrSpiral)) != yTrSpiral))\n",
    "print(\"Testing error:  %.4f\" % np.mean(np.sign(evalforest(trees,xTeSpiral)) != yTeSpiral))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "117853bb8a72b15253ff58aeab3dbda6",
     "grade": false,
     "grade_id": "cell-53ad4beabd63cc86",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "xTrain= np.array([[5,6],[2,5]])\n",
    "yTrain = np.array([-1,1])\n",
    "fig = plt.figure()\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "\n",
    "\n",
    "def onclick_boost(event):\n",
    "    \"\"\"\n",
    "    Visualize boosting, including new point\n",
    "    \"\"\"\n",
    "    global xTrain,yTrain\n",
    "    # create position vector for new point\n",
    "    pos=np.array([[event.xdata,event.ydata]]) \n",
    "    if event.key == 'shift': # add positive point\n",
    "        color='or'\n",
    "        label=1\n",
    "    else: # add negative point\n",
    "        color='ob'\n",
    "        label=-1    \n",
    "    xTrain = np.concatenate((xTrain,pos), axis = 0)\n",
    "    yTrain = np.append(yTrain, label)\n",
    "    marker_symbols = ['o', 'x']\n",
    "    classvals = np.unique(yTrain)\n",
    "            \n",
    "    mycolors = [[0.5, 0.5, 1], [1, 0.5, 0.5]]\n",
    "    \n",
    "    # return 300 evenly spaced numbers over this interval\n",
    "    res=300\n",
    "    xrange = np.linspace(0,1,res)\n",
    "    yrange = np.linspace(0,1,res)\n",
    "    \n",
    "    # repeat this matrix 300 times for both axes\n",
    "    pixelX = repmat(xrange, res, 1)\n",
    "    pixelY = repmat(yrange, res, 1).T\n",
    "    xTe = np.array([pixelX.flatten(), pixelY.flatten()]).T\n",
    "    \n",
    "    # get forest\n",
    "    forest,alphas=boosttree(xTrain,yTrain,maxdepth=3,maxiter=5)\n",
    "    if len(forest) > 0:\n",
    "        fun = lambda X: evalforest(forest,X,alphas)\n",
    "        # test all of these points on the grid\n",
    "        testpreds = fun(xTe)\n",
    "\n",
    "        # reshape it back together to make our grid\n",
    "        Z = testpreds.reshape(res, res)\n",
    "        Z[0,0] = 1 # optional: scale the colors correctly\n",
    "\n",
    "        plt.cla()    \n",
    "        plt.xlim((0,1))\n",
    "        plt.ylim((0,1))\n",
    "        # fill in the contours for these predictions\n",
    "        plt.contourf(pixelX, pixelY, np.sign(Z), colors=mycolors)\n",
    "    \n",
    "    for idx, c in enumerate(classvals):\n",
    "        plt.scatter(xTrain[yTrain == c,0],\n",
    "            xTrain[yTrain == c,1],\n",
    "            marker=marker_symbols[idx],\n",
    "            color='k'\n",
    "            )\n",
    "    plt.show()\n",
    "    \n",
    "cid = fig.canvas.mpl_connect('button_press_event', onclick_boost)\n",
    "plt.title('Use shift-click to add negative points.')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
