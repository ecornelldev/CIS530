{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-138bf686eef190d3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2>About this Project</h2>\n",
    "<p>In this project, you will implement bagging and the random forest algorithm. </p>\n",
    "\n",
    "<h3>Evaluation</h3>\n",
    "\n",
    "<p><strong>This project must be successfully completed and submitted in order to receive credit for this course. Your score on this project will be included in your final grade calculation.</strong><p>\n",
    "    \n",
    "<p>You are expected to write code where you see <em># YOUR CODE HERE</em> within the cells of this notebook. Not all cells will be graded; code input cells followed by cells marked with <em>#Autograder test cell</em> will be graded. Upon submitting your work, the code you write at these designated positions will be assessed using an \"autograder\" that will run all test cells to assess your code. You will receive feedback from the autograder that will identify any errors in your code. Use this feedback to improve your code if you need to resubmit. Be sure not to change the names of any provided functions, classes, or variables within the existing code cells, as this will interfere with the autograder. Also, remember to execute all code cells sequentially, not just those you’ve edited, to ensure your code runs properly.</p>\n",
    "    \n",
    "<p>You can resubmit your work as many times as necessary before the submission deadline. If you experience difficulty or have questions about this exercise, use the Q&A discussion board to engage with your peers or seek assistance from the instructor.<p>\n",
    "\n",
    "<p>Before starting your work, please review <a href=\"https://s3.amazonaws.com/ecornell/global/eCornellPlagiarismPolicy.pdf\">eCornell's policy regarding plagiarism</a> (the presentation of someone else's work as your own without source credit).</p>\n",
    "\n",
    "<h3>Submit Code for Autograder Feedback</h3>\n",
    "\n",
    "<p>Once you have completed your work on this notebook, you will submit your code for autograder review. Follow these steps:</p>\n",
    "\n",
    "<ol>\n",
    "  <li><strong>Save your notebook.</strong></li>\n",
    "  <li><strong>Mark as Completed —</strong> In the blue menu bar along the top of this code exercise window, you’ll see a menu item called <strong>Education</strong>. In the <strong>Education</strong> menu, click <strong>Mark as Completed</strong> to submit your code for autograder/instructor review. This process will take a moment and a progress bar will show you the status of your submission.</li>\n",
    "\t<li><strong>Review your results —</strong> Once your work is marked as complete, the results of the autograder will automatically be presented in a new tab within the code exercise window. You can click on the assessment name in this feedback window to see more details regarding specific feedback/errors in your code submission.</li>\n",
    "  <li><strong>Repeat, if necessary —</strong> The Jupyter notebook will always remain accessible in the first tabbed window of the exercise. To reattempt the work, you will first need to click <strong>Mark as Uncompleted</strong> in the <strong>Education</strong> menu and then proceed to make edits to the notebook. Once you are ready to resubmit, follow steps one through three. You can repeat this procedure as many times as necessary.</li>\n",
    "</ol>\n",
    "<p>You can also download a copy of this notebook in multiple formats using the <strong>Download as</strong> option in the <strong>File</strong> menu above.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started\n",
    "\n",
    "Before you get started, let's import a few packages that you will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cfd4728219aa2e6f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pylab import *\n",
    "from numpy.matlib import repmat\n",
    "import sys\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "import time\n",
    "from helper import *\n",
    "%matplotlib notebook\n",
    "\n",
    "print('You\\'re running python %s' % sys.version.split(' ')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6613495f7b984ddf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In addition, we'll create a 2D spiral dataset of size 150 for visualization and a high dimensional dataset <a href=\"https://archive.ics.uci.edu/ml/datasets/Ionosphere\">ION</a>, which we will use as our binary test classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ea9d14b9dc5c69e4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "xTrSpiral,yTrSpiral,xTeSpiral,yTeSpiral= spiraldata(150)\n",
    "xTrIon,yTrIon,xTeIon,yTeIon= iondata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b74b5db289644016",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "You will use the regression tree from a previous project. As a reminder, the following code shows you how to instantiate a decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-425caeb78952a935",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a regression tree with no restriction on its depth\n",
    "# and weights for each training example to be 1\n",
    "# if you want to create a tree of max_depth k\n",
    "# then call RegressionTree(depth=k)\n",
    "tree = RegressionTree(depth=np.inf)\n",
    "\n",
    "# To fit/train the regression tree\n",
    "tree.fit(xTrSpiral, yTrSpiral)\n",
    "\n",
    "# To use the trained regression tree to make prediction\n",
    "pred = tree.predict(xTrSpiral)\n",
    "\n",
    "# To use the trained regression tree to make a +1/-1 prediction\n",
    "np.sign(tree.predict(xTrSpiral))\n",
    "        \n",
    "tr_err   = np.mean((np.sign(tree.predict(xTrSpiral)) - yTrSpiral)**2)\n",
    "te_err   = np.mean((np.sign(tree.predict(xTeSpiral)) - yTeSpiral)**2)\n",
    "\n",
    "print(\"Training error: %.4f\" % np.mean(np.sign(tree.predict(xTrSpiral)) != yTrSpiral))\n",
    "print(\"Testing error:  %.4f\" % np.mean(np.sign(tree.predict(xTeSpiral)) != yTeSpiral))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a219c78df15f9ea0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<p>The following code defines a function <code>visclassifier()</code>, which plots the decision boundary of a classifier in 2 dimensions. Execute the following code to see what the decision boundary of your tree looks like on the spiral data set. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ea356e95d6f8a95d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def visclassifier(fun,xTr,yTr):\n",
    "    \"\"\"\n",
    "    visualize decision boundary\n",
    "    Define the symbols and colors we'll use in the plots later\n",
    "    \"\"\"\n",
    "\n",
    "    yTr = np.array(yTr).flatten()\n",
    "    \n",
    "    symbols = [\"ko\",\"kx\"]\n",
    "    marker_symbols = ['o', 'x']\n",
    "    mycolors = [[0.5, 0.5, 1], [1, 0.5, 0.5]]\n",
    "    # get the unique values from labels array\n",
    "    classvals = np.unique(yTr)\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    # return 300 evenly spaced numbers over this interval\n",
    "    res=300\n",
    "    xrange = np.linspace(min(xTr[:, 0]), max(xTr[:, 0]),res)\n",
    "    yrange = np.linspace(min(xTr[:, 1]), max(xTr[:, 1]),res)\n",
    "    \n",
    "    # repeat this matrix 300 times for both axes\n",
    "    pixelX = repmat(xrange, res, 1)\n",
    "    pixelY = repmat(yrange, res, 1).T\n",
    "\n",
    "    \n",
    "    xTe = np.array([pixelX.flatten(), pixelY.flatten()]).T\n",
    "\n",
    "    # test all of these points on the grid\n",
    "    testpreds = fun(xTe)\n",
    "    \n",
    "    # reshape it back together to make our grid\n",
    "    Z = testpreds.reshape(res, res)\n",
    "    # Z[0,0] = 1 # optional: scale the colors correctly\n",
    "    \n",
    "    # fill in the contours for these predictions\n",
    "    plt.contourf(pixelX, pixelY, np.sign(Z), colors=mycolors)\n",
    "\n",
    "    # creates x's and o's for training set\n",
    "    for idx, c in enumerate(classvals):\n",
    "        plt.scatter(xTr[yTr == c,0],\n",
    "            xTr[yTr == c,1],\n",
    "            marker=marker_symbols[idx],\n",
    "            color='k'\n",
    "                   )\n",
    "\n",
    "    plt.axis('tight')\n",
    "    # shows figure and blocks\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "tree=RegressionTree(depth=np.inf)\n",
    "tree.fit(xTrSpiral,yTrSpiral) # compute tree on training data \n",
    "visclassifier(lambda X: tree.predict(X),xTrSpiral,yTrSpiral)\n",
    "print(\"Training error: %.4f\" % np.mean(np.sign(tree.predict(xTrSpiral)) != yTrSpiral))\n",
    "print(\"Testing error:  %.4f\" % np.mean(np.sign(tree.predict(xTeSpiral)) != yTeSpiral))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d1376bcc54a16465",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2>Bagging in Action</h2>\n",
    "<h3>Part One: Implement <code>forest</code> [Graded]</h3>\n",
    "<p>CART trees are known to be high variance classifiers\n",
    "(if trained to full depth).\n",
    "    An effective way to prevent overfitting is to use <b>Bagging</b> (short for <strong>b</strong>ootstrap <strong>ag</strong>gregating).\n",
    "Implement the function <code>forest</code>,\n",
    "which builds a forest of regression trees.\n",
    "Each tree should be built using training data\n",
    "drawn by randomly sampling $n$ examples\n",
    "from the training data with replacement.\n",
    "    <em>Do not</em> randomly sample features.\n",
    "The function should output a list of trees.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-forest",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def forest(xTr, yTr, m, maxdepth=np.inf):\n",
    "    \"\"\"Creates a random forest.\n",
    "    \n",
    "    Input:\n",
    "        xTr:      n x d matrix of data points\n",
    "        yTr:      n-dimensional vector of labels\n",
    "        m:        number of trees in the forest\n",
    "        maxdepth: maximum depth of tree\n",
    "        \n",
    "    Output:\n",
    "        trees: list of decision trees of length m\n",
    "    \"\"\"\n",
    "    \n",
    "    n, d = xTr.shape\n",
    "    trees = []\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    for i in range(m):\n",
    "        indices = np.random.choice(n, n)\n",
    "        tree = RegressionTree(depth=maxdepth)\n",
    "        tree.fit(xTr[indices, :], yTr[indices])\n",
    "        trees.append(tree)\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    return trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-forest-selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def forest_test1():\n",
    "    m = 20\n",
    "    x = np.arange(100).reshape((100, 1))\n",
    "    y = np.arange(100)\n",
    "    trees = forest(x, y, m)\n",
    "    return len(trees) == m # make sure there are m trees in the forest\n",
    "\n",
    "def forest_test2():\n",
    "    m = 20\n",
    "    x = np.arange(100).reshape((100, 1))\n",
    "    y = np.arange(100)\n",
    "    max_depth = 4\n",
    "    trees = forest(x, y, m, max_depth)\n",
    "    depths_forest = np.array([tree.depth for tree in trees]) # Get the depth of all trees in the forest\n",
    "    return np.all(depths_forest == max_depth) # make sure that the max depth of all the trees is correct\n",
    "\n",
    "\n",
    "def forest_test3():\n",
    "    s = set()\n",
    "\n",
    "    def DFScollect(tree):\n",
    "        # Do Depth first search to all prediction in the tree\n",
    "        if tree.left is None and tree.right is None:\n",
    "            s.add(tree.prediction)\n",
    "        else:\n",
    "            DFScollect(tree.right)\n",
    "            DFScollect(tree.left)\n",
    "\n",
    "    m = 200\n",
    "    x = np.arange(100).reshape((100, 1))\n",
    "    y = np.arange(100)\n",
    "    trees = forest(x, y, m);\n",
    "\n",
    "    lens = np.zeros(m)\n",
    "\n",
    "    for i in range(m):\n",
    "        s.clear()\n",
    "        DFScollect(trees[i].root)\n",
    "        lens[i] = len(s)\n",
    "\n",
    "    # Check that about 63% of data is represented in each random sample\n",
    "    return abs(np.mean(lens) - 100 * (1 - 1 / np.exp(1))) < 2\n",
    "\n",
    "runtest(forest_test1, 'forest_test1')\n",
    "runtest(forest_test2, 'forest_test2')\n",
    "runtest(forest_test3, 'forest_test3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-forest-test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs forest_test1\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "m = 20\n",
    "x = np.arange(100).reshape((100, 1))\n",
    "y = np.arange(100)\n",
    "trees = forest(x, y, m)\n",
    "assert len(trees) == m # make sure there are n trees in the forest\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-forest-test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs forest_test2\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "m = 20\n",
    "x = np.arange(100).reshape((100, 1))\n",
    "y = np.arange(100)\n",
    "max_depth = 4\n",
    "trees = forest(x, y, m, max_depth)\n",
    "depths_forest = np.array([tree.depth for tree in trees]) # Get the depth of all trees in the forest\n",
    "assert np.all(depths_forest == max_depth)\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-forest-test3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs forest_test3\n",
    "### BEGIN HIDDEN TESTS\n",
    "s = set()\n",
    "\n",
    "def DFScollect(tree):\n",
    "    # Do Depth first search to all prediction in the tree\n",
    "    if tree.left is None and tree.right is None:\n",
    "        s.add(tree.prediction)\n",
    "    else:\n",
    "        DFScollect(tree.right)\n",
    "        DFScollect(tree.left)\n",
    "\n",
    "m = 200\n",
    "x = np.arange(100).reshape((100, 1))\n",
    "y = np.arange(100)\n",
    "trees = forest(x, y, m)\n",
    "\n",
    "lens = np.zeros(m)\n",
    "\n",
    "for i in range(m):\n",
    "    s.clear()\n",
    "    DFScollect(trees[i].root)\n",
    "    lens[i] = len(s)\n",
    "\n",
    "# Check that about 63% of data is represented in each random sample\n",
    "assert abs(np.mean(lens) - 100 * (1 - 1 / np.exp(1))) < 2\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0d951c782f2e7d46",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Part Two: Implement <code>evalforest</code> [Graded]</h3>\n",
    "\n",
    "<p>Now implement the function <code>evalforest</code>, which should take as input a set of $m$ trees and a set of $n$ test inputs and return the average prediction of all the trees.</p>\n",
    "\n",
    "<p>Note that for bagging, we take the average over all trees weighted equally. In a later exercise, you will implement a different version of this function that assigns different weights to different trees.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-evalforest",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def evalforest(trees, X):\n",
    "    \"\"\"Evaluates X using trees.\n",
    "    \n",
    "    Input:\n",
    "        trees:  list of TreeNode decision trees of length m\n",
    "        X:      n x d matrix of data points\n",
    "        alphas: m-dimensional weight vector\n",
    "        \n",
    "    Output:\n",
    "        pred: n-dimensional vector of predictions\n",
    "    \"\"\"\n",
    "    m = len(trees)\n",
    "    n,d = X.shape\n",
    "    \n",
    "    pred = np.zeros(n)\n",
    "    \n",
    "    # BEGIN SOLUTION\n",
    "    alpha = 1/m\n",
    "    for t in range(m):\n",
    "        pred += alpha * trees[t].predict(X)      \n",
    "    # END SOLUTION\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-evalforest-selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def evalforest_test1():\n",
    "    m = 200\n",
    "    x = np.arange(100).reshape((100, 1))\n",
    "    y = np.arange(100)\n",
    "    trees = forest(x, y, m)\n",
    "    \n",
    "    preds = evalforest(trees, x)\n",
    "    return preds.shape == y.shape\n",
    "\n",
    "def evalforest_test2():\n",
    "    m = 200\n",
    "    x = np.ones(10).reshape((10, 1))\n",
    "    y = np.ones(10)\n",
    "    max_depth = 0\n",
    "    \n",
    "    # Create a forest with trees depth 0\n",
    "    # Since the data are all ones, each tree will return 1 as prediction\n",
    "    trees = forest(x, y, m, 0) \n",
    "    pred = evalforest(trees, np.ones((1, 1)))[0]\n",
    "    return np.isclose(pred,1) # the prediction should be equal to the sum of weights\n",
    "    \n",
    "def bagging_test1():\n",
    "    m = 50\n",
    "    xTr = np.random.rand(500,3) - 0.5\n",
    "    yTr = np.sign(xTr[:,0] * xTr[:,1] * xTr[:,2]) # XOR Classification\n",
    "    xTe = np.random.rand(50,3) - 0.5\n",
    "    yTe = np.sign(xTe[:,0] * xTe[:,1] * xTe[:,2])\n",
    "\n",
    "    tree = RegressionTree(depth=5)\n",
    "    tree.fit(xTr, yTr)\n",
    "    oneacc = np.sum(np.sign(tree.predict(xTe)) == yTe)\n",
    "\n",
    "    trees = forest(xTr, yTr, m, maxdepth=5)\n",
    "    multiacc = np.sum(np.sign(evalforest(trees, xTe)) == yTe)\n",
    "\n",
    "    # Check that bagging yields improvement - or doesn't get too much worse\n",
    "    return multiacc * 1.1 >= oneacc\n",
    "\n",
    "def bagging_test2():\n",
    "    m = 50\n",
    "    xTr = (np.random.rand(500,3) - 0.5) * 4\n",
    "    yTr = xTr[:,0] * xTr[:,1] * xTr[:,2] # XOR Regression\n",
    "    xTe = (np.random.rand(50,3) - 0.5) * 4\n",
    "    yTe = xTe[:,0] * xTe[:,1] * xTe[:,2]\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    tree = RegressionTree(depth=3)\n",
    "    tree.fit(xTr, yTr)\n",
    "    oneerr = np.sum(np.sqrt((tree.predict(xTe) - yTe) ** 2))\n",
    "\n",
    "    trees = forest(xTr, yTr, m, maxdepth=3)\n",
    "    multierr = np.sum(np.sqrt((evalforest(trees, xTe) - yTe) ** 2))\n",
    "\n",
    "    # Check that bagging yields improvement - or doesn't get too much worse\n",
    "    return multierr <= oneerr * 1.5\n",
    "\n",
    "runtest(evalforest_test1, 'evalforest_test1')\n",
    "runtest(evalforest_test2, 'evalforest_test2')\n",
    "runtest(bagging_test1, 'bagging_test1')\n",
    "runtest(bagging_test2, 'bagging_test2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-evalforest-test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs evalforest-test1\n",
    "### BEGIN HIDDEN TESTS\n",
    "m = 200\n",
    "x = np.arange(100).reshape((100, 1))\n",
    "y = np.arange(100)\n",
    "trees = forest(x, y, m)\n",
    "\n",
    "preds = evalforest(trees, x)\n",
    "assert preds.shape == y.shape\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-evalforest-test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs evalforest-test2\n",
    "### BEGIN HIDDEN TESTS\n",
    "m = 200\n",
    "x = np.ones(10).reshape((10, 1))\n",
    "y = np.ones(10)\n",
    "max_depth = 0\n",
    "\n",
    "# Create a forest with trees depth 0\n",
    "# Since the data are all ones, each tree will return 1 as prediction\n",
    "trees = forest(x, y, m, 0) \n",
    "pred = evalforest(trees, np.ones((1, 1)))[0]\n",
    "assert np.isclose(pred,1) # the prediction should be equal to the sum of weights\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-bagging-test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs bagging-test1\n",
    "### BEGIN HIDDEN TESTS\n",
    "n = 50\n",
    "xTr = np.random.rand(500,3) - 0.5\n",
    "yTr = np.sign(xTr[:,0] * xTr[:,1] * xTr[:,2]) # XOR Classification\n",
    "xTe = np.random.rand(50,3) - 0.5\n",
    "yTe = np.sign(xTe[:,0] * xTe[:,1] * xTe[:,2])\n",
    "\n",
    "tree = RegressionTree(depth=5)\n",
    "tree.fit(xTr, yTr)\n",
    "oneacc = np.sum(np.sign(tree.predict(xTe)) == yTe)\n",
    "\n",
    "trees = forest(xTr, yTr, n, maxdepth=5)\n",
    "multiacc = np.sum(np.sign(evalforest(trees, xTe)) == yTe)\n",
    "\n",
    "# Check that bagging yields improvement - or doesn't get too much worse\n",
    "assert multiacc * 1.1 >= oneacc\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-bagging-test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs bagging-test2\n",
    "### BEGIN HIDDEN TESTS\n",
    "n = 50\n",
    "xTr = (np.random.rand(500,3) - 0.5) * 4\n",
    "yTr = xTr[:,0] * xTr[:,1] * xTr[:,2] # XOR Regression\n",
    "xTe = (np.random.rand(50,3) - 0.5) * 4\n",
    "yTe = xTe[:,0] * xTe[:,1] * xTe[:,2]\n",
    "\n",
    "np.random.seed(1)\n",
    "tree = RegressionTree(depth=3)\n",
    "tree.fit(xTr, yTr)\n",
    "oneerr = np.sum(np.sqrt((tree.predict(xTe) - yTe) ** 2))\n",
    "\n",
    "trees = forest(xTr, yTr, n, maxdepth=3)\n",
    "multierr = np.sum(np.sqrt((evalforest(trees, xTe) - yTe) ** 2))\n",
    "\n",
    "# Check that bagging yields improvement - or doesn't get too much worse\n",
    "assert multierr <= oneerr * 1.5\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a84a7ba4a0e5d391",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Visualize the Decision Boundary</h3>\n",
    "\n",
    "<p>The following script visualizes the decision boundary of an ensemble of decision tree.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0b793c46d539ba5d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trees=forest(xTrSpiral,yTrSpiral, 50) # compute tree on training data \n",
    "visclassifier(lambda X:evalforest(trees,X),xTrSpiral,yTrSpiral)\n",
    "print(\"Training error: %.4f\" % np.mean(np.sign(evalforest(trees,xTrSpiral)) != yTrSpiral))\n",
    "print(\"Testing error:  %.4f\" % np.mean(np.sign(evalforest(trees,xTeSpiral)) != yTeSpiral))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-03a78b7b9ea09b9a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Evaluate Test and Training Error</h3>\n",
    "\n",
    "<p>The following script evaluates the test and training error of an ensemble of decision trees as we vary the number of trees.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4c5550a5997aaf04",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "M=20 # max number of trees\n",
    "err_trB=[]\n",
    "err_teB=[]\n",
    "alltrees=forest(xTrIon,yTrIon,M)\n",
    "for i in range(M):\n",
    "    trees=alltrees[:i+1]\n",
    "    trErr = np.mean(np.sign(evalforest(trees,xTrIon)) != yTrIon)\n",
    "    teErr = np.mean(np.sign(evalforest(trees,xTeIon)) != yTeIon)\n",
    "    err_trB.append(trErr)\n",
    "    err_teB.append(teErr)\n",
    "    print(\"[%d]training err = %.4f\\ttesting err = %.4f\" % (i,trErr, teErr))\n",
    "\n",
    "plt.figure()\n",
    "line_tr, = plt.plot(range(M),err_trB,label=\"Training Error\")\n",
    "line_te, = plt.plot(range(M),err_teB,label=\"Testing error\")\n",
    "plt.title(\"Random Forest\")\n",
    "plt.legend(handles=[line_tr, line_te])\n",
    "plt.xlabel(\"# of trees\")\n",
    "plt.ylabel(\"error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Interactive Demo</h3>\n",
    "\n",
    "The following demo visualizes the bagged classifier. Add your own points directly on the graph with click and shift+click to see the prediction boundaries. There will be a delay between clicks as the new classifier is trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-614060398e2f62ea",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def onclick_forest(event):\n",
    "    \"\"\"\n",
    "    Visualize forest, including new point\n",
    "    \"\"\"\n",
    "    global xTrain,yTrain,w,b,M\n",
    "    # create position vector for new point\n",
    "    pos=np.array([[event.xdata,event.ydata]]) \n",
    "    if event.key == 'shift': # add positive point\n",
    "        color='or'\n",
    "        label=1\n",
    "    else: # add negative point\n",
    "        color='ob'\n",
    "        label=-1    \n",
    "    xTrain = np.concatenate((xTrain,pos), axis = 0)\n",
    "    yTrain = np.append(yTrain, label)\n",
    "    marker_symbols = ['o', 'x']\n",
    "    classvals = np.unique(yTrain)\n",
    "        \n",
    "    w = np.array(w).flatten()\n",
    "    \n",
    "    mycolors = [[0.5, 0.5, 1], [1, 0.5, 0.5]]\n",
    "    \n",
    "    # return 300 evenly spaced numbers over this interval\n",
    "    res=300\n",
    "    xrange = np.linspace(0, 1,res)\n",
    "    yrange = np.linspace(0, 1,res)\n",
    "    \n",
    "    # repeat this matrix 300 times for both axes\n",
    "    pixelX = repmat(xrange, res, 1)\n",
    "    pixelY = repmat(yrange, res, 1).T\n",
    "\n",
    "    xTe = np.array([pixelX.flatten(), pixelY.flatten()]).T\n",
    "\n",
    "    # get forest\n",
    "    trees=forest(xTrain,yTrain,M)\n",
    "    fun = lambda X:evalforest(trees,X)\n",
    "    # test all of these points on the grid\n",
    "    testpreds = fun(xTe)\n",
    "    \n",
    "    # reshape it back together to make our grid\n",
    "    Z = testpreds.reshape(res, res)\n",
    "    # Z[0,0] = 1 # optional: scale the colors correctly\n",
    "    \n",
    "    plt.cla()    \n",
    "    plt.xlim((0,1))\n",
    "    plt.ylim((0,1))\n",
    "    # fill in the contours for these predictions\n",
    "    plt.contourf(pixelX, pixelY, np.sign(Z), colors=mycolors)\n",
    "    \n",
    "    for idx, c in enumerate(classvals):\n",
    "        plt.scatter(xTrain[yTrain == c,0],\n",
    "            xTrain[yTrain == c,1],\n",
    "            marker=marker_symbols[idx],\n",
    "            color='k'\n",
    "            )\n",
    "    plt.show()\n",
    "    \n",
    "        \n",
    "xTrain= np.array([[5,6]])\n",
    "b=yTrIon\n",
    "yTrain = np.array([1])\n",
    "w=xTrIon\n",
    "M=20\n",
    "\n",
    "%matplotlib notebook\n",
    "fig = plt.figure()\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "cid = fig.canvas.mpl_connect('button_press_event', onclick_forest)\n",
    "print('Note: there is strong delay between points')\n",
    "plt.title('Use shift-click to add negative points.')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
