{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1e2b137b9f48bf8c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2>About this Project</h2>\n",
    "<p>In this project, you will compute the bias, variance, and noise.</p>\n",
    "\n",
    "<h3>Evaluation</h3>\n",
    "\n",
    "<p><strong>This project must be successfully completed and submitted in order to receive credit for this course. Your score on this project will be included in your final grade calculation.</strong><p>\n",
    "    \n",
    "<p>You are expected to write code where you see <em># YOUR CODE HERE</em> within the cells of this notebook. Not all cells will be graded; code input cells followed by cells marked with <em>#Autograder test cell</em> will be graded. Upon submitting your work, the code you write at these designated positions will be assessed using an \"autograder\" that will run all test cells to assess your code. You will receive feedback from the autograder that will identify any errors in your code. Use this feedback to improve your code if you need to resubmit. Be sure not to change the names of any provided functions, classes, or variables within the existing code cells, as this will interfere with the autograder. Also, remember to execute all code cells sequentially, not just those you’ve edited, to ensure your code runs properly.</p>\n",
    "    \n",
    "<p>You can resubmit your work as many times as necessary before the submission deadline. If you experience difficulty or have questions about this exercise, use the Q&A discussion board to engage with your peers or seek assistance from the instructor.<p>\n",
    "\n",
    "<p>Before starting your work, please review <a href=\"https://s3.amazonaws.com/ecornell/global/eCornellPlagiarismPolicy.pdf\">eCornell's policy regarding plagiarism</a> (the presentation of someone else's work as your own without source credit).</p>\n",
    "\n",
    "<h3>Submit Code for Autograder Feedback</h3>\n",
    "\n",
    "<p>Once you have completed your work on this notebook, you will submit your code for autograder review. Follow these steps:</p>\n",
    "\n",
    "<ol>\n",
    "  <li><strong>Save your notebook.</strong></li>\n",
    "  <li><strong>Mark as Completed —</strong> In the blue menu bar along the top of this code exercise window, you’ll see a menu item called <strong>Education</strong>. In the <strong>Education</strong> menu, click <strong>Mark as Completed</strong> to submit your code for autograder/instructor review. This process will take a moment and a progress bar will show you the status of your submission.</li>\n",
    "\t<li><strong>Review your results —</strong> Once your work is marked as complete, the results of the autograder will automatically be presented in a new tab within the code exercise window. You can click on the assessment name in this feedback window to see more details regarding specific feedback/errors in your code submission.</li>\n",
    "  <li><strong>Repeat, if necessary —</strong> The Jupyter notebook will always remain accessible in the first tabbed window of the exercise. To reattempt the work, you will first need to click <strong>Mark as Uncompleted</strong> in the <strong>Education</strong> menu and then proceed to make edits to the notebook. Once you are ready to resubmit, follow steps one through three. You can repeat this procedure as many times as necessary.</li>\n",
    "</ol>\n",
    "<p>You can also download a copy of this notebook in multiple formats using the <strong>Download as</strong> option in the <strong>File</strong> menu above.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-53fdcf615afb6769",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2>Bias-Variance Decomposition</h2>\n",
    "\n",
    "<p>\n",
    "Recall that the squared error can be decomposed into <em>bias</em>, <em>variance</em> and <em>noise</em>: \n",
    "$$\n",
    "    \\underbrace{\\mathbb{E}[(h_D(\\mathbf{x}) - y)^2]}_\\mathrm{Error} = \\underbrace{\\mathbb{E}[(h_D(\\mathbf{x})-\\bar{h}(\\mathbf{x}))^2]}_\\mathrm{Variance} + \\underbrace{\\mathbb{E}[(\\bar{h}(\\mathbf{x})-\\bar{y}(\\mathbf{x}))^2]}_\\mathrm{Bias} + \\underbrace{\\mathbb{E}[(\\bar{y}(\\mathbf{x})-y(\\mathbf{x}))^2]}_\\mathrm{Noise}\\nonumber\n",
    "$$\n",
    "    \n",
    "We will now create a data set for which we can approximately compute this decomposition. \n",
    "The function <strong>`toydata`</strong> generates a binary data set with class $1$ and $2$. Both are sampled from Gaussian distributions:\n",
    "$$\n",
    "p(\\mathbf{x}|y=1)\\sim {\\mathcal{N}}(0,{I}) \\textrm { and } p(\\mathbf{x}|y=2)\\sim {\\mathcal{N}}(\\mu_2,{I}),\n",
    "$$\n",
    "\n",
    "where $\\mathbf{\\mu_2}=[1.75, 1.75]^\\top$ (the global variable <code>OFFSET</code> $\\!=\\!1.75$ regulates these values: $\\mathbf{\\mu_2}=[$<code>OFFSET</code> $, $ <code>OFFSET</code>$]^\\top$).\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Computing noise, bias and variance</h3>\n",
    "<p>\n",
    "    You will need to edit five functions: <strong><code>computeybar</code></strong>,<strong><code>computenoise</code></strong>, <strong><code>computehbar</code></strong>, <strong><code>computenoise</code></strong>, and <strong><code>computevariance</code></strong>. The functions <strong><code>computeybar</code></strong> and <strong><code>computehbar</code></strong> are helper functions to calculate the three components of squared error.\n",
    "    \n",
    "First take a look at <strong><code>biasvariancedemo</code></strong> and make sure you understand where each function should be called and how they contribute to the Bias/Variance/Noise decomposition. <br/><br/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "Before you get started, let's import a few packages that you will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e5ced222ffb03a72",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.matlib import repmat\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from helper import *\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print('You\\'re running python %s' % sys.version.split(' ')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-10f3cee1068653f1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3><code>toydata</code> Helper Function</h3> \n",
    "\n",
    "<p><code>toydata</code> is a helper function used to generate the the binary data with $n/2$ values in class 1 and $n/2$ values in class 2. Class 1 is the label for data drawn from a normal distribution with mean $\\mu = 0$ and standard deviation $\\sigma = 1$. Class 2 is the label for data drawn from a normal distribution with $\\mu = $<code>OFFSET</code> and $\\sigma = 1$. Run the code below to get a visualization of the data.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-68beb9e0fa121c43",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "OFFSET = 1.75\n",
    "X, y = toydata(OFFSET, 1000)\n",
    "\n",
    "# Visualize the generated data\n",
    "ind1 = y == 1\n",
    "ind2 = y == 2\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(X[ind1, 0], X[ind1, 1], c='r', marker='o', label='Class 1')\n",
    "plt.scatter(X[ind2, 0], X[ind2, 1], c='b', marker='o', label='Class 2')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d4332b3b2f191280",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Part One: Noise [Graded]</h3>\n",
    "<p>First, let's focus on the noise. For this, you need to compute the expected label for a given input $\\mathbf{x}$:\n",
    "    \n",
    "$$\\bar y(\\mathbf{x})=1*p(y=1\\;|\\;\\mathbf{x})+2*p(y=2\\;|\\;\\mathbf{x})$$ \n",
    "\n",
    "in  <strong><code>computeybar</code></strong>. You can compute the probability $p(\\mathbf{x}|y)$ with the equations $p(\\mathbf{x}|y=1)\\sim {\\mathcal{N}}(0,{I}) \\textrm { and } p(\\mathbf{x}\\;|\\;y=2)\\sim {\\mathcal{N}}(\\mathbf{\\mu_2},{I})$. Then use Bayes rule to compute  </p><br>\n",
    "$$p(y=1|\\mathbf{x})=\\frac{p(\\mathbf{x}\\;|\\;y=1)p(y=1)}{p(\\mathbf{x}\\;|\\;y=1)p(y=1)+p(\\mathbf{x}\\;|\\;y=2)p(y=2)}$$\n",
    "    \n",
    "<p><strong>Note:</strong> You may want to use the function <em>`normpdf`</em> to compute $p(\\mathbf{x}\\;|\\;y_i)$, which is defined for  you in <strong><code>computeybar</code></strong> in the following cell. Note that <code>normpdf</code> only computes $p(x\\;|\\;y)$ in a single dimension, but you can use the fact that both dimensions are independent to obtain for example $p(\\mathbf{x}\\;|\\;y=1)=p(x_1\\;|\\;y=1)p(x_2\\;|\\;y=1)$. \n",
    "<br/><br/></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-computeybar",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def computeybar(xTe, OFFSET):\n",
    "    \"\"\"\n",
    "    function [ybar]=computeybar(xTe, OFFSET);\n",
    "\n",
    "    computes the expected label 'ybar' for a set of inputs x\n",
    "    generated from two standard Normal distributions (one offset by OFFSET in\n",
    "    both dimensions.)\n",
    "\n",
    "    INPUT:\n",
    "    xTe       : nx2 array of n vectors with 2 dimensions\n",
    "    OFFSET    : The OFFSET passed into the toyData function. The difference in the\n",
    "                mu of labels class1 and class2 for toyData.\n",
    "\n",
    "    OUTPUT:\n",
    "    ybar : a nx1 vector of the expected labels for vectors xTe\n",
    "    noise: \n",
    "    \"\"\"\n",
    "    n, d = xTe.shape\n",
    "    ybar = np.zeros(n)\n",
    "    \n",
    "    # Feel free to use the following function to compute p(x|y)\n",
    "    # By default, mean is 0 and std. deviation is 1.\n",
    "    normpdf = lambda x, mu, sigma: np.exp(-0.5 * np.power((x - mu) / sigma, 2)) / (np.sqrt(2 * np.pi) * sigma)\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    class1 = normpdf(xTe, 0, 1)\n",
    "    class2 = normpdf(xTe, OFFSET, 1)\n",
    "    \n",
    "    class1 = np.multiply(class1[:,0], class1[:,1])\n",
    "    class2 = np.multiply(class2[:,0], class2[:,1])\n",
    "    \n",
    "    num = class1 + 2*class2\n",
    "    den = class1 + class2\n",
    "    \n",
    "    ybar = num / den\n",
    "    ### END SOLUTION\n",
    "    return ybar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ybar-selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def test_ybar1():\n",
    "    OFFSET = 2\n",
    "    n = 1000\n",
    "    xTe, yTe = toydata(OFFSET, n) # Generate n datapoints\n",
    "    ybar = computeybar(xTe, OFFSET)\n",
    "    \n",
    "    return ybar.shape == (n, ) # the output of your ybar should be a n dimensional array\n",
    "\n",
    "def test_ybar2():\n",
    "    OFFSET = 50\n",
    "    # Create an easy dataset\n",
    "    # We set sigma=1 and since the mean is far apart,\n",
    "    # the noise is negligible\n",
    "    xTe = np.array([\n",
    "        [49.308783, 49.620651], \n",
    "        [1.705462, 1.885418], \n",
    "        [ 51.192402, 50.256330],\n",
    "        [0.205998, -0.089885],\n",
    "        [50.853083, 51.833237]])  \n",
    "    yTe = np.array([2, 1, 2, 1, 2])\n",
    "    \n",
    "    ybar = computeybar(xTe, OFFSET)\n",
    "    return np.isclose(np.mean(np.power(yTe - ybar, 2)), 0)\n",
    "\n",
    "def test_ybar3():\n",
    "    OFFSET = 3;\n",
    "\n",
    "    xTe = np.array([\n",
    "        [0.45864, 0.71552],\n",
    "        [2.44662, 1.68167],\n",
    "        [1.00345, 0.15182],\n",
    "        [-0.10560, -0.48155],\n",
    "        [3.07264, 3.81535],\n",
    "        [3.13035, 2.72151],\n",
    "        [2.25265, 3.78697]])\n",
    "    yTe = np.array([1, 2, 1, 1, 2, 2, 2])\n",
    "    \n",
    "    ybar = computeybar(xTe, OFFSET)\n",
    "    \n",
    "    return np.mean(np.power(yTe - ybar, 2)) < 0.0002 # make sure the noise is small\n",
    "\n",
    "runtest(test_ybar1, 'test_ybar1')\n",
    "runtest(test_ybar2, 'test_ybar2')\n",
    "runtest(test_ybar3, 'test_ybar3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ybar1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_ybar1\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "OFFSET = 2\n",
    "n = 1000\n",
    "xTe, yTe = toydata(OFFSET, n) # Generate n datapoints\n",
    "ybar = computeybar(xTe, OFFSET)\n",
    "\n",
    "assert ybar.shape == (n, )\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ybar2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_ybar2\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "OFFSET = 50\n",
    "# Create an easy dataset\n",
    "# We set sigma=1 and since the mean is far apart,\n",
    "# there wouldn't be any noise\n",
    "xTe = np.array([\n",
    "    [49.308783, 49.620651], \n",
    "    [1.705462, 1.885418], \n",
    "    [ 51.192402, 50.256330],\n",
    "    [0.205998, -0.089885],\n",
    "    [50.853083, 51.833237]])  \n",
    "yTe = np.array([2, 1, 2, 1, 2])\n",
    "\n",
    "ybar = computeybar(xTe, OFFSET)\n",
    "noise = np.mean(np.power(yTe - ybar, 2)) # calculate the noise\n",
    "assert np.isclose(noise, 0)\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ybar3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_ybar3\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "OFFSET = 3;\n",
    "\n",
    "xTe = np.array([\n",
    "    [0.45864, 0.71552],\n",
    "    [2.44662, 1.68167],\n",
    "    [1.00345, 0.15182],\n",
    "    [-0.10560, -0.48155],\n",
    "    [3.07264, 3.81535],\n",
    "    [3.13035, 2.72151],\n",
    "    [2.25265, 3.78697]])\n",
    "yTe = np.array([1, 2, 1, 1, 2, 2, 2])\n",
    "\n",
    "ybar = computeybar(xTe, OFFSET)\n",
    "noise = np.mean(np.power(yTe - ybar, 2)) # calculate the noise\n",
    "\n",
    "assert noise < 0.0002\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Noise Continued [Graded]</h4>\n",
    "<p> Now, calculate the noise component $\\mathbb{E}[(\\bar{y}(\\mathbf{x})-y(\\mathbf{x}))^2]$ of the error using the results of <strong><code>computeybar</code></strong> that you implemented above. Remember that $$\\mathbb{E}[(\\bar{y}(\\mathbf{x})-y(\\mathbf{x}))^2] = \\frac{1}{n}\\sum_{i=1}^n(\\bar{y}({x_i})-y({x_i}))^2$$\n",
    "and that <strong><code>computeybar</code></strong> computes $\\bar{y}(\\mathbf{x}) = [\\bar{y}({x_1})\\dots \\bar{y}({x_n})]$. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computenoise(xTe, yTe, OFFSET):\n",
    "    \"\"\"\n",
    "    function noise=computenoise(xTe, OFFSET);\n",
    "\n",
    "    computes the noise, or square mean of ybar - y, for a set of inputs x\n",
    "    generated from two standard Normal distributions (one offset by OFFSET in\n",
    "    both dimensions.)\n",
    "\n",
    "    INPUT:\n",
    "    xTe       : nx2 array of n vectors with 2 dimensions\n",
    "    OFFSET    : The OFFSET passed into the toyData function. The difference in the\n",
    "                mu of labels class1 and class2 for toyData.\n",
    "\n",
    "    OUTPUT:\n",
    "    noise:    : a scalar representing the noise component of the error of xTe\n",
    "    \"\"\"\n",
    "    noise = 0\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    ybar = computeybar(xTe, OFFSET)\n",
    "    noise = np.mean(np.power(yTe - ybar, 2))\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    return noise\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_noise1():\n",
    "    OFFSET = 2\n",
    "    n = 1000\n",
    "    xTe, yTe = toydata(OFFSET, n) # Generate n datapoints\n",
    "    noise = computenoise(xTe, yTe, OFFSET)\n",
    "    \n",
    "    return np.isscalar(noise) \n",
    "\n",
    "def test_noise2():\n",
    "    OFFSET = 50\n",
    "    # Create an easy dataset\n",
    "    # We set sigma=1 and since the mean is far apart,\n",
    "    # the noise is negligible\n",
    "    xTe = np.array([\n",
    "        [49.308783, 49.620651], \n",
    "        [1.705462, 1.885418], \n",
    "        [ 51.192402, 50.256330],\n",
    "        [0.205998, -0.089885],\n",
    "        [50.853083, 51.833237]])  \n",
    "    yTe = np.array([2, 1, 2, 1, 2])\n",
    "    noise = computenoise(xTe, yTe, OFFSET)\n",
    "    return np.isclose(noise,0)\n",
    "\n",
    "def test_noise3():\n",
    "    OFFSET = 3;\n",
    "\n",
    "    xTe = np.array([\n",
    "        [0.45864, 0.71552],\n",
    "        [2.44662, 1.68167],\n",
    "        [1.00345, 0.15182],\n",
    "        [-0.10560, -0.48155],\n",
    "        [3.07264, 3.81535],\n",
    "        [3.13035, 2.72151],\n",
    "        [2.25265, 3.78697]])\n",
    "    yTe = np.array([1, 2, 1, 1, 2, 2, 2])\n",
    "    \n",
    "    ybar = computeybar(xTe, OFFSET)\n",
    "    noise = computenoise(xTe,yTe,OFFSET)\n",
    "    \n",
    "    return noise < 0.0002 # make sure the noise is small\n",
    "\n",
    "runtest(test_noise1, 'test_noise1')\n",
    "runtest(test_noise2, 'test_noise2')\n",
    "runtest(test_noise3, 'test_noise3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_noise1\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "OFFSET = 2\n",
    "n = 1000\n",
    "xTe, yTe = toydata(OFFSET, n) # Generate n datapoints\n",
    "noise = computenoise(xTe, yTe, OFFSET)\n",
    "\n",
    "assert np.isscalar(noise) \n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_noise2\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "OFFSET = 50\n",
    "# Create an easy dataset\n",
    "# We set sigma=1 and since the mean is far apart,\n",
    "# the noise is negligible\n",
    "xTe = np.array([\n",
    "    [49.308783, 49.620651], \n",
    "    [1.705462, 1.885418], \n",
    "    [ 51.192402, 50.256330],\n",
    "    [0.205998, -0.089885],\n",
    "    [50.853083, 51.833237]])  \n",
    "yTe = np.array([2, 1, 2, 1, 2])\n",
    "noise = computenoise(xTe, yTe, OFFSET)\n",
    "assert np.isclose(noise,0)\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_noise3\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "OFFSET = 3;\n",
    "\n",
    "xTe = np.array([\n",
    "    [0.45864, 0.71552],\n",
    "    [2.44662, 1.68167],\n",
    "    [1.00345, 0.15182],\n",
    "    [-0.10560, -0.48155],\n",
    "    [3.07264, 3.81535],\n",
    "    [3.13035, 2.72151],\n",
    "    [2.25265, 3.78697]])\n",
    "yTe = np.array([1, 2, 1, 1, 2, 2, 2])\n",
    "\n",
    "noise = computenoise(xTe,yTe,OFFSET)\n",
    "\n",
    "assert noise < 0.0002 # make sure the noise is small\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2a0090c31f38a547",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Visualizing the Data**:\n",
    "You can now see the error of the bayes classifier. Below is a plotting of the two classes of points and the misclassified points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ad17a864ee4ad0e9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "OFFSET = 1.75\n",
    "np.random.seed(1)\n",
    "xTe, yTe = toydata(OFFSET, 1000)\n",
    "\n",
    "# compute Bayes Error\n",
    "ybar = computeybar(xTe, OFFSET)\n",
    "predictions = np.round(ybar)\n",
    "errors = predictions != yTe\n",
    "err = errors.sum() / len(yTe) * 100\n",
    "print('Error of Bayes classifier: %.1f%%.' % err)\n",
    "\n",
    "# plot data\n",
    "ind1 = yTe == 1\n",
    "ind2 = yTe == 2\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(xTe[ind1, 0], xTe[ind1, 1], c='r', marker='o')\n",
    "plt.scatter(xTe[ind2, 0], xTe[ind2, 1], c='b', marker='o')\n",
    "plt.scatter(xTe[errors, 0], xTe[errors, 1], c='k', s=100, alpha=0.2)\n",
    "plt.title(\"Plot of data (misclassified points highlighted)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f05e0af9fb83da3b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Part Two: Bias [Graded]</h3>\n",
    "\n",
    "<p>For the bias, you will need the average classifier $\\bar{h}$. Although we cannot compute the expected value  $\\bar h\\!=\\!\\mathbb{E}[h]$, we can approximate it by sampling many training sets $D_1, \\dots, D_m$ and training a classifier on each, getting $h_{D_1},\\dots, h_{D_m}$. You can then average their predictions on each data point:</p> $$\\bar{h}(\\mathbf{x})\\approx \\frac{1}{m}\\sum_{i=1}^m h_{D_i}(\\mathbf{x})$$\n",
    "    \n",
    " <p>Edit the function <strong><code>computehbar</code></strong> to do this. Average over <code>NMODELS</code> ($m$) different $h_D$, each trained on a different data set of <code>Nsmall</code> inputs drawn from the same distribution. Feel free to call <strong><code>toydata(OFFSET, Nsmall)</code></strong> to obtain more data sets. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ff7ca5bceeb099d7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We are going to use the regression tree that we used in our previous project as our $h_D$ . To remind you, the following code shows you how to instantiate a decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-660c9a79c812ba96",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "xTr, yTr = toydata(OFFSET, 100)\n",
    "\n",
    "# Create a regression tree with no restriction on its depth\n",
    "# if you want to create a tree of depth k\n",
    "# then call RegressionTree(depth=k)\n",
    "tree = RegressionTree(depth=np.inf)\n",
    "\n",
    "# To fit/train the regression tree\n",
    "tree.fit(xTr, yTr)\n",
    "\n",
    "# To use the trained regression tree to make prediction\n",
    "pred = tree.predict(xTr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-computehbar",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def computehbar(xTe, depth, Nsmall, NMODELS, OFFSET):\n",
    "    \"\"\"\n",
    "    function [hbar]=computehbar(xTe, sigma, lmbda, NSmall, NMODELS, OFFSET);\n",
    "\n",
    "    computes the expected prediction of the average regression tree (hbar)\n",
    "    for data set xTe. \n",
    "\n",
    "    The regression tree should be trained using data of size Nsmall and is drawn from toydata with OFFSET \n",
    "    \n",
    "\n",
    "    The \"infinite\" number of models is estimated as an average over NMODELS. \n",
    "\n",
    "    INPUT:\n",
    "    xTe       | nx2 matrix, of n column-wise input vectors (each 2-dimensional)\n",
    "    depth     | Depth of the tree \n",
    "    NSmall    | Number of points to subsample\n",
    "    NMODELS   | Number of Models to average over\n",
    "    OFFSET    | The OFFSET passed into the toyData function. The difference in the\n",
    "                mu of labels class1 and class2 for toyData.\n",
    "    OUTPUT:\n",
    "    hbar | nx1 vector with the predictions of hbar for each test input\n",
    "    \"\"\"\n",
    "    n = xTe.shape[0]\n",
    "    hbar = np.zeros(n)\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    for _ in range(NMODELS):\n",
    "        ## fill in code here\n",
    "        xTr, yTr = toydata(OFFSET, Nsmall)\n",
    "        model = RegressionTree(depth=depth)\n",
    "        model.fit(xTr, yTr)\n",
    "        hbar += model.predict(xTe)\n",
    "        \n",
    "    hbar /= NMODELS\n",
    "    ### END SOLUTION\n",
    "    return hbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-hbar-selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def test_hbar1():\n",
    "    OFFSET = 2\n",
    "    depth = 2\n",
    "    Nsmall = 10\n",
    "    NMODELS = 10 \n",
    "    n = 1000\n",
    "    xTe, yTe = toydata(OFFSET, n)\n",
    "    hbar = computehbar(xTe, depth, Nsmall, NMODELS, OFFSET)\n",
    "    return hbar.shape == (n, ) # the dimension of hbar should be (n, )\n",
    "\n",
    "def test_hbar2():\n",
    "    OFFSET = 50\n",
    "    # Create an easy dataset\n",
    "    # We set sigma=1 and since the mean is far apart,\n",
    "    # the noise is negligible\n",
    "    xTe = np.array([\n",
    "        [49.308783, 49.620651], \n",
    "        [1.705462, 1.885418], \n",
    "        [51.192402, 50.256330],\n",
    "        [0.205998, -0.089885],\n",
    "        [50.853083, 51.833237]])  \n",
    "    yTe = np.array([2, 1, 2, 1, 2])\n",
    "    \n",
    "    depth = 2\n",
    "    Nsmall = 10\n",
    "    NMODELS = 1\n",
    "    \n",
    "    # since the mean is far apart, the tree should be able to learn perfectly\n",
    "    hbar = computehbar(xTe, depth, Nsmall, NMODELS, OFFSET) \n",
    "    ybar = computeybar_grader(xTe, OFFSET)\n",
    "    bias = np.mean(np.power(hbar-ybar,2))\n",
    "    return np.isclose(bias, 0) # the bias should be close to zero\n",
    "\n",
    "def test_hbar3():\n",
    "    OFFSET = 3;\n",
    "\n",
    "    xTe = np.array([\n",
    "        [0.45864, 0.71552],\n",
    "        [2.44662, 1.68167],\n",
    "        [1.00345, 0.15182],\n",
    "        [-0.10560, -0.48155],\n",
    "        [3.07264, 3.81535],\n",
    "        [3.13035, 2.72151],\n",
    "        [2.25265, 3.78697]])\n",
    "    yTe = np.array([1, 2, 1, 1, 2, 2, 2])\n",
    "    \n",
    "    depth = 3\n",
    "    Nsmall = 10\n",
    "    NMODELS = 100\n",
    "    \n",
    "    # set the random seed to ensure consistent behavior\n",
    "    np.random.seed(1)\n",
    "    hbar = computehbar(xTe, depth, Nsmall, NMODELS, OFFSET) \n",
    "    ybar = computeybar_grader(xTe, OFFSET)\n",
    "    bias = np.mean(np.power(hbar-ybar, 2))\n",
    "    return np.abs(bias - 0.0017) < 0.001 # the bias should be close to 0.007\n",
    "\n",
    "runtest(test_hbar1, 'test_hbar1')\n",
    "runtest(test_hbar2, 'test_hbar2')\n",
    "runtest(test_hbar3, 'test_hbar3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-hbar1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_hbar1\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "OFFSET = 2\n",
    "depth = 2\n",
    "Nsmall = 10\n",
    "NMODELS = 10 \n",
    "n = 1000\n",
    "xTe, yTe = toydata(OFFSET, n)\n",
    "hbar = computehbar(xTe, depth, Nsmall, NMODELS, OFFSET)\n",
    "assert hbar.shape == (n, )\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-hbar2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_hbar2\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "OFFSET = 50\n",
    "# Create an easy dataset\n",
    "# We set sigma=1 and since the mean is far apart,\n",
    "# the noise is negligible\n",
    "xTe = np.array([\n",
    "    [49.308783, 49.620651], \n",
    "    [1.705462, 1.885418], \n",
    "    [51.192402, 50.256330],\n",
    "    [0.205998, -0.089885],\n",
    "    [50.853083, 51.833237]])  \n",
    "yTe = np.array([2, 1, 2, 1, 2])\n",
    "\n",
    "depth = 2\n",
    "Nsmall = 10\n",
    "NMODELS = 1\n",
    "\n",
    "# since the mean is far apart, the tree should be able to learn perfectly\n",
    "hbar = computehbar(xTe, depth, Nsmall, NMODELS, OFFSET) \n",
    "ybar = computeybar_grader(xTe, OFFSET)\n",
    "bias = np.mean(np.power(hbar-ybar,2))\n",
    "assert np.isclose(bias, 0) # the bias should be close to zero\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-hbar3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_hbar3\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "OFFSET = 3;\n",
    "\n",
    "xTe = np.array([\n",
    "    [0.45864, 0.71552],\n",
    "    [2.44662, 1.68167],\n",
    "    [1.00345, 0.15182],\n",
    "    [-0.10560, -0.48155],\n",
    "    [3.07264, 3.81535],\n",
    "    [3.13035, 2.72151],\n",
    "    [2.25265, 3.78697]])\n",
    "yTe = np.array([1, 2, 1, 1, 2, 2, 2])\n",
    "\n",
    "depth = 3\n",
    "Nsmall = 10\n",
    "NMODELS = 100\n",
    "\n",
    "# set the random seed to ensure consistent behavior\n",
    "np.random.seed(1)\n",
    "hbar = computehbar(xTe, depth, Nsmall, NMODELS, OFFSET) \n",
    "ybar = computeybar_grader(xTe, OFFSET)\n",
    "bias = np.mean(np.power(hbar-ybar, 2))\n",
    "assert np.abs(bias - 0.0017) < 0.001\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Bias Continued [Graded]</h4>\n",
    "\n",
    "Now, we're equipped to calculate the bias $\\mathbb{E}[(\\bar{h}(\\mathbf{x})-\\bar{y}(\\mathbf{x}))^2]$. Like when calculating the noise, you need to take the mean of a square difference over your input points: \n",
    "$$\\mathbb{E}[(\\bar{h}(\\mathbf{x})-\\bar{y}(\\mathbf{x}))^2] = \\frac{1}{n}\\sum_{i=1}^n(\\bar{h}({x_i})-\\bar{y}({x_i}))^2$$\n",
    "\n",
    "You can call both <code><strong>computehbar</strong></code> and <code><strong>computeybar</strong></code> to calculate the bias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computebias(xTe, depth, Nsmall, NMODELS, OFFSET):\n",
    "    \"\"\"\n",
    "    function bias = computebias(xTe, sigma, lmbda, NSmall, NMODELS, OFFSET);\n",
    "\n",
    "    computes the bias for data set xTe. \n",
    "\n",
    "    The regression tree should be trained using data of size Nsmall and is drawn from toydata with OFFSET \n",
    "    \n",
    "\n",
    "    The \"infinite\" number of models is estimated as an average over NMODELS. \n",
    "\n",
    "    INPUT:\n",
    "    xTe       | nx2 matrix, of n column-wise input vectors (each 2-dimensional)\n",
    "    depth     | Depth of the tree \n",
    "    NSmall    | Number of points to subsample\n",
    "    NMODELS   | Number of Models to average over\n",
    "    OFFSET    | The OFFSET passed into the toyData function. The difference in the\n",
    "                mu of labels class1 and class2 for toyData.\n",
    "    OUTPUT:\n",
    "    bias | a scalar representing the bias of the input data\n",
    "    \"\"\"\n",
    "    noise = 0\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    hbar = computehbar(xTe, depth, Nsmall, NMODELS, OFFSET) \n",
    "    ybar = computeybar(xTe, OFFSET)\n",
    "    bias = np.mean(np.power(hbar-ybar,2))\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    return bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_bias1():\n",
    "    OFFSET = 2\n",
    "    depth = 2\n",
    "    Nsmall = 10\n",
    "    NMODELS = 10 \n",
    "    n = 1000\n",
    "    xTe, yTe = toydata(OFFSET, n)\n",
    "    bias = computebias(xTe, depth, Nsmall, NMODELS, OFFSET)\n",
    "    return np.isscalar(bias) # the dimension of hbar should be (n, )\n",
    "\n",
    "def test_bias2():\n",
    "    OFFSET = 50\n",
    "    # Create an easy dataset\n",
    "    # We set sigma=1 and since the mean is far apart,\n",
    "    # the noise is negligible\n",
    "    xTe = np.array([\n",
    "        [49.308783, 49.620651], \n",
    "        [1.705462, 1.885418], \n",
    "        [51.192402, 50.256330],\n",
    "        [0.205998, -0.089885],\n",
    "        [50.853083, 51.833237]])  \n",
    "    yTe = np.array([2, 1, 2, 1, 2])\n",
    "    \n",
    "    depth = 2\n",
    "    Nsmall = 10\n",
    "    NMODELS = 1\n",
    "    \n",
    "    # since the mean is far apart, the tree should be able to learn perfectly\n",
    "    bias = computebias(xTe, depth, Nsmall, NMODELS, OFFSET)\n",
    "    return np.isclose(bias, 0) # the bias should be close to zero\n",
    "\n",
    "def test_bias3():\n",
    "    OFFSET = 3;\n",
    "\n",
    "    xTe = np.array([\n",
    "        [0.45864, 0.71552],\n",
    "        [2.44662, 1.68167],\n",
    "        [1.00345, 0.15182],\n",
    "        [-0.10560, -0.48155],\n",
    "        [3.07264, 3.81535],\n",
    "        [3.13035, 2.72151],\n",
    "        [2.25265, 3.78697]])\n",
    "    yTe = np.array([1, 2, 1, 1, 2, 2, 2])\n",
    "    \n",
    "    depth = 3\n",
    "    Nsmall = 10\n",
    "    NMODELS = 100\n",
    "    \n",
    "    # set the random seed to ensure consistent behavior\n",
    "    bias = computebias(xTe, depth, Nsmall, NMODELS, OFFSET)\n",
    "    return np.abs(bias - 0.0017) < 0.001 # the bias should be close to 0.007\n",
    "\n",
    "runtest(test_bias1, 'test_bias1')\n",
    "runtest(test_bias2, 'test_bias2')\n",
    "runtest(test_bias3, 'test_bias3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_bias1\n",
    "### BEGIN HIDDEN TESTS\n",
    "OFFSET = 2\n",
    "depth = 2\n",
    "Nsmall = 10\n",
    "NMODELS = 10 \n",
    "n = 1000\n",
    "xTe, yTe = toydata(OFFSET, n)\n",
    "bias = computebias(xTe, depth, Nsmall, NMODELS, OFFSET)\n",
    "assert np.isscalar(bias) # variance should be a scalar\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_bias2\n",
    "### BEGIN HIDDEN TESTS\n",
    "OFFSET = 50\n",
    "# Create an easy dataset\n",
    "# We set sigma=1 and since the mean is far apart,\n",
    "# the noise is negligible\n",
    "xTe = np.array([\n",
    "    [49.308783, 49.620651], \n",
    "    [1.705462, 1.885418], \n",
    "    [51.192402, 50.256330],\n",
    "    [0.205998, -0.089885],\n",
    "    [50.853083, 51.833237]])  \n",
    "yTe = np.array([2, 1, 2, 1, 2])\n",
    "\n",
    "depth = 2\n",
    "Nsmall = 10\n",
    "NMODELS = 1\n",
    "\n",
    "# since the mean is far apart, the tree should be able to learn perfectly\n",
    "bias = computebias(xTe, depth, Nsmall, NMODELS, OFFSET)\n",
    "assert np.isclose(bias, 0) # the bias should be close to zero\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_bias3\n",
    "### BEGIN HIDDEN TESTS\n",
    "OFFSET = 3;\n",
    "\n",
    "xTe = np.array([\n",
    "    [0.45864, 0.71552],\n",
    "    [2.44662, 1.68167],\n",
    "    [1.00345, 0.15182],\n",
    "    [-0.10560, -0.48155],\n",
    "    [3.07264, 3.81535],\n",
    "    [3.13035, 2.72151],\n",
    "    [2.25265, 3.78697]])\n",
    "yTe = np.array([1, 2, 1, 1, 2, 2, 2])\n",
    "\n",
    "depth = 3\n",
    "Nsmall = 10\n",
    "NMODELS = 100\n",
    "\n",
    "# set the random seed to ensure consistent behavior\n",
    "bias = computebias(xTe, depth, Nsmall, NMODELS, OFFSET)\n",
    "assert np.abs(bias - 0.0017) < 0.001\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-484ccd205f7b43fc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Part Three: Variance [Graded]</h3>\n",
    "<p>Finally, to compute the variance, we need to compute the term $\\mathbb{E}[(h_D-\\bar{h})^2]$. We will calculate it directly by editing <strong><code>computevariance</code></strong>. \n",
    "    \n",
    "We can approach this calculation by first considering $\\bar{v}(\\mathbf{x})= (h_D(\\mathbf x)-\\bar{h} ( \\mathbf x))^2$. Once again, you cannot compute the expected value exactly, but you can  approximate this term by averaging over <code>NMODELS</code> models. You can use your previous function to compute $\\bar{h}$, and then estimate\n",
    "$$\\bar{v}(\\mathbf{x})=\\mathbb{E}_D[(h_D(\\mathbf{x})-\\bar{h}(\\mathbf{x}))^2]\\approx \\frac{1}{m}\\sum_{j=1}^m(h_{D_j}(\\mathbf{x_i})-\\bar{h}(\\mathbf{x_i}))^2$$\n",
    "where once again $D_1,\\dots,D_m$ are i.i.d. training data sets. Note that the output of this function is a vector; in other words, you should not be taking an average over the $n$ input points.\n",
    "</p>\n",
    "\n",
    "Finally, you will need to return the variance itself, which is $$\\mathbb{E}[(h_D(\\mathbf{x})-\\bar{h}(\\mathbf{x}))^2] = \\mathbb{E}[\\bar{v}(\\mathbf{x})] = \\frac{1}{n}\\sum_{i=1}^n \\bar{v}({x}_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-computevariance",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def computevariance(xTe, depth, hbar, Nsmall, NMODELS, OFFSET):\n",
    "    \"\"\"\n",
    "    function variance=computevbar(xTe,sigma,lmbda,hbar,Nsmall,NMODELS,OFFSET)\n",
    "\n",
    "    computes the variance of classifiers trained on data sets from\n",
    "    toydata.m with pre-specified \"OFFSET\" and \n",
    "    with kernel regression with sigma and lmbda\n",
    "    evaluated on xTe. \n",
    "    the prediction of the average classifier is assumed to be stored in \"hbar\".\n",
    "\n",
    "    The \"infinite\" number of models is estimated as an average over NMODELS. \n",
    "\n",
    "    INPUT:\n",
    "    xTe       : nx2 matrix, of n column-wise input vectors (each 2-dimensional)\n",
    "    depth     : Depth of the tree \n",
    "    hbar      : nx1 vector of the predictions of hbar on the inputs xTe\n",
    "    Nsmall    : Number of samples drawn from toyData for one model\n",
    "    NModel    : Number of Models to average over\n",
    "    OFFSET    : The OFFSET passed into the toyData function. The difference in the\n",
    "                mu of labels class1 and class2 for toyData.\n",
    "    \n",
    "    OUTPUT:\n",
    "    vbar      : nx1 vector of the difference between each model prediction and the\n",
    "                average model prediction for each input\n",
    "                \n",
    "    \"\"\"\n",
    "    n = xTe.shape[0]\n",
    "    vbar = np.zeros(n)\n",
    "    variance = 0\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    for j in range(NMODELS):\n",
    "        ## fill in code here\n",
    "        xTr, yTr = toydata(OFFSET, Nsmall)\n",
    "        model = RegressionTree(depth=depth)\n",
    "        model.fit(xTr, yTr)\n",
    "        vbar += np.square(hbar - model.predict(xTe))\n",
    "                 \n",
    "    vbar /= NMODELS\n",
    "    variance = np.mean(vbar)\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    return variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_variance1():\n",
    "    OFFSET = 2\n",
    "    depth = 2\n",
    "    Nsmall = 10\n",
    "    NMODELS = 10 \n",
    "    n = 1000\n",
    "    xTe, yTe = toydata(OFFSET, n)\n",
    "    hbar = computehbar_grader(xTe, depth, Nsmall, NMODELS, OFFSET)\n",
    "    var = computevariance(xTe, depth, hbar, Nsmall, NMODELS, OFFSET)\n",
    "    return np.isscalar(var) # variance should be a scalar\n",
    "\n",
    "def test_variance2():\n",
    "    OFFSET = 50\n",
    "    # Create an easy dataset\n",
    "    # We set sigma=1 and since the mean is far apart,\n",
    "    # the noise is negligible\n",
    "    xTe = np.array([\n",
    "        [49.308783, 49.620651], \n",
    "        [1.705462, 1.885418], \n",
    "        [51.192402, 50.256330],\n",
    "        [0.205998, -0.089885],\n",
    "        [50.853083, 51.833237]])  \n",
    "    yTe = np.array([2, 1, 2, 1, 2])\n",
    "    \n",
    "    depth = 2\n",
    "    Nsmall = 10\n",
    "    NMODELS = 10\n",
    "    \n",
    "    # since the noise is negligible, the tree should be able to learn perfectly\n",
    "    hbar = computehbar_grader(xTe, depth, Nsmall, NMODELS, OFFSET) \n",
    "    var = computevariance(xTe, depth, hbar, Nsmall, NMODELS, OFFSET)\n",
    "    return np.isclose(var, 0) # the bias should be close to zero\n",
    "\n",
    "def test_variance3():\n",
    "    OFFSET = 3;\n",
    "\n",
    "    xTe = np.array([\n",
    "        [0.45864, 0.71552],\n",
    "        [2.44662, 1.68167],\n",
    "        [1.00345, 0.15182],\n",
    "        [-0.10560, -0.48155],\n",
    "        [3.07264, 3.81535],\n",
    "        [3.13035, 2.72151],\n",
    "        [2.25265, 3.78697]])\n",
    "    yTe = np.array([1, 2, 1, 1, 2, 2, 2])\n",
    "    \n",
    "    depth = 3\n",
    "    Nsmall = 10\n",
    "    NMODELS = 100\n",
    "    \n",
    "    # set the random seed to ensure consistent behavior\n",
    "    np.random.seed(1)\n",
    "    # since the noise is negligible, the tree should be able to learn perfectly\n",
    "    hbar = computehbar_grader(xTe, depth, Nsmall, NMODELS, OFFSET) \n",
    "    var = computevariance(xTe, depth, hbar, Nsmall, NMODELS, OFFSET)\n",
    "    return np.abs(var - 0.0404) < 0.0015 # the variance should be close to 0.0404\n",
    "\n",
    "runtest(test_variance1, 'test_variance1')\n",
    "runtest(test_variance2, 'test_variance2')\n",
    "runtest(test_variance3, 'test_variance3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-var1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_variance1\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "OFFSET = 2\n",
    "depth = 2\n",
    "Nsmall = 10\n",
    "NMODELS = 10 \n",
    "n = 1000\n",
    "xTe, yTe = toydata(OFFSET, n)\n",
    "hbar = computehbar_grader(xTe, depth, Nsmall, NMODELS, OFFSET)\n",
    "var = computevariance(xTe, depth, hbar, Nsmall, NMODELS, OFFSET)\n",
    "assert np.isscalar(var) # variance should be a scalar\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-var2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_variance2\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "OFFSET = 50\n",
    "# Create an easy dataset\n",
    "# We set sigma=1 and since the mean is far apart,\n",
    "# the noise is negligible\n",
    "xTe = np.array([\n",
    "    [49.308783, 49.620651], \n",
    "    [1.705462, 1.885418], \n",
    "    [51.192402, 50.256330],\n",
    "    [0.205998, -0.089885],\n",
    "    [50.853083, 51.833237]])  \n",
    "yTe = np.array([2, 1, 2, 1, 2])\n",
    "\n",
    "depth = 2\n",
    "Nsmall = 10\n",
    "NMODELS = 10\n",
    "\n",
    "# since the noise is negligible, the tree should be able to learn perfectly\n",
    "hbar = computehbar_grader(xTe, depth, Nsmall, NMODELS, OFFSET) \n",
    "var = computevariance(xTe, depth, hbar, Nsmall, NMODELS, OFFSET)\n",
    "assert np.isclose(var, 0) # the bias should be close to zero\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-var3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_variance3\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "OFFSET = 3;\n",
    "\n",
    "xTe = np.array([\n",
    "    [0.45864, 0.71552],\n",
    "    [2.44662, 1.68167],\n",
    "    [1.00345, 0.15182],\n",
    "    [-0.10560, -0.48155],\n",
    "    [3.07264, 3.81535],\n",
    "    [3.13035, 2.72151],\n",
    "    [2.25265, 3.78697]])\n",
    "yTe = np.array([1, 2, 1, 1, 2, 2, 2])\n",
    "\n",
    "depth = 3\n",
    "Nsmall = 10\n",
    "NMODELS = 100\n",
    "\n",
    "# set the random seed to ensure consistent behavior\n",
    "np.random.seed(1)\n",
    "# since the noise is negligible, the tree should be able to learn perfectly\n",
    "hbar = computehbar_grader(xTe, depth, Nsmall, NMODELS, OFFSET) \n",
    "var = computevariance(xTe, depth, hbar, Nsmall, NMODELS, OFFSET)\n",
    "assert np.abs(var - 0.0404) < 0.0015 # the variance should be close to 0.0404\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3284bddfa0d87f94",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Review the Error for Various Depths</h3>\n",
    "\n",
    "<p>If you did everything in the three previous graded sections correctly and execute the following cell, you should see how the error decomposes (roughly) into bias, variance and noise for various depths.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# biasvariancedemo\n",
    "\n",
    "OFFSET = 1.75\n",
    "# how big is the training set size N\n",
    "Nsmall = 75\n",
    "# how big is a really big data set (approx. infinity)\n",
    "Nbig = 7500\n",
    "# how many models do you want to average over\n",
    "NMODELS = 100\n",
    "# What regularization constants to evaluate\n",
    "depths = [0, 1, 2, 3, 4, 5, 6, np.inf]\n",
    "\n",
    "# we store\n",
    "Ndepths = len(depths)\n",
    "lbias = np.zeros(Ndepths)\n",
    "lvariance = np.zeros(Ndepths)\n",
    "ltotal = np.zeros(Ndepths)\n",
    "lnoise = np.zeros(Ndepths)\n",
    "lsum = np.zeros(Ndepths)\n",
    "\n",
    "# Different regularization constant classifiers\n",
    "for i in range(Ndepths):\n",
    "    depth = depths[i]\n",
    "    # use this data set as an approximation of the true test set\n",
    "    xTe,yTe = toydata(OFFSET, Nbig)\n",
    "    \n",
    "    # Estimate AVERAGE ERROR (TOTAL)\n",
    "    total = 0\n",
    "    for j in range(NMODELS):\n",
    "        # Set the seed for consistent behavior\n",
    "        xTr2,yTr2 = toydata(OFFSET, Nsmall)\n",
    "        model = RegressionTree(depth=depth)\n",
    "        model.fit(xTr, yTr)\n",
    "        total += np.mean((model.predict(xTe) - yTe) ** 2)\n",
    "    total /= NMODELS\n",
    "    \n",
    "    # Estimate Noise\n",
    "    noise = computenoise(xTe, yTe, OFFSET)\n",
    "    \n",
    "    # Estimate Bias\n",
    "    bias = computebias(xTe,depth,Nsmall, NMODELS, OFFSET)\n",
    "    \n",
    "    # Estimating VARIANCE\n",
    "    hbar = computehbar(xTe, depth, Nsmall, NMODELS, OFFSET)\n",
    "    variance = computevariance(xTe, depth, hbar, Nsmall, NMODELS, OFFSET)\n",
    "    \n",
    "    # print and store results\n",
    "    lbias[i] = bias\n",
    "    lvariance[i] = variance\n",
    "    ltotal[i] = total\n",
    "    lnoise[i] = noise\n",
    "    lsum[i] = lbias[i]+lvariance[i]+lnoise[i]\n",
    "    \n",
    "    if np.isinf(depths[i]):\n",
    "        print('Depth infinite: Bias: %2.4f Variance: %2.4f Noise: %2.4f Bias+Variance+Noise: %2.4f Test error: %2.4f'\n",
    "          % (lbias[i],lvariance[i],lnoise[i],lsum[i],ltotal[i]))\n",
    "    else:\n",
    "        print('Depth: %d: Bias: %2.4f Variance: %2.4f Noise: %2.4f Bias+Variance+Noise: %2.4f Test error: %2.4f'\n",
    "          % (depths[i],lbias[i],lvariance[i],lnoise[i],lsum[i],ltotal[i]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot results\n",
    "%matplotlib notebook\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(lbias[:Ndepths], '*', c='r',linestyle='-',linewidth=2)\n",
    "plt.plot(lvariance[:Ndepths], '*', c='k', linestyle='-',linewidth=2)\n",
    "plt.plot(lnoise[:Ndepths], '*', c='g',linestyle='-',linewidth=2)\n",
    "plt.plot(ltotal[:Ndepths], '*', c='b', linestyle='-',linewidth=2)\n",
    "plt.plot(lsum[:Ndepths], '*', c='k', linestyle='--',linewidth=2)\n",
    "\n",
    "plt.legend([\"Bias\",\"Variance\",\"Noise\",\"Test error\",\"Bias+Var+Noise\"]);\n",
    "plt.xlabel(\"Depth\",fontsize=18);\n",
    "plt.ylabel(\"Squared Error\",fontsize=18);\n",
    "plt.xticks([i for i in range(Ndepths)], depths);"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
