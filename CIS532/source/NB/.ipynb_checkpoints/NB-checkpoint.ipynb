{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0e0849dc3c6dd888",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2>Project 3: Na&iuml;ve Bayes</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c5829ae6905c4931",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<blockquote>\n",
    "    <center>\n",
    "    <img src=\"nb.png\" width=\"200px\" />\n",
    "    </center>\n",
    "      <p><cite><center>\"All models are wrong, but some are useful.\"<br>\n",
    "       -- George E.P. Box\n",
    "      </center></cite></p>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3baa72d698526cb5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Introduction</h3>\n",
    "<!--AÃ°albrandr-->\n",
    "\n",
    "<p>A&eth;albrandr is visiting America from Norway and has been having the hardest time distinguishing boys and girls because of the weird American names like Jack and Jane.  This has been causing lots of problems for A&eth;albrandr when he goes on dates. When he heard that Cornell has a Machine Learning class, he asked that we help him identify the gender of a person based on their name to the best of our ability.  In this project, you will implement Na&iuml;ve Bayes to predict if a name is male or female. </p>\n",
    "\n",
    "<strong>How to submit:</strong> You can submit your code using the red <strong>Submit</strong> button above. This button will send any code below surrounded by <strong>#&lt;GRADED&gt;</strong><strong>#&lt;/GRADED&gt;</strong> tags below to the autograder, which will then run several tests over your code. By clicking on the <strong>Details</strong> dropdown next to the Submit button, you will be able to view your submission report once the autograder has completed running. This submission report contains a summary of the tests you have failed or passed, as well as a log of any errors generated by your code when we ran it.\n",
    "\n",
    "Note that this may take a while depending on how long your code takes to run! Once your code is submitted you may navigate away from the page as you desire -- the most recent submission report will always be available from the Details menu.\n",
    "\n",
    "<p><strong>Evaluation:</strong> Your code will be autograded for technical\n",
    "correctness and--on some assignments--speed. Please <em>do not</em> change the names of any provided functions or classes within the code, or you will wreak havoc on the autograder. Furthermore, <em>any code not surrounded by <strong>#&lt;GRADED&gt;</strong><strong>#&lt;/GRADED&gt;</strong> tags will not be run by the autograder</em>. However, the correctness of your implementation -- not the autograder's output -- will be the final judge of your score.  If necessary, we will review and grade assignments individually to ensure that you receive due credit for your work.\n",
    "\n",
    "<p><strong>Academic Integrity:</strong> We will be checking your code against other submissions in the class for logical redundancy. If you copy someone else's code and submit it with minor changes, we will know. These cheat detectors are quite hard to fool, so please don't try. We trust you all to submit your own work only; <em>please</em> don't let us down. If you do, we will pursue the strongest consequences available to us.\n",
    "\n",
    "<p><strong>Getting Help:</strong> You are not alone!  If you find yourself stuck  on something, contact the course staff for help.  Office hours, section, and the <a href=\"https://piazza.com/class/iyag4nk2rsxsv\">Piazza</a> are there for your support; please use them.  If you can't make our office hours, let us know and we will schedule more.  We want these projects to be rewarding and instructional, not frustrating and demoralizing.  But, we don't know when or how to help unless you ask.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-30dffab85bf9ce51",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3> Of boys and girls </h3>\n",
    "\n",
    "<p> Take a look at the files <code>girls.train</code> and <code>boys.train</code>. For example with the unix command <pre>cat girls.train</pre> \n",
    "<pre>\n",
    "...\n",
    "Addisyn\n",
    "Danika\n",
    "Emilee\n",
    "Aurora\n",
    "Julianna\n",
    "Sophia\n",
    "Kaylyn\n",
    "Litzy\n",
    "Hadassah\n",
    "</pre>\n",
    "Believe it or not, these are all more or less common girl names. The problem with the current file is that the names are in plain text, which makes it hard for a machine learning algorithm to do anything useful with them. We therefore need to transform them into some vector format, where each name becomes a vector that represents a point in some high dimensional input space. </p>\n",
    "\n",
    "<p>That is exactly what the following Python function <code>name2features</code> does: </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashfeatures(baby, B, FIX):\n",
    "    v = np.zeros(B)\n",
    "    for m in range(FIX):\n",
    "        featurestring = \"prefix\" + baby[:m]\n",
    "        v[hash(featurestring) % B] = 1\n",
    "        featurestring = \"suffix\" + baby[-1*m:]\n",
    "        v[hash(featurestring) % B] = 1\n",
    "    return v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name2features(filename, B=128, FIX=3, LoadFile=True):\n",
    "    \"\"\"\n",
    "    Output:\n",
    "        X : n feature vectors of dimension B, (nxB)\n",
    "    \"\"\"\n",
    "    # read in baby names\n",
    "    if LoadFile:\n",
    "        with open(filename, 'r') as f:\n",
    "            babynames = [x.rstrip() for x in f.readlines() if len(x) > 0]\n",
    "    else:\n",
    "        babynames = filename.split('\\n')\n",
    "    n = len(babynames)\n",
    "    X = np.zeros((n, B))\n",
    "    for i in range(n):\n",
    "        X[i,:] = hashfeatures(babynames[i], B, FIX)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-844dbe02c2f5bdc3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "It reads every name in the given file and converts it into a 128-dimensional feature vector. </p> \n",
    "\n",
    "<p>Can you figure out what the features are? (Understanding how these features are constructed will help you later on in the competition.)<br></p>\n",
    "\n",
    "<p>We have provided you with a python function <code>genTrainFeatures</code>, which transforms the names into features and loads them into memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genTrainFeatures(dimension=128):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        dimension: desired dimension of the features\n",
    "    Output: \n",
    "        X: n feature vectors of dimensionality d (nxd)\n",
    "        Y: n labels (-1 = girl, +1 = boy) (n)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load in the data\n",
    "    Xgirls = name2features(\"girls.train\", B=dimension)\n",
    "    Xboys = name2features(\"boys.train\", B=dimension)\n",
    "    X = np.concatenate([Xgirls, Xboys])\n",
    "    \n",
    "    # Generate Labels\n",
    "    Y = np.concatenate([-np.ones(len(Xgirls)), np.ones(len(Xboys))])\n",
    "    \n",
    "    # shuffle data into random order\n",
    "    ii = np.random.permutation([i for i in range(len(Y))])\n",
    "    \n",
    "    return X[ii, :], Y[ii]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f76be5bf3de52f69",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "You can call the following command to load in the features and the labels of all boys and girls names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = genTrainFeatures(128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e91e263ce575ecdf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3> The Na&iuml;ve Bayes Classifier </h3>\n",
    "\n",
    "<p> The Na&iuml;ve Bayes classifier is a linear classifier based on Bayes Rule. The following questions will ask you to finish these functions in a pre-defined order. <br>\n",
    "<strong>As a general rule, you should avoid tight loops at all cost.</strong></p>\n",
    "<p>(a) Estimate the class probability $P(y)$ in \n",
    "<b><code>naivebayesPY</code></b>\n",
    ". This should return the probability that a sample in the training set is positive or negative, independent of its features.\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4f7e522474be17dd",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def naivebayesPY(X, Y):\n",
    "    \"\"\"\n",
    "    naivebayesPY(Y) returns [pos,neg]\n",
    "\n",
    "    Computation of P(Y)\n",
    "    Input:\n",
    "        X : n input vectors of d dimensions (nxd)\n",
    "        Y : n labels (-1 or +1) (nx1)\n",
    "\n",
    "    Output:\n",
    "        pos: probability p(y=1)\n",
    "        neg: probability p(y=-1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # add one positive and negative example to avoid division by zero (\"plus-one smoothing\")\n",
    "    Y = np.concatenate([Y, [-1,1]])\n",
    "    n = len(Y)\n",
    "    ### BEGIN SOLUTION\n",
    "    pos = np.mean(Y == 1)\n",
    "    neg = np.mean(Y == -1)\n",
    "    return pos, neg\n",
    "    ### END SOLUTION\n",
    "\n",
    "\n",
    "pos,neg = naivebayesPY(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e3d763d037cc430a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<p>(b) Estimate the conditional probabilities $P([\\mathbf{x}]_{\\alpha}|y)$ in \n",
    "<b><code>naivebayesPXY</code></b> Notice that by construction, our features are binary categorical features. \n",
    ".  Use a <b>categorical</b> distribution as model and return the probability vectors for each features being 1 given a class label.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dfe700a89a699035",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def naivebayesPXY(X,Y):\n",
    "    \"\"\"\n",
    "    naivebayesPXY(X, Y) returns [posprob,negprob]\n",
    "    \n",
    "    Input:\n",
    "        X : n input vectors of d dimensions (nxd)\n",
    "        Y : n labels (-1 or +1) (n)\n",
    "    \n",
    "    Output:\n",
    "        posprob: probability vector of p(x_alpha = 1|y=1)  (d)\n",
    "        negprob: probability vector of p(x_alpha = 1|y=-1) (d)\n",
    "    \"\"\"\n",
    "    \n",
    "    # add one positive and negative example to avoid division by zero (\"plus-one smoothing\")\n",
    "    n, d = X.shape\n",
    "    X = np.concatenate([X, np.ones((2,d)), np.zeros((2,d))])\n",
    "    Y = np.concatenate([Y, [-1,1,-1,1]])\n",
    "    n, d = X.shape\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    posprob = np.mean(X[Y == 1], axis=0)\n",
    "    negprob = np.mean(X[Y == -1], axis=0)\n",
    "    return posprob, negprob\n",
    "    ### END SOLUTION\n",
    "    \n",
    "\n",
    "posprob,negprob = naivebayesPXY(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-718eb20a3bd07d46",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<p>(b) Assume you are using the natural log. Calculate the log likelihood $\\log P(\\mathbf{x}|y)$ for each point in X_test in \n",
    "<b><code>loglikelihood</code></b>. Think carefully how you would use the Na&iuml;ve Bayes assumption to calculate the likelihood and how you can use the fact $\\log(ab) = \\log a + \\log b$ to simplify your calculations.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-798aec7ac926bcbe",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def loglikelihood(posprob, negprob, X_test, Y_test):\n",
    "    \"\"\"\n",
    "    loglikelihood(posprob, negprob, X_test, Y_test) returns loglikelihood of each point in X_test\n",
    "    \n",
    "    Input:\n",
    "        posprob: conditional probabilities for the positive class (d)\n",
    "        negprob: conditional probabilities for the negative class (d)\n",
    "        X_test : features (nxd)\n",
    "        Y_test : labels (-1 or +1) (n)\n",
    "    \n",
    "    Output:\n",
    "        loglikelihood of each point in X_test (n)\n",
    "    \"\"\"\n",
    "    n, d = X_test.shape\n",
    "    loglikelihood = np.zeros(n)\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    pos_ind = (Y_test == 1)\n",
    "    loglikelihood[pos_ind] = X_test[pos_ind]@np.log(posprob) + (1 - X_test[pos_ind])@np.log(1 - posprob)\n",
    "    neg_ind = (Y_test == -1)\n",
    "    loglikelihood[neg_ind] = X_test[neg_ind]@np.log(negprob) + (1 - X_test[neg_ind])@np.log(1 - negprob)\n",
    "    ### END SOLUTION\n",
    "    return loglikelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-939ef69834417c3a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<p>(c) Observe that for a test point $\\mathbf{x}_{test}$, we should classify it as positive if the log ratio $\\log\\left(\\frac{P(y=1 | \\mathbf{x} = \\mathbf{x}_{test})}{P(y=-1|\\mathbf{x} = \\mathbf{x}_{test})}\\right) > 0$ and negative otherwise. Implement the <b><code>naivebayes_pred</code></b> by first calculating the log ratio $\\log\\left(\\frac{P(y=1 | \\mathbf{x} = \\mathbf{x}_{test})}{P(y=-1|\\mathbf{x} = \\mathbf{x}_{test})}\\right)$ for each test point in X_test using Bayes rule and predict the label of the test points by looking at the log ratio. When calculating the log likelihood, think carefully how you can use the fact $\\log \\left(\\frac{a}{b}\\right) = \\log{a} - \\log{b}$ to simplify your calculations.\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-15b1864517dfd35f",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "def naivebayes_pred(pos, neg, posprob, negprob, X_test):\n",
    "    \"\"\"\n",
    "    naivebayes_pred(pos, neg, posprob, negprob, X_test) returns the prediction of each point in X_test\n",
    "    \n",
    "    Input:\n",
    "        pos: class probability for the negative class\n",
    "        neg: class probability for the positive class\n",
    "        posprob: conditional probabilities for the positive class (d)\n",
    "        negprob: conditional probabilities for the negative class (d)\n",
    "        X_test : features (nxd)\n",
    "    \n",
    "    Output:\n",
    "        prediction of each point in X_test (n)\n",
    "    \"\"\"\n",
    "    n, d = X_test.shape\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    loglikelihood_ratio = loglikelihood(posprob, negprob, X_test, np.ones(n)) - \\\n",
    "        loglikelihood(posprob, negprob, X_test, -np.ones(n)) + np.log(pos) - np.log(neg)\n",
    "    preds = - np.ones(n)\n",
    "    preds[loglikelihood_ratio > 0] = 1\n",
    "    return preds\n",
    "    ### END SOLUTION\n",
    "    \n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d31d3867bef12ca7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "You can now test your code with the following interactive name classification script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a53cd34f0d01fec0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "Training classifier ...\n",
      "Training error: 22.83%\n",
      "Please enter your name>\n",
      "Paul\n",
      "Paul, I am sure you are a nice boy.\n",
      "\n",
      "Please enter your name>\n",
      "Emily\n",
      "Emily, I am sure you are a nice boy.\n",
      "\n",
      "Please enter your name>\n",
      "Lara\n",
      "Lara, I am sure you are a nice girl.\n",
      "\n",
      "Please enter your name>\n",
      "Emilee\n",
      "Emilee, I am sure you are a nice girl.\n",
      "\n",
      "Please enter your name>\n",
      "Tamy\n",
      "Tamy, I am sure you are a nice girl.\n",
      "\n",
      "Please enter your name>\n",
      "Kilian\n",
      "Kilian, I am sure you are a nice boy.\n",
      "\n",
      "Please enter your name>\n",
      "Cheng\n",
      "Cheng, I am sure you are a nice boy.\n",
      "\n",
      "Please enter your name>\n",
      "Carol\n",
      "Carol, I am sure you are a nice boy.\n",
      "\n",
      "Please enter your name>\n",
      "Tony\n",
      "Tony, I am sure you are a nice boy.\n",
      "\n",
      "Please enter your name>\n",
      "Pauline\n",
      "Pauline, I am sure you are a nice girl.\n",
      "\n",
      "Please enter your name>\n"
     ]
    }
   ],
   "source": [
    "DIMS = 128\n",
    "print('Loading data ...')\n",
    "X,Y = genTrainFeatures(DIMS)\n",
    "print('Training classifier ...')\n",
    "pos, neg = naivebayesPY(X, Y)\n",
    "posprob, negprob = naivebayesPXY(X, Y)\n",
    "error = np.mean(naivebayes_pred(pos, neg, posprob, negprob, X) != Y)\n",
    "print('Training error: %.2f%%' % (100 * error))\n",
    "\n",
    "while True:\n",
    "    print('Please enter your name>')\n",
    "    yourname = input()\n",
    "    if len(yourname) < 1:\n",
    "        break\n",
    "    xtest = name2features(yourname,B=DIMS,LoadFile=False)\n",
    "    pred = naivebayes_pred(pos, neg, posprob, negprob, xtest)\n",
    "    if pred > 0:\n",
    "        print(\"%s, I am sure you are a nice boy.\\n\" % yourname)\n",
    "    else:\n",
    "        print(\"%s, I am sure you are a nice girl.\\n\" % yourname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f42c92ce4c86ad4f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3> Feature Extraction (Competition)</h3>\n",
    "\n",
    "<p>(d) (<b>optional</b>) As always, this programming project also includes a competition.  We will rank all submissions by how well your Na&iuml;ve Bayes classifier performs on a secret test set. If you want to improve your classifier modify <code>name2features2</code> below.   The automatic reader will use your Python script to extract features and train your classifier on the same names training set by calling the function with only one argument--the name of a file containing a list of names.  The given implementation is the same as the given <code>name2features</code> above.\n",
    "</p>\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e410305232551246",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def hashfeatures(baby, B):\n",
    "    v = np.zeros(B)\n",
    "    for letter in baby:\n",
    "        v[hash(letter) % B] = 1\n",
    "    return v\n",
    "\n",
    "def name2features2(filename, B=128, LoadFile=True):\n",
    "    \"\"\"\n",
    "    Output:\n",
    "    X : n feature vectors of dimension B, (nxB)\n",
    "    \"\"\"\n",
    "    # read in baby names\n",
    "    if LoadFile:\n",
    "        with open(filename, 'r') as f:\n",
    "            babynames = [x.rstrip() for x in f.readlines() if len(x) > 0]\n",
    "    else:\n",
    "        babynames = filename.split('\\n')\n",
    "    n = len(babynames)\n",
    "    X = np.zeros((n, B))\n",
    "    for i in range(n):\n",
    "        X[i,:] = hashfeatures(babynames[i], B)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Credits</h4>\n",
    " The name classification idea originates from <a href=\"http://nickm.com\">Nick Montfort</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-2862737c2a8af7ca",
     "locked": true,
     "points": 0,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "# Instructor's internal Code\n",
    "boy_train_file =  \"boys.train\"\n",
    "girl_train_file = \"girls.train\"\n",
    "boy_test_file = \"boys.test\"\n",
    "girl_test_file = \"girls.test\"\n",
    "\n",
    "def hashfeatures_grader(baby, B, FIX):\n",
    "    v = np.zeros(B)\n",
    "    for m in range(FIX):\n",
    "        featurestring = \"prefix\" + baby[:m]\n",
    "        v[hash(featurestring) % B] = 1\n",
    "        featurestring = \"suffix\" + baby[-1*m:]\n",
    "        v[hash(featurestring) % B] = 1\n",
    "    return v\n",
    "\n",
    "def name2features_grader(filename, B=128, FIX=3, LoadFile=True):\n",
    "    \"\"\"\n",
    "    Output:\n",
    "    X : n feature vectors of dimension B, (nxB)\n",
    "    \"\"\"\n",
    "    # read in baby names\n",
    "    if LoadFile:\n",
    "        with open(filename, 'r') as f:\n",
    "            babynames = [x.rstrip() for x in f.readlines() if len(x) > 0]\n",
    "    else:\n",
    "        babynames = filename.split('\\n')\n",
    "    n = len(babynames)\n",
    "    X = np.zeros((n, B))\n",
    "    for i in range(n):\n",
    "        X[i,:] = hashfeatures_grader(babynames[i], B, FIX)\n",
    "    return X\n",
    "\n",
    "def genTrainFeatures_grader(dimension=128, fix=3, g=girl_train_file, b=boy_train_file ):\n",
    "    Xgirls = name2features_grader(g, B=dimension, FIX=fix)\n",
    "    Xboys = name2features_grader(b, B=dimension, FIX=fix)\n",
    "    X = np.concatenate([Xgirls, Xboys])\n",
    "\n",
    "    Y = np.concatenate([-np.ones(len(Xgirls)), np.ones(len(Xboys))])\n",
    "\n",
    "    ii = np.random.permutation([i for i in range(len(Y))])\n",
    "\n",
    "    return X[ii, :], Y[ii]\n",
    "\n",
    "def analyze_grader(kind,truth,preds):\n",
    "    truth = truth.flatten()\n",
    "    preds = preds.flatten()\n",
    "    if kind == 'abs':\n",
    "        # compute the absolute difference between truth and predictions\n",
    "        output = np.sum(np.abs(truth - preds)) / float(len(truth))\n",
    "    elif kind == 'acc':\n",
    "        if len(truth) == 0 and len(preds) == 0:\n",
    "            output = 0\n",
    "        else:\n",
    "            output = np.sum(truth == preds) / float(len(truth))\n",
    "    return output\n",
    "\n",
    "## instructor code for testing ##\n",
    "\n",
    "def naivebayesPY_grader(x,y):\n",
    "    y = np.concatenate([y, [-1,1]])\n",
    "    n = len(y)\n",
    "    pos = np.mean(y == 1)\n",
    "    neg = np.mean(y == -1)\n",
    "    return pos, neg\n",
    "\n",
    "def naivebayesPXY_grader(x,y):\n",
    "    n, d = x.shape\n",
    "    x = np.concatenate([x, np.ones((2,d)), np.zeros((2, d))])\n",
    "    y = np.concatenate([y, [-1,1,-1,1]])\n",
    "    n, d = x.shape\n",
    "    posprob = np.mean(x[y == 1], axis=0)\n",
    "    negprob = np.mean(x[y == -1], axis=0)\n",
    "    return posprob, negprob\n",
    "\n",
    "def loglikelihood_grader(posprob, negprob, x_test, y_test):\n",
    "    # calculate the likelihood of each of the point in x_test log P(x | y)\n",
    "    n, d = x_test.shape\n",
    "    loglikelihood = np.zeros(n)\n",
    "    pos_ind = (y_test == 1)\n",
    "    loglikelihood[pos_ind] = x_test[pos_ind]@np.log(posprob) + (1 - x_test[pos_ind])@np.log(1 - posprob)\n",
    "    neg_ind = (y_test == -1)\n",
    "    loglikelihood[neg_ind] = x_test[neg_ind]@np.log(negprob) + (1 - x_test[neg_ind])@np.log(1 - negprob)\n",
    "    \n",
    "    return loglikelihood\n",
    "\n",
    "def naivebayes_pred_grader(pos, neg, posprob, negprob, x_test):\n",
    "    n, d = x_test.shape \n",
    "    #     raise NotImplementedError\n",
    "    loglikelihood_ratio = loglikelihood_grader(posprob, negprob, x_test, np.ones(n)) - \\\n",
    "        loglikelihood_grader(posprob, negprob, x_test, -np.ones(n)) + np.log(pos) - np.log(neg)\n",
    "    preds = - np.ones(n)\n",
    "    preds[loglikelihood_ratio > 0] = 1\n",
    "    return preds\n",
    "\n",
    "X,Y = genTrainFeatures_grader(128)\n",
    "posY, negY = naivebayesPY_grader(X, Y)\n",
    "posprobXY, negprobXY = naivebayesPXY_grader(X, Y)\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d656e38f95af6b5f",
     "locked": true,
     "points": 4,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'naivebayesPY' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-09de5ba386a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mnaivebayesPY_test1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"naivebayesPY test failed - probabilities not adding to 1\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mnaivebayesPY_test2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"naivebayesPY test failed - calculation of P(Y) seems incorrect\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mnaivebayesPY_test3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"naivebayesPY test failed - calculation of P(Y) seems incorrect\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-09de5ba386a1>\u001b[0m in \u001b[0;36mnaivebayesPY_test1\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Check if probabilities sum to 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnaivebayesPY_test1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mneg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnaivebayesPY\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mneg\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'naivebayesPY' is not defined"
     ]
    }
   ],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "# Check if probabilities sum to 1\n",
    "def naivebayesPY_test1():\n",
    "    pos,neg = naivebayesPY(X,Y)\n",
    "    return np.linalg.norm(pos + neg - 1) < 1e-5\n",
    "\n",
    "# Test the Naive Bayes function on a simple matrix\n",
    "def naivebayesPY_test2():\n",
    "    x = np.array([[0,1],[1,0]])\n",
    "    y = np.array([-1,1])\n",
    "    pos, neg = naivebayesPY(x,y)\n",
    "    pos0, neg0 = .5, .5\n",
    "    test = np.linalg.norm(pos - pos0) + np.linalg.norm(neg - neg0)\n",
    "    return test < 1e-5\n",
    "\n",
    "# Test the Naive Bayes on a longer non-square matrix\n",
    "def naivebayesPY_test3():\n",
    "    x = np.array([[0,1,1,0,1],\n",
    "        [1,0,0,1,0],\n",
    "        [1,1,1,1,0],\n",
    "        [0,1,1,0,1],\n",
    "        [1,0,1,0,0],\n",
    "        [0,0,1,0,0],\n",
    "        [1,1,1,0,1]])    \n",
    "    y = np.array([1,-1, 1, 1,-1,-1, 1])\n",
    "    pos, neg = naivebayesPY(x,y)\n",
    "    pos0, neg0 = 5/9., 4/9.\n",
    "    test = np.linalg.norm(pos - pos0) + np.linalg.norm(neg - neg0)\n",
    "    return test < 1e-5\n",
    "\n",
    "# Tests that student didn't delete plus-one smoothing\n",
    "def naivebayesPY_test4():\n",
    "    x = np.array([[0,1,1,0,1],\n",
    "        [1,0,0,1,0]])    \n",
    "    y = np.array([1,1])\n",
    "    pos, neg = naivebayesPY(x,y)\n",
    "    pos0, neg0 = 3/4., 1/4.\n",
    "    test = np.linalg.norm(pos - pos0) + np.linalg.norm(neg - neg0)\n",
    "    return test < 1e-5\n",
    "\n",
    "assert naivebayesPY_test1(), \"naivebayesPY test failed - probabilities not adding to 1\\n\"\n",
    "assert naivebayesPY_test2(), \"naivebayesPY test failed - calculation of P(Y) seems incorrect\\n\" \n",
    "assert naivebayesPY_test3(), \"naivebayesPY test failed - calculation of P(Y) seems incorrect\\n\"\n",
    "assert naivebayesPY_test4(), \"naivebayesPY test failed - did you do plus one smoothing?\\n\"\n",
    "### END HIDDEN TESTS"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
