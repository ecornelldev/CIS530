{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3baa72d698526cb5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2>About this Project</h2>\n",
    "<p>In this project, you will use naive bayes to build a baby name classifier that is able to distinguish a boy name and a girl name, based on some features. You'll train your classifier to identify certain qualities of text and classify new text examples based on those qualities.</p>\n",
    "\n",
    "<h3>Evaluation</h3>\n",
    "\n",
    "<p>You are expected to write code where you see <em># YOUR CODE HERE</em> within the cells of this notebook. Upon submitting your work, the code you write at these designated positions will be assessed using an \"autograder\" that will run a series of tests on your code. You will receive instant feedback from the autograder that will identify issues with and errors in your code. Use this feedback to improve your code if you need to resubmit. Be sure not to change the names of any provided functions, classes, or variables within the existing code cells, as this will interfere with the autograder. Also, remember to execute all code cells, not just those you’ve edited, to ensure the code runs properly.</p>\n",
    "    \n",
    "<p>You can resubmit your work as many times as necessary before the submission deadline. If you experience difficulty or have questions about this exercise, use the Q&A discussion board to engage with your peers or seek assistance from the instructor.<p>\n",
    "\n",
    "<p><strong>This exercise must be successfully completed in order to receive credit for this course.</strong><p>\n",
    "\n",
    "<p>Before starting your work, please review <a href=\"https://s3.amazonaws.com/ecornell/global/eCornellPlagiarismPolicy.pdf\">eCornell's policy regarding plagiarism</a> (the presentation of someone else's work as your own without source credit).</p>\n",
    "\n",
    "<h3>Submit Code for Autograder/Instructor Feedback</h3>\n",
    "\n",
    "<p>Once you have completed your work on this notebook, you will submit your code for autograder/instructor review. Follow these steps:</p>\n",
    "\n",
    "<ol>\n",
    "<li>Save your notebook. Though the system should automatically save your progress, you should ensure the latest version of your work is saved before submitting. </li>\n",
    "  <li>In the blue menu bar along the top of the code exercise window, you’ll see a menu item called <strong>Education</strong>. In the <strong>Education</strong> menu, click <strong>Mark as Completed</strong> to submit your code for autograder/instructor review. This process will take a moment and a progress bar will show you the status of your submission.</li>\n",
    "\t<li>Once your work is marked as complete, the results of the autograder will automatically be presented in a new tab within the code exercise window. You can click on the assessment name in this feedback window to see more details regarding specific feedback/errors in your code submission.</li>\n",
    "  <li>The Jupyter notebook will always remain accessible in the first tabbed window of the exercise. To reattempt the work, you will first need to click <strong>Mark as Uncompleted</strong> in the <strong>Education</strong> menu and then proceed to make edits to the notebook. Once you are ready to resubmit, follow steps one through three. You can repeat this procedure as many times as necessary.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-30dffab85bf9ce51",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2>Getting Started</h2>\n",
    "<h3>Prepare Text for Machine Learning </h3>\n",
    "\n",
    "<p> If we are to create a classifier for text, we'll first need to think about the format of our data. Take a look at the files <code>girls.train</code> and <code>boys.train</code>. For example with the unix command: <pre>cat girls.train</pre> \n",
    "<pre>\n",
    "...\n",
    "Addisyn\n",
    "Danika\n",
    "Emilee\n",
    "Aurora\n",
    "Julianna\n",
    "Sophia\n",
    "Kaylyn\n",
    "Litzy\n",
    "Hadassah\n",
    "</pre>\n",
    "This file contains names that are more or less commonly used for girls. The problem with the current data in this file is that the names are in plain text, which is not a format our machine learning algorithm can work with effectively. You need to transform these plain text names into some vector format, where each name becomes a vector that represents a point in some high dimensional input space. </p>\n",
    "\n",
    "<p>That is exactly what the following Python function <code>name2features</code> does, by arbitrarily chunking and hashing different string extractions from each baby name inputted, thus transforming the string into a quantitative feature vector:</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Python Initialization</h3>\n",
    "<p>Please run the following code to initialize your Python kernel. You should be running a version of Python 3.x.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from helper import runtest, hashfeatures_grader, name2features_grader, \\\n",
    "genTrainFeatures_grader, analyze_grader, naivebayesPY_grader, naivebayesPXY_grader, \\\n",
    "loglikelihood_grader, naivebayes_pred_grader\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>The <code>hashfeatures</code> and <code>name2features</code> Functions</h3>\n",
    "<p>Below, the <code>hashfeatures</code> and <code>name2features</code> functions will take the plain text names and convert them to feature vectors so that you'll be able to work with the data effectively.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashfeatures(baby, B, FIX):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        baby : a string representing the baby's name to be hashed\n",
    "        B: the number of dimensions to be in the feature vector\n",
    "        FIX: the number of chunks to extract and hash from each string\n",
    "    \n",
    "    Output:\n",
    "        v: a feature vector representing the input string\n",
    "    \"\"\"\n",
    "    v = np.zeros(B)\n",
    "    for m in range(FIX):\n",
    "        featurestring = \"prefix\" + baby[:m]\n",
    "        v[hash(featurestring) % B] = 1\n",
    "        featurestring = \"suffix\" + baby[-1*m:]\n",
    "        v[hash(featurestring) % B] = 1\n",
    "    return v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name2features(filename, B=128, FIX=3, LoadFile=True):\n",
    "    \"\"\"\n",
    "    Output:\n",
    "        X : n feature vectors of dimension B, (nxB)\n",
    "    \"\"\"\n",
    "    # read in baby names\n",
    "    if LoadFile:\n",
    "        with open(filename, 'r') as f:\n",
    "            babynames = [x.rstrip() for x in f.readlines() if len(x) > 0]\n",
    "    else:\n",
    "        babynames = filename.split('\\n')\n",
    "    n = len(babynames)\n",
    "    X = np.zeros((n, B))\n",
    "    for i in range(n):\n",
    "        X[i,:] = hashfeatures(babynames[i], B, FIX)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-844dbe02c2f5bdc3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<p>In the code cell above, <code>name2features</code> reads every name in the given file and converts it into a 128-dimensional feature vector by first assembling substrings (based on the parameter 'FIX'), then hashing these assembled substrings and modifying the feature vector index (the modulo of the number of dimensions) that corresponds to this hash value. </p> \n",
    "\n",
    "<p>Can you see how the feature vector for each name changes with different parameters? (Understanding how these features are constructed will help you later on in the challenge.)<br></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>The <code>genTrainFeatures</code> Function</h3>\n",
    "<p>We have provided you with a python function <code>genTrainFeatures</code>, which transforms the names into features and loads them into memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genTrainFeatures(dimension=128):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        dimension: desired dimension of the features\n",
    "    Output: \n",
    "        X: n feature vectors of dimensionality d (nxd)\n",
    "        Y: n labels (-1 = girl, +1 = boy) (n)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load in the data\n",
    "    Xgirls = name2features(\"girls.train\", B=dimension)\n",
    "    Xboys = name2features(\"boys.train\", B=dimension)\n",
    "    X = np.concatenate([Xgirls, Xboys])\n",
    "    \n",
    "    # Generate Labels\n",
    "    Y = np.concatenate([-np.ones(len(Xgirls)), np.ones(len(Xboys))])\n",
    "    \n",
    "    # shuffle data into random order\n",
    "    ii = np.random.permutation([i for i in range(len(Y))])\n",
    "    \n",
    "    return X[ii, :], Y[ii]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f76be5bf3de52f69",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<p>You can call the following command to return two vectors, one holding all the concatenated feature vectors and one holding the labels of all boys and girls names.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = genTrainFeatures(128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> The Na&iuml;ve Bayes Classifier </h2>\n",
    "\n",
    "<p> The Na&iuml;ve Bayes classifier is a linear classifier based on Bayes Rule. The following cells will walk you through steps and ask you to finish the necessary functions in a pre-defined order. Code cells requiring your input will display # YOUR CODE HERE and graded portions will be adequately labeled. <br>\n",
    "<strong>As a general rule, you should avoid tight loops at all cost.</strong></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e91e263ce575ecdf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Part One [Graded]</h3>\n",
    "<i>\n",
    "<p>Estimate the class probability $P(y)$ in \n",
    "<b><code>naivebayesPY</code></b>. This should return the probability that a sample in the training set is positive or negative, independent of its features.\n",
    "</p>\n",
    "</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4f7e522474be17dd",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def naivebayesPY(X, Y):\n",
    "    \"\"\"\n",
    "    naivebayesPY(Y) returns [pos,neg]\n",
    "\n",
    "    Computation of P(Y)\n",
    "    Input:\n",
    "        X : n input vectors of d dimensions (nxd)\n",
    "        Y : n labels (-1 or +1) (nx1)\n",
    "\n",
    "    Output:\n",
    "        pos: probability p(y=1)\n",
    "        neg: probability p(y=-1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # add one positive and negative example to avoid division by zero (\"plus-one smoothing\")\n",
    "    Y = np.concatenate([Y, [-1,1]])\n",
    "    n = len(Y)\n",
    "    ### BEGIN SOLUTION\n",
    "    pos = np.mean(Y == 1)\n",
    "    neg = np.mean(Y == -1)\n",
    "    #return ('bobo')\n",
    "    return pos, neg\n",
    "    ### END SOLUTION\n",
    "\n",
    "\n",
    "pos, neg = naivebayesPY(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c354b70b07815ba7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h4>Code Tests</h4>\n",
    "<p>The four tests below will check your code in the previous cell. You must pass all four tests in order to receive full credit on part 1.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d656e38f95af6b5f",
     "locked": true,
     "points": 4,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Test: naivebayesPY_test1 ... ✔ Passed!\n",
      "Running Test: naivebayesPY_test2 ... ✔ Passed!\n",
      "Running Test: naivebayesPY_test3 ... ✔ Passed!\n",
      "Running Test: naivebayesPY_test4 ... ✔ Passed!\n"
     ]
    }
   ],
   "source": [
    "# Check if probabilities sum to 1\n",
    "def naivebayesPY_test1():\n",
    "    pos, neg = naivebayesPY(X,Y)\n",
    "    return np.linalg.norm(pos + neg - 1) < 1e-5\n",
    "\n",
    "# Test the Naive Bayes PY function on a simple example\n",
    "def naivebayesPY_test2():\n",
    "    x = np.array([[0,1],[1,0]])\n",
    "    y = np.array([-1,1])\n",
    "    pos, neg = naivebayesPY(x,y)\n",
    "    pos0, neg0 = .5, .5\n",
    "    test = np.linalg.norm(pos - pos0) + np.linalg.norm(neg - neg0)\n",
    "    return test < 1e-5\n",
    "\n",
    "# Test the Naive Bayes PY function on another example\n",
    "def naivebayesPY_test3():\n",
    "        x = np.array([[0,1,1,0,1],\n",
    "            [1,0,0,1,0],\n",
    "            [1,1,1,1,0],\n",
    "            [0,1,1,0,1],\n",
    "            [1,0,1,0,0],\n",
    "            [0,0,1,0,0],\n",
    "            [1,1,1,0,1]])    \n",
    "        y = np.array([1,-1, 1, 1,-1,-1, 1])\n",
    "        pos, neg = naivebayesPY(x,y)\n",
    "        pos0, neg0 = 5/9., 4/9.\n",
    "        test = np.linalg.norm(pos - pos0) + np.linalg.norm(neg - neg0)\n",
    "        return test < 1e-5\n",
    "\n",
    "# Tests that student didn't accidentally delete plus-one smoothing\n",
    "def naivebayesPY_test4():\n",
    "    x = np.array([[0,1,1,0,1],[1,0,0,1,0]])    \n",
    "    y = np.array([1,1])\n",
    "    pos, neg = naivebayesPY(x,y)\n",
    "    pos0, neg0 = 3/4., 1/4.\n",
    "    test = np.linalg.norm(pos - pos0) + np.linalg.norm(neg - neg0)\n",
    "    return test < 1e-5    \n",
    "        \n",
    "    \n",
    "runtest(naivebayesPY_test1, 'naivebayesPY_test1')\n",
    "runtest(naivebayesPY_test2,'naivebayesPY_test2')\n",
    "runtest(naivebayesPY_test3,'naivebayesPY_test3')\n",
    "runtest(naivebayesPY_test4,'naivebayesPY_test4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e3d763d037cc430a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Part Two [Graded]</h3>\n",
    "<i>\n",
    "<p>Estimate the conditional probabilities $P([\\mathbf{x}]_{\\alpha}|y)$ in \n",
    "<b><code>naivebayesPXY</code></b>. Notice that by construction, our features are binary categorical features. Use a <b>categorical</b> distribution as model and return the probability vectors for each features being 1 given a class label.\n",
    "</p> \n",
    "</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dfe700a89a699035",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def naivebayesPXY(X,Y):\n",
    "    \"\"\"\n",
    "    naivebayesPXY(X, Y) returns [posprob,negprob]\n",
    "    \n",
    "    Input:\n",
    "        X : n input vectors of d dimensions (nxd)\n",
    "        Y : n labels (-1 or +1) (n)\n",
    "    \n",
    "    Output:\n",
    "        posprob: probability vector of p(x_alpha = 1|y=1)  (d)\n",
    "        negprob: probability vector of p(x_alpha = 1|y=-1) (d)\n",
    "    \"\"\"\n",
    "    \n",
    "    # add one positive and negative example to avoid division by zero (\"plus-one smoothing\")\n",
    "    n, d = X.shape\n",
    "    X = np.concatenate([X, np.ones((2,d)), np.zeros((2,d))])\n",
    "    Y = np.concatenate([Y, [-1,1,-1,1]])\n",
    "    n, d = X.shape\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    posprob = np.mean(X[Y == 1], axis=0)\n",
    "    negprob = np.mean(X[Y == -1], axis=0)\n",
    "    return posprob, negprob\n",
    "    ### END SOLUTION\n",
    "    \n",
    "\n",
    "posprob, negprob = naivebayesPXY(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Test: naivebayesPXY_test1 ... ✔ Passed!\n",
      "Running Test: naivebayesPXY_test2 ... ✔ Passed!\n",
      "Running Test: naivebayesPXY_test3 ... ✔ Passed!\n"
     ]
    }
   ],
   "source": [
    "# Run this code to test your Naive Bayes function\n",
    "\n",
    "posprobXY, negprobXY = naivebayesPXY_grader(X, Y)\n",
    "\n",
    "\n",
    "def naivebayesPXY_test1():\n",
    "    x = np.array([[0,1],[1,0]])\n",
    "    y = np.array([-1,1])\n",
    "    pos, neg = naivebayesPXY(x,y)\n",
    "    pos0, neg0 = naivebayesPXY_grader(x,y)\n",
    "    test = np.linalg.norm(pos - pos0) + np.linalg.norm(neg - neg0)\n",
    "    return test < 1e-5\n",
    "\n",
    "def naivebayesPXY_test2():\n",
    "    pos, neg = naivebayesPXY(X,Y)\n",
    "    test = np.linalg.norm(pos - posprobXY) + np.linalg.norm(neg - negprobXY)\n",
    "    return test < 1e-5\n",
    "\n",
    "def naivebayesPXY_test3():\n",
    "    pos, neg = naivebayesPXY(X,Y)\n",
    "    return pos.shape == posprobXY.shape and neg.shape == negprobXY.shape\n",
    "\n",
    "runtest(naivebayesPXY_test1,'naivebayesPXY_test1')\n",
    "runtest(naivebayesPXY_test2,'naivebayesPXY_test2')\n",
    "runtest(naivebayesPXY_test3,'naivebayesPXY_test3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-718eb20a3bd07d46",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Part Three [Graded]</h3>\n",
    "\n",
    "<i>\n",
    "<p>Assume you are using the natural log. Calculate the log likelihood $\\log P(\\mathbf{x}|y)$ for each point in X_test in \n",
    "<b><code>loglikelihood</code></b>. Think carefully how you would use the Na&iuml;ve Bayes assumption to calculate the likelihood and how you can use the fact $\\log(ab) = \\log a + \\log b$ to simplify your calculations.\n",
    "</p> \n",
    "<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-798aec7ac926bcbe",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def loglikelihood(posprob, negprob, X_test, Y_test):\n",
    "    \"\"\"\n",
    "    loglikelihood(posprob, negprob, X_test, Y_test) returns loglikelihood of each point in X_test\n",
    "    \n",
    "    Input:\n",
    "        posprob: conditional probabilities for the positive class (d)\n",
    "        negprob: conditional probabilities for the negative class (d)\n",
    "        X_test : features (nxd)\n",
    "        Y_test : labels (-1 or +1) (n)\n",
    "    \n",
    "    Output:\n",
    "        loglikelihood of each point in X_test (n)\n",
    "    \"\"\"\n",
    "    n, d = X_test.shape\n",
    "    loglikelihood = np.zeros(n)\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    pos_ind = (Y_test == 1)\n",
    "    loglikelihood[pos_ind] = X_test[pos_ind]@np.log(posprob) + (1 - X_test[pos_ind])@np.log(1 - posprob)\n",
    "    neg_ind = (Y_test == -1)\n",
    "    loglikelihood[neg_ind] = X_test[neg_ind]@np.log(negprob) + (1 - X_test[neg_ind])@np.log(1 - negprob)\n",
    "    ### END SOLUTION\n",
    "    return loglikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Test: loglikelihood_test1 ... ✔ Passed!\n",
      "Running Test: loglikelihood_test2 ... ✔ Passed!\n",
      "Running Test: loglikelihood_test3 ... ✔ Passed!\n"
     ]
    }
   ],
   "source": [
    "# Test your log likelihood function by running the tests in this cell\n",
    "\n",
    "def loglikelihood_test1():\n",
    "    x = np.array([[0,1],[1,0]])\n",
    "    y = np.array([-1,1])\n",
    "    posprobXY, negprobXY = naivebayesPXY_grader(X, Y)\n",
    "    loglike = loglikelihood(posprobXY[:2], negprobXY[:2], x, y)\n",
    "    loglike0 = loglikelihood_grader(posprobXY[:2], negprobXY[:2], x, y)\n",
    "    test = np.linalg.norm(loglike - loglike0)\n",
    "    return test < 1e-5\n",
    "\n",
    "def loglikelihood_test2():\n",
    "    x = np.array([[1,0,1,0,1,1], \n",
    "        [0,0,1,0,1,1], \n",
    "        [1,0,0,1,1,1], \n",
    "        [1,1,0,0,1,1]])\n",
    "    y = np.array([-1,1,1,-1])\n",
    "    posprobXY, negprobXY = naivebayesPXY_grader(X, Y)\n",
    "    loglike = loglikelihood(posprobXY[:6], negprobXY[:6], x, y)\n",
    "    loglike0 = loglikelihood_grader(posprobXY[:6], negprobXY[:6], x, y)\n",
    "    test = np.linalg.norm(loglike - loglike0)\n",
    "    return test < 1e-5\n",
    "\n",
    "def loglikelihood_test3():\n",
    "    x = np.array([[1,1,1,1,1,1], \n",
    "        [0,0,1,0,0,0], \n",
    "        [1,1,0,1,1,1], \n",
    "        [0,1,0,0,0,1], \n",
    "        [0,1,1,0,1,1], \n",
    "        [1,0,0,0,0,1], \n",
    "        [0,1,1,0,1,1]])\n",
    "    y = np.array([1, 1, 1 ,1,-1,-1, 1])\n",
    "    posprobXY, negprobXY = naivebayesPXY_grader(X, Y)\n",
    "    loglike = loglikelihood(posprobXY[:6], negprobXY[:6], x, y)\n",
    "    loglike0 = loglikelihood_grader(posprobXY[:6], negprobXY[:6], x, y)\n",
    "    test = np.linalg.norm(loglike - loglike0)\n",
    "    return test < 1e-5\n",
    "\n",
    "runtest(loglikelihood_test1, 'loglikelihood_test1')\n",
    "runtest(loglikelihood_test2, 'loglikelihood_test2')\n",
    "runtest(loglikelihood_test3, 'loglikelihood_test3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-939ef69834417c3a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Part Four [Graded]</h3>\n",
    "\n",
    "<i>\n",
    "<p>Observe that for a test point $\\mathbf{x}_{test}$, we should classify it as positive if the log ratio $\\log\\left(\\frac{P(y=1 | \\mathbf{x} = \\mathbf{x}_{test})}{P(y=-1|\\mathbf{x} = \\mathbf{x}_{test})}\\right) > 0$ and negative otherwise. Implement the <b><code>naivebayes_pred</code></b> by first calculating the log ratio $\\log\\left(\\frac{P(y=1 | \\mathbf{x} = \\mathbf{x}_{test})}{P(y=-1|\\mathbf{x} = \\mathbf{x}_{test})}\\right)$ for each test point in X_test using Bayes rule and predict the label of the test points by looking at the log ratio. When calculating the log likelihood, think carefully how you can use the fact $\\log \\left(\\frac{a}{b}\\right) = \\log{a} - \\log{b}$ to simplify your calculations.\n",
    "</p>\n",
    "</i>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-15b1864517dfd35f",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def naivebayes_pred(pos, neg, posprob, negprob, X_test):\n",
    "    \"\"\"\n",
    "    naivebayes_pred(pos, neg, posprob, negprob, X_test) returns the prediction of each point in X_test\n",
    "    \n",
    "    Input:\n",
    "        pos: class probability for the negative class\n",
    "        neg: class probability for the positive class\n",
    "        posprob: conditional probabilities for the positive class (d)\n",
    "        negprob: conditional probabilities for the negative class (d)\n",
    "        X_test : features (nxd)\n",
    "    \n",
    "    Output:\n",
    "        prediction of each point in X_test (n)\n",
    "    \"\"\"\n",
    "    n, d = X_test.shape\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    loglikelihood_ratio = loglikelihood(posprob, negprob, X_test, np.ones(n)) - \\\n",
    "        loglikelihood(posprob, negprob, X_test, -np.ones(n)) + np.log(pos) - np.log(neg)\n",
    "    preds = - np.ones(n)\n",
    "    preds[loglikelihood_ratio > 0] = 1\n",
    "    return preds\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Test: naivebayes_pred_test1 ... ✔ Passed!\n",
      "Running Test: naivebayes_pred_test2 ... ✔ Passed!\n",
      "Running Test: naivebayes_pred_test3 ... ✔ Passed!\n",
      "Running Test: naivebayes_pred_test4 ... ✔ Passed!\n"
     ]
    }
   ],
   "source": [
    "X,Y = genTrainFeatures_grader(128)\n",
    "posY, negY = naivebayesPY_grader(X, Y)\n",
    "\n",
    "def naivebayes_pred_test1():\n",
    "    preds = naivebayes_pred(posY, negY, posprobXY, negprobXY, X)\n",
    "    return np.all(np.logical_or(preds == -1 , preds == 1))\n",
    "\n",
    "def naivebayes_pred_test2():\n",
    "    naivebayesPXY_grader(X, Y)\n",
    "    x_test = np.array([[0,1],[1,0]])\n",
    "    preds = naivebayes_pred_grader(posY, negY, posprobXY[:2], negprobXY[:2], x_test)\n",
    "    student_preds = naivebayes_pred(posY, negY, posprobXY[:2], negprobXY[:2], x_test)\n",
    "    acc = analyze_grader(\"acc\", preds, student_preds)\n",
    "    return np.abs(acc - 1) < 1e-5\n",
    "\n",
    "def naivebayes_pred_test3():\n",
    "    x_test = np.array([[1,0,1,0,1,1], \n",
    "        [0,0,1,0,1,1], \n",
    "        [1,0,0,1,1,1], \n",
    "        [1,1,0,0,1,1]])\n",
    "    naivebayesPXY_grader(X, Y)\n",
    "    preds = naivebayes_pred_grader(posY, negY, posprobXY[:6], negprobXY[:6], x_test)\n",
    "    student_preds = naivebayes_pred(posY, negY, posprobXY[:6], negprobXY[:6], x_test)\n",
    "    acc = analyze_grader(\"acc\", preds, student_preds)\n",
    "    return np.abs(acc - 1) < 1e-5\n",
    "\n",
    "def naivebayes_pred_test4():\n",
    "    x_test = np.array([[1,1,1,1,1,1], \n",
    "        [0,0,1,0,0,0], \n",
    "        [1,1,0,1,1,1], \n",
    "        [0,1,0,0,0,1], \n",
    "        [0,1,1,0,1,1], \n",
    "        [1,0,0,0,0,1], \n",
    "        [0,1,1,0,1,1]])\n",
    "    naivebayesPXY_grader(X, Y)\n",
    "    preds = naivebayes_pred_grader(posY, negY, posprobXY[:6], negprobXY[:6], x_test)\n",
    "    student_preds = naivebayes_pred(posY, negY, posprobXY[:6], negprobXY[:6], x_test)\n",
    "    acc = analyze_grader(\"acc\", preds, student_preds)\n",
    "    return np.abs(acc - 1) < 1e-5\n",
    "\n",
    "runtest(naivebayes_pred_test1, 'naivebayes_pred_test1')\n",
    "runtest(naivebayes_pred_test2, 'naivebayes_pred_test2')\n",
    "runtest(naivebayes_pred_test3, 'naivebayes_pred_test3')\n",
    "runtest(naivebayes_pred_test4, 'naivebayes_pred_test4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d31d3867bef12ca7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "You can now test your code with the following interactive name classification script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a53cd34f0d01fec0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DIMS = 128\n",
    "print('Loading data ...')\n",
    "X,Y = genTrainFeatures(DIMS)\n",
    "print('Training classifier ...')\n",
    "pos, neg = naivebayesPY(X, Y)\n",
    "posprob, negprob = naivebayesPXY(X, Y)\n",
    "error = np.mean(naivebayes_pred(pos, neg, posprob, negprob, X) != Y)\n",
    "print('Training error: %.2f%%' % (100 * error))\n",
    "\n",
    "while True:\n",
    "    print('Please enter your name>')\n",
    "    yourname = input()\n",
    "    if len(yourname) < 1:\n",
    "        break\n",
    "    xtest = name2features(yourname,B=DIMS,LoadFile=False)\n",
    "    pred = naivebayes_pred(pos, neg, posprob, negprob, xtest)\n",
    "    if pred > 0:\n",
    "        print(\"%s, I am sure you are a nice boy.\\n\" % yourname)\n",
    "    else:\n",
    "        print(\"%s, I am sure you are a nice girl.\\n\" % yourname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f42c92ce4c86ad4f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2> Challenge: Feature Extraction Challenge</h2>\n",
    "\n",
    "<p>Let's test how well your Na&iuml;ve Bayes classifier performs on a secret test set. If you want to improve your classifier modify <code>name2features2</code> below.   The automatic reader will use your Python script to extract features and train your classifier on the same names training set by calling the function with only one argument--the name of a file containing a list of names.  The given implementation is the same as the given <code>name2features</code> above.\n",
    "</p>\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e410305232551246",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def hashfeatures(baby, B, FIX):\n",
    "    v = np.zeros(B)\n",
    "    for m in range(FIX):\n",
    "        featurestring = \"prefix\" + baby[:m]\n",
    "        v[hash(featurestring) % B] = 1\n",
    "        featurestring = \"suffix\" + baby[-1*m:]\n",
    "        v[hash(featurestring) % B] = 1\n",
    "    return v\n",
    "\n",
    "def name2features2(filename, B=128, FIX=3, LoadFile=True):\n",
    "    \"\"\"\n",
    "    Output:\n",
    "        X : n feature vectors of dimension B, (nxB)\n",
    "    \"\"\"\n",
    "    # read in baby names\n",
    "    if LoadFile:\n",
    "        with open(filename, 'r') as f:\n",
    "            babynames = [x.rstrip() for x in f.readlines() if len(x) > 0]\n",
    "    else:\n",
    "        babynames = filename.split('\\n')\n",
    "    n = len(babynames)\n",
    "    X = np.zeros((n, B))\n",
    "    for i in range(n):\n",
    "        X[i,:] = hashfeatures(babynames[i], B, FIX)\n",
    "        \n",
    "    ### BEGIN SOLUTION\n",
    "    ### END SOLUTION\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Credits</h4>\n",
    " The name classification idea originates from <a href=\"http://nickm.com\">Nick Montfort</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-2862737c2a8af7ca",
     "locked": true,
     "points": 0,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "X,Y = genTrainFeatures_grader(128)\n",
    "\n",
    "def naivebayesPY_test1():\n",
    "    pos, neg = naivebayesPY(X,Y)\n",
    "    return np.linalg.norm(pos + neg - 1) < 1e-5\n",
    "\n",
    "# Test the Naive Bayes PY function on a simple example\n",
    "def naivebayesPY_test2():\n",
    "    x = np.array([[0,1],[1,0]])\n",
    "    y = np.array([-1,1])\n",
    "    pos, neg = naivebayesPY(x,y)\n",
    "    pos0, neg0 = .5, .5\n",
    "    test = np.linalg.norm(pos - pos0) + np.linalg.norm(neg - neg0)\n",
    "    return test < 1e-5\n",
    "\n",
    "# Test the Naive Bayes PY function on another example\n",
    "def naivebayesPY_test3():\n",
    "        x = np.array([[0,1,1,0,1],\n",
    "            [1,0,0,1,0],\n",
    "            [1,1,1,1,0],\n",
    "            [0,1,1,0,1],\n",
    "            [1,0,1,0,0],\n",
    "            [0,0,1,0,0],\n",
    "            [1,1,1,0,1]])    \n",
    "        y = np.array([1,-1, 1, 1,-1,-1, 1])\n",
    "        pos, neg = naivebayesPY(x,y)\n",
    "        pos0, neg0 = 5/9., 4/9.\n",
    "        test = np.linalg.norm(pos - pos0) + np.linalg.norm(neg - neg0)\n",
    "        return test < 1e-5\n",
    "\n",
    "# Tests that student didn't accidentally delete plus-one smoothing\n",
    "def naivebayesPY_test4():\n",
    "    x = np.array([[0,1,1,0,1],[1,0,0,1,0]])    \n",
    "    y = np.array([1,1])\n",
    "    pos, neg = naivebayesPY(x,y)\n",
    "    pos0, neg0 = 3/4., 1/4.\n",
    "    test = np.linalg.norm(pos - pos0) + np.linalg.norm(neg - neg0)\n",
    "    return test < 1e-5    \n",
    "### END HIDDEN TESTS\n",
    "#Autograder cell- do not edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-231b49a507601eff",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "posprobXY, negprobXY = naivebayesPXY_grader(X, Y)\n",
    "\n",
    "def naivebayesPXY_test1():\n",
    "    x = np.array([[0,1],[1,0]])\n",
    "    y = np.array([-1,1])\n",
    "    pos, neg = naivebayesPXY(x,y)\n",
    "    pos0, neg0 = naivebayesPXY_grader(x,y)\n",
    "    test = np.linalg.norm(pos - pos0) + np.linalg.norm(neg - neg0)\n",
    "    return test < 1e-5\n",
    "\n",
    "def naivebayesPXY_test2():\n",
    "    pos, neg = naivebayesPXY(X,Y)\n",
    "    test = np.linalg.norm(pos - posprobXY) + np.linalg.norm(neg - negprobXY)\n",
    "    return test < 1e-5\n",
    "\n",
    "def naivebayesPXY_test3():\n",
    "    pos, neg = naivebayesPXY(X,Y)\n",
    "    return pos.shape == posprobXY.shape and neg.shape == negprobXY.shape\n",
    "\n",
    "### END HIDDEN TESTS\n",
    "#Autograder cell- do not edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9c52a579312975b7",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "def loglikelihood_test1():\n",
    "    x = np.array([[0,1],[1,0]])\n",
    "    y = np.array([-1,1])\n",
    "    loglike = loglikelihood(posprobXY[:2], negprobXY[:2], x, y)\n",
    "    loglike0 = loglikelihood_grader(posprobXY[:2], negprobXY[:2], x, y)\n",
    "    test = np.linalg.norm(loglike - loglike0)\n",
    "    return test < 1e-5\n",
    "\n",
    "def loglikelihood_test2():\n",
    "    x = np.array([[1,0,1,0,1,1], \n",
    "        [0,0,1,0,1,1], \n",
    "        [1,0,0,1,1,1], \n",
    "        [1,1,0,0,1,1]])\n",
    "    y = np.array([-1,1,1,-1])\n",
    "    loglike = loglikelihood(posprobXY[:6], negprobXY[:6], x, y)\n",
    "    loglike0 = loglikelihood_grader(posprobXY[:6], negprobXY[:6], x, y)\n",
    "    test = np.linalg.norm(loglike - loglike0)\n",
    "    return test < 1e-5\n",
    "\n",
    "def loglikelihood_test3():\n",
    "    x = np.array([[1,1,1,1,1,1], \n",
    "        [0,0,1,0,0,0], \n",
    "        [1,1,0,1,1,1], \n",
    "        [0,1,0,0,0,1], \n",
    "        [0,1,1,0,1,1], \n",
    "        [1,0,0,0,0,1], \n",
    "        [0,1,1,0,1,1]])\n",
    "    y = np.array([1, 1, 1 ,1,-1,-1, 1])\n",
    "    loglike = loglikelihood(posprobXY[:6], negprobXY[:6], x, y)\n",
    "    loglike0 = loglikelihood_grader(posprobXY[:6], negprobXY[:6], x, y)\n",
    "    test = np.linalg.norm(loglike - loglike0)\n",
    "    return test < 1e-5\n",
    "\n",
    "assert loglikelihood_test1(), \"loglikelihood test failed - calculation of loglikelihood seems incorrect\\n\"\n",
    "assert loglikelihood_test2(), \"loglikelihood test failed - calculation of loglikelihood seems incorrect\\n\"\n",
    "assert loglikelihood_test3(), \"loglikelihood test failed - calculation of loglikelihood seems incorrect\\n\"\n",
    "### END HIDDEN TESTS\n",
    "#Autograder cell- do not edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ea5dc1d4f9623704",
     "locked": true,
     "points": 4,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "# check whether the predictions are +1 or neg 1\n",
    "def naivebayes_pred_test1():\n",
    "    preds = naivebayes_pred(posY, negY, posprobXY, negprobXY, X)\n",
    "    return np.all(np.logical_or(preds == -1 , preds == 1))\n",
    "\n",
    "def naivebayes_pred_test2():\n",
    "    x_test = np.array([[0,1],[1,0]])\n",
    "    preds = naivebayes_pred_grader(posY, negY, posprobXY[:2], negprobXY[:2], x_test)\n",
    "    student_preds = naivebayes_pred(posY, negY, posprobXY[:2], negprobXY[:2], x_test)\n",
    "    acc = analyze_grader(\"acc\", preds, student_preds)\n",
    "    return np.abs(acc - 1) < 1e-5\n",
    "\n",
    "def naivebayes_pred_test3():\n",
    "    x_test = np.array([[1,0,1,0,1,1], \n",
    "        [0,0,1,0,1,1], \n",
    "        [1,0,0,1,1,1], \n",
    "        [1,1,0,0,1,1]])\n",
    "    preds = naivebayes_pred_grader(posY, negY, posprobXY[:6], negprobXY[:6], x_test)\n",
    "    student_preds = naivebayes_pred(posY, negY, posprobXY[:6], negprobXY[:6], x_test)\n",
    "    acc = analyze_grader(\"acc\", preds, student_preds)\n",
    "    return np.abs(acc - 1) < 1e-5\n",
    "\n",
    "def naivebayes_pred_test4():\n",
    "    x_test = np.array([[1,1,1,1,1,1], \n",
    "        [0,0,1,0,0,0], \n",
    "        [1,1,0,1,1,1], \n",
    "        [0,1,0,0,0,1], \n",
    "        [0,1,1,0,1,1], \n",
    "        [1,0,0,0,0,1], \n",
    "        [0,1,1,0,1,1]])\n",
    "    preds = naivebayes_pred_grader(posY, negY, posprobXY[:6], negprobXY[:6], x_test)\n",
    "    student_preds = naivebayes_pred(posY, negY, posprobXY[:6], negprobXY[:6], x_test)\n",
    "    acc = analyze_grader(\"acc\", preds, student_preds)\n",
    "    return np.abs(acc - 1) < 1e-5\n",
    "\n",
    "assert naivebayes_pred_test1(), \"naivebayes_pred test failed - Your classification results should only contain -1 or 1\\n\"\n",
    "assert naivebayes_pred_test2(), \"naivebayes_pred test failed - Your classification results are incorrect\\n\"\n",
    "assert naivebayes_pred_test3(), \"naivebayes_pred test failed - Your classification results are incorrect\\n\"\n",
    "assert naivebayes_pred_test4(), \"naivebayes_pred test failed - Your classification results are incorrect\\n\"\n",
    "### END HIDDEN TESTS\n",
    "#Autograder cell- do not edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d1a30da7ad2e505e",
     "locked": true,
     "points": 0,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'girl_train_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-d383ce8bf54e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### BEGIN HIDDEN TESTS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mgenTrainFeatures_comp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgirl_train_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mboy_train_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mboysList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'girl_train_file' is not defined"
     ]
    }
   ],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "def genTrainFeatures_comp(g=girl_train_file, b=boy_train_file):\n",
    "    with open(b, 'r') as f:\n",
    "        boysList = [x.rstrip() for x in f.readlines() if len(x) > 0]\n",
    "    with open(g, 'r') as f:\n",
    "        girlsList = [x.rstrip() for x in f.readlines() if len(x) > 0]\n",
    "    xTrStr = np.concatenate([girlsList, boysList])\n",
    "    yTr = np.concatenate([-np.ones(len(girlsList)), np.ones(len(boysList))])\n",
    "    ii = np.random.permutation([i for i in range(len(yTr))])\n",
    "    xTrStr = xTrStr[ii]\n",
    "    yTr = yTr[ii]\n",
    "    with open('shuffled.test', 'w') as f:\n",
    "        for name in xTrStr:\n",
    "            f.write('{0}\\n'.format(name))\n",
    "    return name2features2('shuffled.test'), yTr\n",
    "\n",
    "def competition_test():\n",
    "    XTr,YTr = genTrainFeatures_comp()\n",
    "    pos, neg = naivebayesPY(XTr, YTr)\n",
    "    posprob, negprob = naivebayesPXY(XTr, YTr)\n",
    "    XTe,YTe = genTrainFeatures_comp(g= girl_test_file, b=boy_test_file)\n",
    "    # XTe, YTe = genTrainFeatures_comp()\n",
    "    preds = naivebayes_pred(pos, neg, posprob, negprob, XTe)\n",
    "    return analyze_grader(\"acc\", YTe, preds)\n",
    "\n",
    "print('The accuracy of your features on the hidden test set is {:.2f}%'.format(competition_test()*100))\n",
    "### END HIDDEN TESTS\n",
    "#Autograder cell- do not edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
